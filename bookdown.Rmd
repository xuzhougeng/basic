--- 
title: "生物信息学最佳实践--基础篇"
author: "曾健明"
date: "`r Sys.Date()`"
documentclass: ctexbook
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
colorlinks: yes
lot: yes
lof: yes
geometry: [b5paper, tmargin=2.5cm, bmargin=2.5cm, lmargin=3.5cm, rmargin=2.5cm]
site: bookdown::bookdown_site
description: "对应着生信技能树论坛的6个基础板块，包括数据产出，数据规范，数据库资源，统计学，计算机以及生物学基础"
github-repo: best-practices-in-bioinformatics/basic
cover-image: cover.png
---

```{r setup, include=FALSE}
options(
  htmltools.dir.version = FALSE, formatR.indent = 2, width = 55, digits = 4
)

# 填上你需要用到的包，如 c('ggplot2', 'dplyr')
lapply(c('Rsamtools'), function(pkg) {
  if (system.file(package = pkg) == '') install.packages(pkg)
})
```

# 前言 {-} 

生物信息学的重头戏就是处理各种各样的数据，**第 \@ref(sequencing) 章**介绍了生物信息学主流数据产生方法，包括多种多样的芯片技术，二代测序技术和三代测序技术。

既然数据的来源是如此的丰富多样，而且指定标准的单位比较多，就必然会产生各种各样的数据存储形式来规范化交流，**第 \@ref(filetype) 章**会详细介绍fastq,fasta,sam,bam,vcf,gff,gtf,bed,MAF等，理解了它们才能真正的入门生物信息学。

了解了生物信息学领域的常见数据的产生和数据规范还不够，非常高频的需求是跟已有的各种数据库资源做比较，包括与参考基因组比对，基因结构以及功能的注释等等。**第\@ref(database) 章**会详细介绍生信工作者必须了解的主流数据库，包括NCBI,UCSC,ENSEMBL，以及GO,KEGG,GENCODE,SRTINGDB,还有TCGA,ENCODE,1000GENOMES等等。

做生物信息学数据分析的大多统计学不是太好，而且我们这本书却是基础教程，整个架构就类似于一个网站首页的导航条，所以我们不会太深入讲解统计学原理以及公式。在**第\@ref(statistics) 章**会罗列一些统计学常识以及高频统计学分析方法。当然，也会推荐一些好的学习资源供大家深入学习提高自己。

至于计算机方面，我一直给大家强调一个**去可视化**的精神，虽然表面上看是反人类的操作习惯，但是在数据分析领域非常实用。因为大量的生物信息学软件并没有对桌面操作系统兼容，同时**批量操作**也是常态，这个时候就无法用鼠标傻瓜式的不停点击了。
如果是Windows电脑，那么必须安装,git,notepad++,everything,xshell,winscp这些软件，然后想方设法找到linux服务器连接到终端就可以处理数据啦。如果是MAC的话，直接打开终端即可。要数量在终端操作各种数据就必须学好shell命令。如果是常规的流程化分析，那么shell脚本就足够了。但真正的数据分析过程中经常会遇到个性化的需求，可以选择perl或者python来完成。同时R语言也是必修，因为它涵盖了统计分析，可视化，以及生物信息学bioconductor包。这些全部**第\@ref(computering) 章**会介绍。

最后是生物学基础，在**第\@ref(biology) 章**介绍。因为大部分看这本书的朋友都是生物学背景，所以把这个放在最后。其实主要也是介绍概念，生物学概念太磅礴了，这里只是介绍那些在生物信息学数据处理实战中经常用得着的那些知识点。首当其冲的就是中心法则啦，以及它包含的DNA,RNA,PROTEIN，为了量化这一个法则而扩展出了各种测序组学分析领域。

我用了两个 R 包编译这本书，分别是 **knitr**\index{knitr} [@xie2015] 和 **bookdown**\index{bookdown} [@R-bookdown]。以下是我的 R 进程信息：

```{r}
sessionInfo()
```

## 致谢 {-}

非常感谢[生信菜鸟团](http://www.bio-info-trainee.com/)以及[生信技能树](http://www.biotrainee.com/)对我的帮助。艾玛，要不是他们神一样的队友，我两年前就写完这本书了。


```{block2, type='flushright', html.tag='p'}
曾健明 
于 珠海横琴某渔村
```

<!--chapter:end:index.Rmd-->

# 作者简介 {#author .unnumbered}

* [我的博客](http://www.bio-info-trainee.com/)
* [我们的论坛](http://www.biotrainee.com/forum.php)
* [捐赠我](http://www.bio-info-trainee.com/donate)

#### 编者-曾健明 {-}

生信菜鸟团博客博主,同时也是生信技能树创始人。

工作经历：CAFS-->sustc-->Lilly-->universityof Macau（表观遗传学方向）

生信技能：WES/RNA-seq/CHIP-seq的相关数据处理流程搭建及项目实战；大型服务器的安装与维护；生物信息数据处理环境搭建；简单数据处理网页工具制作。

作为菜鸟团博主和论坛创始人，最初与大家分享自己的生信的学习笔记及心得体会。促进生信的学习和交流，构建出完整的生物技能树。搭建生信技术人员联盟，从入门到进阶帮助到每一位生信人。

最期待看到团队成员的成长，以及论坛稳健发展和各版块完善。带领团队和论坛成员完善生信技能树的同时，自己也收获前所未有的锻炼，希望自己不忘初心。
 
#### 编者-赵飞 {-}

中科院上海植物生理生态研究所硕博连读二年级，主要方向为生物信息与植物表观遗传。目前任生信技能树公众号编辑，正在努力学习各种数据分析所需技能。

喜欢分享和交流，曾著有《靠谱学长说:聊聊考研复习这件事》文集!

生信技能：Linux系统使用，shell和R的基础编程，简单数据库网站开发，转录组学(lncRNA/small RNA)数据分析.

希望和大家共同进步，共同成长。

#### 编者-曹换换 {-}

天津大学精仪学院硕士，生物信息方向（肿瘤）。

一个活脱脱的理想主义者！

生信技能：生信入门阶段，常用语言R语言。Linux系统的安装，使用初级。简单的python语言使用。

希望有更多的人参与生信技能树，无私分享！做大做强！

其实我立志想做美少女生信讲师~~~哈哈哈哈哈哈~~~要不然健美操白跳了


#### 编者-徐洲更 {-}

中科院上海植物生理生态研究所硕博连读生，主要方向为植物遗传学和生物信息学。  

喜欢写作，在简书，生信技能树公众号，生信媛公众号都可以看到我的文章哟。

生信技能：熟练掌握Linux基本操作，擅长用R编程，基于Python进行网页开发，转录组学(mRNA)数据分析，重测序遗传定位

生命不息，折腾不止

#### 编者-覃千山 {-}

本科毕业于东北林业大学，硕士毕业于中国农业科学院。现已在苏州工作两年。

生信技能：宏基因组分析，NGS流程搭建，引物设计；熟练python、R、linux，会点网页，会点latex

铁汉1990和sam's note的博主。尝试，总结，分享，不断成长，不断创造。

#### 团队-生信菜鸟团公众号  {-}

![生信菜鸟团公众号二维码](http://www.bio-info-trainee.com/wp-content/uploads/2017/03/1505528391.png) 


#### 团队-生信技能树公众号  {-}

![生信技能树公众号二维码](http://www.bio-info-trainee.com/wp-content/uploads/2017/03/1505525834.png) 



<!--chapter:end:00-author.Rmd-->

\mainmatter

# 数据产生 {#sequencing}

做软件开发和建模的毕竟是少数，大部分人进入生物信息学领域的第一步就是数据处理，有经验的分析者各种各样的数据都会遇到，本章节会一一谈到。 

其中大部分生物信息学相关数据都会由全世界各地的科研工作者上传到GEO数据库，包括各种芯片数据及针对各种物种的测序数据，它们的[样本数排序](https://www.ncbi.nlm.nih.gov/geo/browse/?view=platforms&display=20&zsort=samples)如下：

![平台流行程度](image/C1/GEO_platform_top50.png)

由上表可以看出，到目前为止（2017年）**累积应用**最广泛仍然是mRNA的表达谱检测芯片，以affymetrix的 Human Genome U133 Plus 2.0 Array和illumina的HumanHT-12 V4.0 expression beadchip为代表。
高通量测序作为后起之秀已经逐渐在超过芯片数据样本量了，而且高通量测序几乎都是illumina的测序仪产出的，包括HiSeq 2000/2500，nextSeq500,MiSeq等等。虽然HiSeq X10并没有排在前面，原因其实是GEO公布的数据与现实有两到三年的时差，因为测序数据需要发表文章之后才会公布，而且可以设置一到两年的释放期限。

值得一提的是HiSeq X10作为目前通量最高的测序仪，其大多测的是人类癌症病人相关数据，而这些数据因为隐私问题，很多都是不予公开的，需要注册申请才能下载。

下面我们就GEO里面存放的数据进行分门别类的介绍，分首先是芯片数据，包括各种生物学应用的芯片，然后是高通量测序数据，包括二代三代测序数据。

## 芯片技术

> 早期的生物信息学数据都是由芯片产生，2001年做两百多个mRNA表达谱芯片数据分析都可以发nature，虽然现在逐渐被NGS替代了，但是成本方面却还是它们的优点，所以仍然会有不少科研项目选择芯片来检测样本。

这样，做生物信息学数据分析的我们还是得掌握它们的基本分类，历史，以及处理方法。而且已经发表的芯片数据量非常庞大，在里面进行一些探索性挖掘也会有意想不到的收获。当然，前提是要下功夫了解芯片，学习它们。

### 三大芯片制造商

> 在GEO数据库里面查看GPL信息，就可以看到芯片厂商出品的数据排名，基本上就是 affymetrix , illumina , agilent 这3家公司为主。

#### Affymetrix {-}

昂飞（Affymetrix）公司，基因芯片行业的先驱，全球销量第一的基因芯片厂家。它是第一款商业化基因芯片的诞生之地，它是全球基因芯片行业标准的制定者。

上个世纪八十年代，Stephen P.A. Fodor博士与他的科学家团队将组合化学技术与半导体制造技术结合，尝试在小玻璃芯片上建立大量生物数据。这一革命性的想法经过他们十多年的钻研尝试，成为了现实——他们发明出革命性的基因芯片GeneChip®技术。这一技术迅速进行商业转化，由Affymax公司的Affymetrix部门专职运营。

1992年，Stephen P.A. Fodor博士带领Affymetrix部门从Affymax公司独立出来，在硅谷中心单独成立公司，名字仍采用Affymetrix，也就是现在的Affymetrix公司。公司最初的发展受到美国政府先进技术计划项目的经费支持，而公司也没辜负联邦政府的期望，1994年就开始将GeneChip®专利技术进行商业化运营，并于1996年在纳斯达克上市。Affymetrix以基因芯片为立家之本，陆续收购包括分子生物学试剂USB、免疫学试剂公司eBioscience等多家生物医学公司，形成了环绕基因组学和蛋白组学的庞大产品生态群。

Affymetrix公司致力于研发能够在细胞、蛋白和基因水平对生物系统进行多重和平行分析的技术，促进科研人员研究成果的转化，使科学家和临床医生能够将其研究成果更快地转化为对疾病的治疗技术，并在农业领域促进基因标记辅助育种技术的发展。

Affymetrix公司的客户遍布于生物学科研工作的各个领域，包括生物制药、临床诊断、农业育种、食品安全和消费品行业，同时还包括科研院所、政府实验室和其他非盈利性研究机构等。

Affymetrix公司曾入围全球50个最具创新能力公司和2011年度Frost & Sullivan北美产品领袖奖。公司于1996年在美国纳斯达克(NASDAQ)上市，总市值超过30亿美元，20年来，凭借在美国股票市场上的融资，Affymetrix公司在企业并购的路上一路狂奔。

```
2000年2月，Affymetrix 收购了DNA芯片仪器公司Genetic MicroSystems；
2000年10月，Affymetrix收购了计算基因组学公司Neomorphic；
2005年，Affymetrix收购了基因研究公司 ParAllele BioScience，随后又收购了分子生物学和生物化学试剂产品公司USB及膜蛋白提取纯化产品公司Anatrace；
2008年，Affymetrix收购了基因标记公司True Materials 和芯片公司Panomics；
2012年，收购了免疫肿瘤研究和诊断公司eBioscience。
```

然而，Affymetrix公司也逃不过被并购的命运。在2016年1月9日，全球最大的科学仪器生产商赛默飞世尔科技公司表示，同意13亿美元现金收购美国芯片制造商Affymetrix，以加强其基因分析产品相关业务。

在GEO数据库里面，[Affymetrix芯片的样本排序是](https://www.ncbi.nlm.nih.gov/geo/browse/?view=platforms&search=Affymetrix&display=20&zsort=samples)：

![Affymetrix芯片排序](image/C1/GEO_Affymetrix_top50.png)

#### Illumina  {-}

> Illumina总部位于美国加州圣地哥。它们的二代测序技术太出名以至于掩盖掉了其芯片产品的关辉，在GEO数据库可以查到它们的芯片科研市场占有率也不可小觑。
Illumina的核心技术是光纤微珠芯片平台和Solexa测序平台，广泛应用于基因组学研究。
其新一代激光共聚焦光纤微珠芯片技术，具有高密度，高灵敏度，高重复性，定制灵活等特点。
该平台主要应用于功能基因组学基因表达的研究和SNP基因分型研究，可高精度地对数千甚至数万个SNP位点及基因表达谱、LOH、染色体片段重复等进行检测和分析。

芯片优点有：

```
•杂交效率高-每个微珠连有约80万条同种探针，加之微珠的空间效应，杂交效率高。
•数据质量高-每个珠子在芯片上随机分布，且高达15~30倍重复，数据质量高。
•完全的质控-独特的解码技术，使得每个珠子每种探针都能得到质控。
•良好的重复性-芯片具有良好的重复性，相关系数R2大于0.99。
•高灵敏度低上样量-低至ng级的核酸样本就即可有效检测。
•定制灵活-可根据不同使用者研究需求量身打造芯片产品。
•广泛的应用范围-光纤微珠芯片广泛应用于SNP基因分型研究、基因表达的研究和甲基化研究。
```

在GEO数据库里面，[Illumina芯片的样本排序是](https://www.ncbi.nlm.nih.gov/geo/browse/?view=platforms&search=Illumina&display=20&zsort=samples)：

![Illumina芯片排序](image/C1/GEO_Illumina_top50.png)


不得不提的是Illumina公司的产品线除了上面提讲解的通量生物芯片，更为流行的其实是高通量测序仪，当然，还有少量定量PCR仪。

高通量生物芯片技术是Illumina公司成长的第一个里程碑，在人类基因组计划完成后，生命科学研究进入后基因组时代，对于SNP的研究成了基因功能研究的主要技术手段，国际上也有了专门针对SNP研究的Hapmap计划，而Illumina公司的高通量芯片称为Hapmap计划的主要技术平台，所有Hapmap的数据，80％以上都来自于Illumina的高通量芯片平台，全世界科学家选择Illumina芯片平台的主要原因也在于此。

高通量测序仪可谓是Illumina最大的产品线，高通量测序技术的出现和发展可以说是近五年时间内生命科学领域最具有革命性发展的技术之一，它极大的加速了整个科学研究的进程，也改变了应用领域和临床研究及诊断的思路。目前，公司主打产品有MiSeq测序仪、HiSeq X Ten测序仪、Miseq FGx测序仪、NextSeq 500/550桌上型测序仪、MiniSeq台式测序仪等等。

定量PCR仪Eco，是Illumina公司在2010年收购另外一个定量PCR仪厂家而扩增的产品线。定量PCR仪是目前分子生物学实验中常用的核苷酸检测设备，已经被广泛应用于生命科学领域的各个方面，包括新药开发研究、药物疗效研究、药物预后指标研究等等。

#### agilent {-}

Agilent的生物芯片（系统）和别的公司的生物芯片（系统）一样，同样由：扫描仪、生物芯片、分析软件，三部分组成。

Agilent的芯片扫描仪，叫SureScan DX。SureScan DX已经取得了欧洲的CE认证，和中国的CFDA认证，可以应用于临床。

Agilent的CGH生物芯片，在细胞遗传学中有着很广泛的接受度，并可以临床应用。
Agilent的表达谱芯片，是用荧光素直接标记的，检测灵敏度高、检测速度快、检测的线性范围大，很受欢迎。另外比较受欢迎的就是Agilent的miRNA相关芯片了。

在GEO数据库里面，[Agilent芯片的样本排序是](https://www.ncbi.nlm.nih.gov/geo/browse/?view=platforms&search=agilent&display=20&zsort=samples)：

![Agilent芯片排序](image/C1/GEO_Agilent_top50.png)



### 表达谱芯片

> 最初的芯片就是为了检测mRNA的表达量而设计开发的，而affymetrix就是其中的佼佼者。

表达谱DNA芯片（DNA microarrays for gene expression profiles）是指将大量DNA片段或寡核苷酸固定在玻璃、硅、塑料等硬质载体上制备成基因芯片.
待测样品中的mRNA被提取后，通过逆转录获得cDNA，并在此过程中标记荧光，然后与包含上千个基因的DNA芯片进行杂交反应30min~20h后，将芯片上未发生结合反应的片段洗去，
再对玻片进行激光共聚焦扫描，测定芯片上个点的荧光强度，从而推算出待测样品中各种基因的表达水平。
用于研究基因表达的芯片可以有两种：① cDNA芯片；② 寡核苷酸芯片。


早期的表达谱芯片主要是Affymetrix公司的hgu系列，包括95,133 出货量最大的应该是**hgu133plus2系列**了，如下

![](image/C1/affymetrix-133plus2-array.png)

统称为旧版的affymetrix芯片，用R语言的affy包即可读取进行数据处理，我曾经写过教程：[用affy包读取affymetix的基因表达芯片数据-CEL格式数据](http://www.bio-info-trainee.com/1580.html) 就是下面的代码：

```
## 把所有的cel文件都放在一个文件夹(比如/home/jmzeng/your/celfiles文件夹)下面，文件夹地址赋给变量 dir_cels
dir_cels='/home/jmzeng/your/celfiles'
library(affy)
#perform mas5 normalization
affy_data = ReadAffy(celfile.path=dir_cels)
eset.mas5 = mas5(affy_data)
exprSet.nologs = exprs(eset.mas5)
exprSet = log(exprSet.nologs, 2)  #transform to Log_2 if needed
### 上面是是mas5)，下面是rma，两种不同的芯片数据处理方法，最后都是要生成表达矩阵。
library(affy)
data <- ReadAffy(celfile.path=dir_cels) 
eset <- rma(data)
write.exprs(eset,file="data.txt")
```

还有新版的表达谱芯片，比如Human Gene 1.0 St Array  

![](image/C1/affymetrix-2.0ST-array.png)

用R语言的oligo包来进行数据处理，我曾经写过教程：[用oligo包来读取affymetix的基因表达芯片数据-CEL格式数据](http://www.bio-info-trainee.com/1586.html)，把芯片数据转换为表达矩阵的核心代码如下：

```
library(oligo)
celFiles <- list.celfiles()
affyRaw <- read.celfiles(celFiles)
library(pd.mogene.2.0.st)  
## 根据芯片平台来载入芯片设计包，没办法自动选择
## 不同的芯片探针包不一样：mogene20sttranscriptcluster.db
eset <- rma(affyRaw)
write.exprs(eset,file="data.txt")
```

affymetrix还开发了**更新版**的表达谱检测芯片，比如针对人类的HST2.0，据文献报道其很多方面都优于转录组测序，这款芯片设计比较复杂，如下：

![](image/C1/affymetrix-human2.0ST-characterization.png)

不过，illumina的表达谱芯片也是非常出名的，比如Illumina HumanHT-12 V4.0 expression beadchip。 

需要用R包lumi来处理illumina的bead系列表达芯片，[见我博客教程](用lumi包来处理illumina的bead系列表达芯片)

上述表达谱芯片在GEO里面的代表性数据有：

| GEO地址                                    | 数据描述                                     |
| ---------------------------------------- | ---------------------------------------- |
| [GSE11072](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE11072) | 2009-gastric cancer SBC Human 16K cDNA Microarray |
| [GSE42872](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE42872) | 2015-melanoma-vemurafenib HuGene-1_0-st  |
| [GSE2467](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE24673) | 2015-hub-gene-mcode-retinoblastoma  HuGene-1_0-st |
| [GSE22863](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE22863) | 2011-NSCLC HuGene-1_0-st                 |
| GSE622221, GSE4180414, GSE5140122        | A total of 117 samples (54 cases and 63 controls) Affymetrix Human Genome U133 Plus 2.0 Array  2015-HCC |
| [GSE21815](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE21815) | 2016-CRC Agilent-014850 Whole Human Genome Microarray 4x44K |

公布这些数据的文章里面都有对这些数据处理的方法的详细描述，以及对应的分析结果。


### 基因分型芯片

> 在人类基因组中，基因变异以许多不同的形式存在，包括体细胞突变、单核苷酸多态性（SNP）、拷贝数变异（CNV）和结构改变（插入缺失，Indel）。研究这些变异需要强大的基因分型工具。
其中最出名就是Affymetrix的SNP6.0芯片以及illumina的OMIN系列，下面是详细介绍：


一、基于GeneChip平台的基因分型芯片

* 芯片推荐： Genome-Wide Human SNP Array 6.0
* 芯片介绍： Affymetrix Genome-Wide Human SNP 6.0芯片产品涵盖超过1,800,000个遗传变异标志物：包括超过906,600个SNP和超过946,000个用于检测拷贝数变化（CNV, Copy Number Variation）的探针。


* 芯片推荐：DMET™ Plus
* 芯片介绍：人类基因组上的差异（SNP、插入、缺失、复制等）会导致不同个体对同一药物产生不同的反应。针对这种情况，Affymetrix推出了DMET（Drug-MetabolizingEnzymes and Transporters）Plus完整解决方案。它能检测225个基因中与药物代谢和转运有关的1936个分子标记。这些分子标记已经在不同种族、至少1200个个体中验证。通过检验不同个体分子标记的差异，研究者们可以发现药物发挥不同作用的遗传机制，从而决定药物的剂量和使用对象。

二、基于GeneTitan平台的Axiom®基因分型芯片

Axiom®基因分型解决方案为您提供多种芯片。您可以选择要研究物种的自定义内容，也可以选择来自Axiom®基因组数据库的基因型经过验证的内容。

芯片推荐：Infinium OmniZhongHua-8 Kit

覆盖了中国人特有常见和稀有变异，是第一款人类种群特异的全基因组芯片。经过优化的标签SNP内容来自HapMap所有三个阶段以及千人基因组计划（1kGP），可用于在中国人种群中探索全新的疾病和性状关联。特别覆盖中国人81%的常见变异（MAF>5%）和60%的稀有变异（MAF> 2.5%），适合全基因组关联研究（GWAS）。采用Illumina专利的BeadArrayTM技术，可提供非常高的数据质量，平均检出率>99%，重复率>99.9%。

illumina的Omni系列芯片经过了许多年的市场迭代，能检测的位点也越来越多，下面是它的一个简单的汇总表格。
 
![illumina的Omni系列芯片进化史](image/C1/illumina_omni_array_list.png)

参考：https://www.illumina.com/Documents/products/datasheets/datasheet_gwas_roadmap.pdf

### CNV芯片

> 拷贝数变异（CNV）是由基因组发生重排而导致的，一般指长度为1 kb 以上的基因组大片段的拷贝数增加或者减少，主要表现为亚显微水平的缺失和重复。
CNV 是基因组结构变异 ( SV ) 的重要组成部分。CNV 位点的突变率远高于SNP，是人类疾病的重要致病因素之一。
大量研究表明，尤其是肿瘤，与拷贝数变异密切先关。

能检测CNV的手段主要有：全基因组测序、生物芯片、CNVseq、CNVplex、AccuCopy、质谱法、Taqman法。

#### Affymetrix SNP6.0 array  {-}

其中最出名的要属Affymetrix SNP6.0 array这款芯片 ，在TCGA计划和CCLE里面都有。芯片特有超过180万个遗传变异标记，包括单核苷酸多态性（SNP）以及拷贝数变异（CNV）。对CCLE数据库可以做的分析非常多，强烈建议初学者花时间好好了解一下，[我博客也写过教程](http://www.bio-info-trainee.com/1327.html)
 
值得一提的是拷贝数变异的检测目前是以芯片技术为标准，NGS技术仍然是落后于芯片技术，在这一点上面。对于Affymetrix SNP6.0 array这款芯片得到也是CEL文件的原始数据，可以走PICNIC软件的流程来进行分析，得到最后的拷贝数变异的片段。

#### affymetrix公司的OncoScan平台 {-}

> 使用Affymetrix OncoScan FFPE芯片对肿瘤相关样品(包括cfDNA与FFPE样本)进行试验与数据分析，并通过OncoScan 3.01软件找到与临床相关的拷贝数变异（CNV）和杂合性缺失（LOH）。


突破性的技术：在短短48小时内完成全基因组范围的实体瘤拷贝数分析，仅需80ng起始DNA。
全新的OncoScan FFPE芯片利用Affymetrix独特的分子倒置探针（MIP）技术，能够快速经济地分析来自FFPE样本的少量的高度降解的DNA，让实体瘤癌症分析向前迈进了一大步。
这一新产品在一次实验中即可提供全基因组范围的拷贝数数据，且在全基因组内大约900个已知的癌基因、杂合性缺失（LOH）上的分辨率极高，同时可以提供临床相关的获得性突变的数据。在分析软件方面，研发人员升级了早一代的BioDiscovery Nexus软件，增加了许多转为癌症分析设计的功能，命名为OncoScan™ Nexus Express软件，它实现了几分钟内数百个样本的拷贝数分析能力，也将随产品免费提供。
此项技术之前是通过Affymetrix研究服务实验室的OncoScan™ Express 2.0服务来提供的，展现出空前的成功率，在过去两年已有70多篇文章发表。该平台实现了高度降解样本的DNA检测，从保存了十年或以上的FFPE样本中挖掘出丰富的信息。

目前Affymetrix的OncoScan在GEO平台的地址是：https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GPL18602

| Status          | Public on Apr 23, 2014                   |
| --------------- | ---------------------------------------- |
| Title           | [OncoScan] Affymetrix OncoScan FFPE Assay |
| Technology type | in situ oligonucleotide                  |
| Distribution    | commercial                               |
| Organism        | [Homo sapiens](https://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi?mode=Info&id=9606) |
| Manufacturer    | Affymetrix                               |

截止到目前(2017年09月07日)仅有22个实验数据，共355个样本的公共数据被上传。是非常新的平台


### 甲基化芯片

> DNA甲基化对基因表达调控起着重要作用，而且这种调控作用可以随着细胞分裂在世代间稳定遗传。
甲基化多发生于胞嘧啶位置，在细胞和组织分化、发育、衰老、疾病发生以及适应环境等过程中，甲基化状态可能随时发生变化。
非正常的DNA甲基化及其导致的基因表达往往与疾病相关联，例如肿瘤、精神性疾病、X-染色体沉默、自身免疫疾病等。

DNA甲基化已成为当前分子生物学的研究热点之一。而芯片是探索DNA甲基化程度的一大利器，在通量，覆盖度，性价比方面都优于二代测序技术。
且主要由illumina公司生产，产品进化路线是 27k --> 450K --> 850K。 
在TCGA计划的早期使用过不少27K的芯片，但是其覆盖位点太少，中后期全面转向为450K，同时也是GEO数据库里面被发表最多的甲基化芯片数据。

下面着重介绍一下最新款的IlluminaInfinium Methylation EPICBeadChip（850K）

该款芯片全面覆盖表观基因组的关联研究，可以基于单核苷酸分辨率定量检测基因组中的甲基化位点，主要优势如下

* ``高通量的定量检测``：可同时检测>850,000个位点；
* ``全面的全基因组覆盖范围``：覆盖>95%的CpG岛，99%的RefSeq基因，保留了>90%的450K甲基化位点；
* ``覆盖了增强子区域``：在增强子区域新增了350,000个甲基化位点（包括ENCODEenhancers，ENCODEopenchromatin，>80% ofFANTOM5 Enhancers，GENCODEgenes，promoters，UTRs）；
* ``分辨率高``：单碱基分辨率，可以直接检测到发生甲基化的准确位点；
* ``可重复性高``：技术重复的重现性>98%，与450K芯片的重复性>98%；
* ``DNA起始量低``：仅需250ng，相比于450K芯片的1ug DNA，大大节约了样品量；
* ``高质量的实验数据``：使用Infinium技术，无其它甲基化DNA捕获方法常有的偏向性，同时采用Infinium I和Infinium II探针设计，进一步保证高质量的实验数据；
* ``兼容FFPE样本``，支持LIMS及自动化系统。

一个基因会设计多个探针，尤其是结构复杂的基因，一般会在启动子区域，utr区域，外显子，内含子等genebody上面设计探针，如果想把探针ID对应到具体的基因的具体区域，需要去其芯片官网找**manifest文件**。

### miRNA芯片

> microRNA(microRNA)是一种小的内源性非编码RNA分子，大约由21-25个核苷酸组成。通过与靶基因的3’UTR配对，促进mRNA的降解或抑制mRNA的翻译，从而抑制靶基因的表达。

现在，miRBase共收录223个物种的28645个microRNA前体，35828个成熟microRNA (miRBase, v21, 2014年6月)。这些数据可以在它的[官网](ftp://mirbase.org/pub/mirbase/21/)直接下载.

为miRNA研究设计芯片的主要是Agilent公司，比较成熟的产品是Agilent microRNA芯片（V21.0）产品信息

| pecies|Name| P/N  |Design ID | Format  |
| -----|:-------------:| -----:| :-------------:| -----:| 
| Human| SurePrint G3 Human miRNA Microarray, Release 21 |  G4872A| 070156| 8 x 60K  |
| Mouse|SurePrint G3 Mouse miRNA Microarray, Release 21 |G4872A|070155| 8 x 60K  |
| Rat| SurePrint Rat miRNA Microarray, Release 21|  G4471A|070154| 8 x 15K  | 
  
  

## 二代测序技术

>又称下一代测序，就是NGS啦

2005年454公司的 Genome Sequencer 20 System，2006年illumina公司的Solexa和2007年ABI公司的SOLiD标志着第二代测序技术的诞生。
之后陆续在2006年推出454 FLX系统、2010年推出illumina HiSeq系统和同年推出的基于半导体芯片测序技术的Ion PGM已经陆续成为当前基因测序的中坚力量。

### NGS先行者：454及其序列数据

说到二代测序，454公司可谓是不折不扣的吃螃蟹的人。早在2005年，454就推出了第一个基于焦磷酸测序原理的高通量基因组测序系统——Genome Sequencer 20 System，这也是NGS领域的第一台测序仪。随后，454被罗氏收购，并在2006年推出了名动测序界的GS FLX测序系统。

454的技术核心在于乳液PCR和焦磷酸测序法，该技术能获得较长的序列，平均读长可达400 bp；相对于illumina平台其主要缺点是无法准确测定单碱基重复序列的长度。

454 GS-FLX测序仪产生的序列文件格式为SFF (Standard Flowgram Format)，是一种二进制文件。虽然目前有很多软件可以直接读取SFF格式文件进行分析，但更多的软件还是需要把SFF格式转换为Fna-Qual格式，这种格式把序列和与序列对应的碱基质量分别放在以Fna和Qual为后缀的文件里(文本文件)。

①.fna数据文件保存碱基序列   

```
>HKSD5CR01D6P3Ilength=70 xy=1599_2828 region=1 run=R_2012_03_15_01_23_26_GGAGTAGCATGCGTGACGAATCGTAGTTCCGACCATAACGATGCCGACCTTTGACCA
```

②qual数据文件保存碱基质量值
```
>HKSD5CR01D6P3Ilength=70 xy=1599_2828 region=1 run=R_2012_03_15_01_23_26_40 40 40 40 40 40 4040 40 40 40 40 40 40 40 40 40 40 40 40 39 39 39 40 40 40 34 34 34 34 40 30 3030 40 39 39 39 38 38 37 40 40 38 38 32 24 17 17 20 20 26 30 30 36 36 37
```

随着测序成本的不断降低，尽管读长偏长，但成本过于昂贵的454测序技术逐渐退出了科研界。
然而，虽然454的生命科学测序业务早已关停，454带来的影响却仍不容磨灭。
来自众多大型科研项目（例如人类微生物组计划）中的海量454序列信息，仍是当今研究中不可或缺的重要参考数据和引用来源。

### 渐行渐远：SOLiD及其序列数据

再来谈谈几乎快被遗忘的ABI公司。事实上，在454和illumina走入市场之前，ABI一直处理测序市场上傲视群雄的霸主地位。
在454和illumina初亮相后，由于SOLiD系统采用的是连接法而非PCR反应，因此对于高GC含量的样本具有非常大的优势。
然而，后续SOLiD系统通量难以提升，且读长短、成本高，也已慢慢退出历史舞台。

SOLiD技术，其主要核心是油包水PCR和基于“双碱基编码原理”连接测序法。
该技术主要优势在于每个位置的碱基均被检测两次，测序准确性高达99.9999%，算是第二代测序技术中准确性最高的。
但是该技术的读长仅为2X50 bp，后续序列拼接等比较复杂，且由于是双碱基对应一个荧光信号，一旦发生错误，容易在荧光解码阶段产生连锁错误。

SOLiD技术的原始数据格式是csfasta及与其匹配的qual，可以利用[python脚本](http://edison.cremag.org/resources/seq-analysis/tools/solid2fastq/solid2fastq.py)或者[perl脚本](http://www.bbmriwiki.nl/svn/bwa_45_patched/solid2fastq.pl)
将其转为csfastq(color space fastq)，与fastq格式一致，四行记录一碱基序列，区别在于第二行不是碱基序列，而是color的编码，如下：

```
@SRR2967009.1 100_1000_1168_F3
T10011023211201220121202030102221012302121010131001
+
2@@@@>@?@@@@<@@//;@@/@9?@8@=@@@6;6@66;<@6@67?2?;/@
```

colorspace编码规则为：

虽然csfastq与目前流行的fastq格式记录的测序序列不同，但是csfastq文件也可作为fastqc的输入文件进行质控,并可利用多款软件进行后续分析，如SHRiMP、Sequel、BFAST和Bowtie等等。
 
### 一家独大：illumina及其数据

illumina在NGS乃至整个测序市场中的霸主地位已毋庸置疑。引用illumina官方说法：“世界上90%以上的测序数据都由Illumina仪器产生”，不较真的话，这句话确实在某种程度上反应了illumina雄踞NGS市场的现状。尤其是HiSeq系列测序仪的问世，以通量高，产量大，生产规模著称，能够快速、经济的进行大规模平行测序，在大型全基因组测序，全转录组，全外显子组测序，靶向基因测序方面优势明显。

下面是illumina测序仪产品发布的时间线：

发布日期|测序仪型号
---|---
2010.01| HiSeq 2000
2011.02| MiSeq
2012.01| HiSeq 2500
2013.11| MiSeq Dx
2014.01| HiSeq X Ten，NextSeq 500
2015.01| HiSeq X Five，HiSeq 3000，HiSeq 4000，NextSeq 550，MiSeq FGx
2016.01| MiniSeq
2017.01| NovaSeqTM系列


HiSeq 3000/4000系统则基于成熟的HiSeq 2500系统，采用创新的有序流动槽技术最大限度提高效率，3.5天内可完成12个基因组、100个转录组或180个外显子组测序。HiSeq 3000/HiSeq 4000测序系统为生产级测序能力设立了一个全新的标准。

HiSeq X Ten系统的问世完成了人类历史上一大里程碑事件——千元基因组时代的到来。HiSeq X Ten系统是由一套共10台超高通量的HiSeq X仪器组成，每年能带来超过18,000个人类基因组，而每个基因组的价格约为1000美元，让癌症和复杂疾病的研究达到新的水平。虽然目前GEO里面的数据HiSeq X Ten测序仪产出的数据并不是独占鳌头，但也只是时间问题。

NovaSeq系列测序仪问世，毫无疑问标志着测序新纪元的到来，旨在将基因组测序的价格进一步降至100美元。全新的NovaSeq系列测序系统，突破技术革新，具有可扩展的通量、灵活简便的配置和简化的操作流程，允许以更大的深度来发现罕见的遗传变异，为大规模发现复杂疾病变异打开了全新的市场。**NovaSeq 测序平台单次运行最多可检测48个人类基因组，产出 6Tb 的数据量，运行时间缩短至40个小时。**


illumina技术核心在于桥式PCR和边合成边测序（SBS）。illumina测序系统的碱基读取也是基于化学发光法来的，给每一个碱基加入荧光基团，通过拍照捕捉发光的碱基，如此就得到了DNA的原始序列信息。

也就是说，illumina原始读取的是图像数据文件，解析后才形成碱基序列文件。对应到序列文件，其测序源文件为BCL格式文件（per-cycle BCL basecall file），BCL是一种包含碱基信号和图块质量信息的二进制文件，在进入下游分析前，BCL文件会经由Casava碱基识别(Base Calling)转化为原始测序序列（Sequenced Reads），转化得到的序列我们称之为Raw Data或Raw Reads，格式为Fastq。

Fastq是一种文件文件，如果Fastq文件大小在M级，一般可以直接用文本编辑器（如Notepad++）打开。打开之后，就可以看到类似下面表格中第二列的内容，每四行记录一段序列，这个序列也是有名字的，叫reads。如果采用的是illumina MiSeq PE250，则第二行碱基和第四行碱基质量值均为250个，即reads长度为250 bp。


|Illumina Seqence identifier|\@HWUSI-EAS100R:6:73:941:1973#0/1|  
|---------------------------|:------------------------------:| 
|Read bases	|GATTTGGGGTTCAAAGCAGTATCGATCAAATAG...|
|+	|+|
|Phred quality scores(ASCII)|	!''*((((***+))%%%++)(%%%%).1***-+。。。|
 
首行*illumina sequence identifier**记录的信息比较复杂

illumina sequence identifier中包含了此次测序的信息，根据平台不同包含的信息有所区别，一般来说可能包含以下的部分元素：
```
@<instrument>:<run number>:<flowcell ID>:<lane>:<tile>:<x-pos>:<y-pos>:<UMI> <read>:<is filtered>:<control number>:<index>
```

我们再回头看上面的例子，就可以对应查看这条序列文件中的测序信息了： 

|标识符|解释|  
|-----|:---:|
|@|必须以\@开头|
|HWUSI-EAS100R|	the unique instrument name|
|6|flowcell lane|
|73|tile number within the flowcell lane|
|941|'x'-coordinate of the cluster within the tile|
|1973|'y'-coordinate of the cluster within the tile|
|#0|index number for a multiplexed sample (0 for no indexing)|
|/1|the member of a pair, /1 or /2 (paired-end or mate-pair reads only)|

注：Run、Flowcell、Lane和tile的详细辨析，我们在第2章节还会更详细的讲解fastq格式的数据。


## 2.5代测序

> Ion Torrent  测序仪是第一个不需要光学系统的商业测序仪，所采用的技术为半导体测序，通过半导体芯片直接将化学信号转换为数字信号。是非常适合扩增子测序的革命性技术。它是一种经济、快速、简单、规模可扩展的测序技术。因为测序时间短，仪器设备便宜等特点而被各种研究所采用。

**一、前世今生**

Ion Torrent平台最初由Jonathan Rothberg于2007年开发，几经转手，2013年后归属Thermo Fisher，是他们家的掌中宝。目前有2010年发布的PGM，2012年发布Proton，2015年发布了Ion S5/XL。主要利用半导体芯片（无需荧光标记）技术，具有经济、快速、简单、规模可扩展的特点，适用于临床诊断和医学等使用场景，目前国内已有达安OEM版本DA8600。值得一提的是部分ion torrent测序仪受到了CFDA的认证。

**二、半导体测序原理** 

Ion Torrent的核心是半导体芯片测序技术，该技术采集的信号为pH值的变化，其他第二代测序技术主要是通过拍照采集荧光信号。该技术的主要优点是快速，主要缺点与Roche公司的454技术相同，在判读单碱基重复序列时存在困难。

自然情况下，DNA聚合酶将一个核苷酸渗入到DNA分子中就会释放出一个H+，导致局部可检验的PH值发生变化。如果发生结合，就会释放H+,相应的溶液PH值会发生改变，被离子传感器检测到，从而转换为数字信号。


如果检测DNA链上有两个相同的碱基，检测到的电压双倍，芯片则记录两个相同的碱基。

如果模板到的下个核苷酸与微芯片流的核苷酸不匹配，则检测不到电压，也不会记录碱基。



**三、优缺点** 

torrent测序仪拥有图形化的用户界面，便捷的操作方式，商业化的试剂盒，这肯定是专门为医院的朋友们准备的，他们不需要具体的代码与分析原理，只需要按照protocol一步步照着做，点点按钮就能生成可用的临床报告了，应该说想法是好的，客户定位是准确的。但是我觉得NGS从实验到分析那么多步骤，如果不懂NGS分析原理与流程，对图形化的参数与文件不了解，很可能得不到理想的分析结果从而对torrent丧失信心。而torrent软件相对比较封闭，专业的数据分析人员不太好下手，所以普遍对torrent数据敬而远之。



## 三代测序

层出不穷的三代小鲜肉中，表现最为亮眼的莫过于2010年推出的PacBio RS系列和2012年推出的Nanopore MinION。2015年PacBio又推出了万众瞩目的Sequel。
能进入三代测序仪队列的还有Nanopore公司，也在2017年推出了GridION，为更方便、快捷、准确、经济的测序提供了无限可能。

### 异军突起的pacbio

PacBio SMRT技术其实也应用了边合成边测序的思想，并以SMRT芯片为测序载体。测序时，不需要对目标DNA进行PCR扩增，而是直接在目标片段两端加上两个发卡结构的接头，形成一个连续的环状结构。也因此，PacBio系统在读长上显示了极大的优势。目前比较受市场热捧的三代测序是PacBio的RSⅡ和2015年推出的Sequel。

PacBio下机产生的序列文件以HDF5格式存储。可以采用h5dump命令来查看H5文件内容。

**1. 查看碱基序列：**

用代码``h5dump –d /PulseData/BaseCalls/Basecall raw.h5 > Basecall.info`` ,文件内容如下：

```
DATA {(0): 67, 71,67, 67, 65, 71, 67, 71, 65, 65, 84, 71, 71, 67, 84, 71, 67, (17): 71, 71, 71,71, 65, 65, 71, 67, 65, 71, 65, 65, 65, 84, 84, 65, 84, (34): 67, 67, 71, 84,65, 65, 65, 67, 84, 71, 84, 84, 71, 67, 84, 71, 67,
```

该文件采用的ASCII码的编码方式存储的碱基序列：A=> 65, C=>67, G=>71, T=>84。

**2. 查看碱基质量值：**

用代码``h5dump -d /PulseData/BaseCalls/QualityValue raw.h5 > Basecall.quality``,文件内容如下,其碱基质量值采用与illumina技术一致：

```
DATA {(0): 51, 44,42, 44, 24, 24, 51, 51, 51, 51, 50, 20, 20, 20, 50, 51, 51, (17): 48, 48, 48,47, 9, 9, 9, 51, 51, 46, 31, 31, 31, 31, 44, 51, 51, 30, (35): 30, 51, 51, 7,7, 7, 7, 51, 51, 44, 44, 44, 51, 51, 50, 27, 27, 26,
```


### 长到天际的Nanopore 


Oxford Nanopore 公司2005年在英国牛津成立，其运用的纳米孔测序技术使得DNA链在一个单通道中就能够被解码和识别，而不需要将长链打断成小短链。由于实现了DNA聚合酶内在自身的延续性和反应速度，Nanopore读长更长速度更快；同时由于能直接检测每个碱基的特征性电流，因而能对修饰碱基进行测序，对于表观遗传学研究具有极高的价值；因此，这款长到天际的测序仪，非常有潜力横扫当前测序格局。

2014年春天推出U盘大小的便携式MinION测序仪，仪器售价仅需$1000，据官网报道最长Reads可长达960 Kb，2014年10月推出平板大小的台式测序仪PromethION，有48个flow cell，可以单独运行也可以并行，2017年推出桌面式GridION X5测序仪。

Nanopore目前还主要在测试和生产阶段，尚未大规模应用，其应用主要体现在微生物等小基因组生物上。推出至今，其最亮眼的表现莫过于2014年西非埃博拉病毒爆发，MinION以最快的速度破译病毒序列，名噪一时。随着独特的纳米孔技术的成熟和完善，未来在即时检测、太空应用、大众检测等方面会有很大的想象空间。

Nanopore测序得到的序列文件的格式基础也是[HDF5](https://support.hdfgroup.org/HDF5/)，下机产生后缀为Fast5的序列文档。Fast5文件可经由[Poretools软件](http://poretools.readthedocs.io/en/latest/)转换为Fastq文件或Fasta，然后进行后续数据分析。

-  ①  应用Poretools将fast5转换为fastq，示例见：
http://poretools.readthedocs.io/en/latest/content/examples.html#poretools-fastq
- ②  应用Poretools将fast5转换为fasta，示例见：
http://poretools.readthedocs.io/en/latest/content/examples.html#poretools-fasta


在测序市场中，一代测序因其准确度高，仍作为突变检测、单菌鉴定等的金标准而存在。
以illumina HiSeq和MiSeq为代表的二代测序势头强劲，主打低成本和高通量，2017新机型NovaSeq更宣称已将测序成本降至百美金。
科研市场上三代测序最常见的莫过于PacBio，辅以冉冉上升的新星Nanopore等，主打长读长策略，直击二代测序碎片化序列的软肋，在基因组de novo上表现不俗，错误率较高，但可被矫正。




<!--chapter:end:01-sequencing.Rmd-->

# 数据存放类型 {#filetype}

各行各业都有在自己的标准体系，生物信息学数据分析也不例外，各个厂商出品的芯片系列，还有各种NGS组学分析，都会涉及到不同的分析步骤，有着丰富多样的中间文件。其中一些常用的文件就被规定成文件格式。
文件格式那么多，都可以了解一二，当然，不需要背诵它们所有的细节，不过对下面我们单独拿出来详细介绍的还是尽量要耳熟能详。

简单来说，测序得到的是带有质量值的碱基序列(fastq格式)，参加基因组是(fasta格式)，用比对工具把fastq格式的序列回帖到对应的fasta格式的参考基因组序列，就可以产生sam格式的比对文件。
把sam格式的文本文件压缩成二进制的bam文件可以节省空间，如果对参考基因组上面的各个区段标记它们的性质，比如哪些区域是外显子，内含子，UTR等等，这就是gtf/gff格式。
如果只是为了单纯描述某个基因组区域，就是bed格式文件，记录染色体号以及起始终止坐标，正负链即可。
如果是记录某些位点或者区域碱基的变化，就是VCF文件格式。

上面的描述只是简要的介绍了文件有哪些，以及它们的名字，下面就正式对它们进行详细的介绍。


## FASTQ

FASTQ是基于文本的，保存生物序列（通常是核酸序列）和其测序质量信息的标准格式。
其序列以及质量信息都是使用一个ASCII字符标示，最初由Sanger开发。
目的是将FASTA序列与质量数据放到一起，目前已经成为高通量测序结果的实施标准。


### 定义和示例  {-}

FASTQ文件中每个序列通常有四行：

```
第一行是序列标识以及相关的描述信息，以‘@’开头 
第二行是序列
第三行以‘+’开头，后面是序列标示符、描述信息，或者什么也不加，但是“+”不能少。
第四行，是质量信息，和第二行的序列相对应，每一个序列都有一个质量评分，根据评分体系的不同，每个字符的含义表示的数字也不相同。
```

一个简单的示例如下：
```
@SEQ_ID
GATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAATCCATTTGTTCAACTCACAGTTT
+
!''*((((***+))%%%++)(%%%%).1***-+*''))**55CCF>>>>>>CCCCCCC65
```


格式口诀：
```
 *‘@’开头引标识*
 *序列老二对老四*
 *老三没“+”不好使*
 *老四质量分数最多事*
```

### 序列标识  {-}

上面说到第一行是序列标识以及相关的描述信息，以‘@’开头。可以像上面的示例那么简单，但如果是正规测序仪下机的真实数据，通常会很复杂。比如：

```
@EAS139:136:FC706VJ:2:2104:15343:197393 1:Y:18:ATCACG
```

这个序列标识以及相关描述信息以冒号分割，每一个字段信息如下：
  
字段| 解释
---|---
EAS139 | 	the unique instrument name
136| 	the run id
FC706VJ | 	the flowcell id
2| flowcell lane
2104 |	tile number within the flowcell lane
15343 | ‘x’-coordinate of the cluster within the tile
197393 |	‘y’-coordinate of the cluster within the tile
1| the member of a pair, 1 or 2 (paired-end or mate-pair reads only)
Y | 	Y if the read fails filter (read is bad), N otherwise
18 | 	0 when none of the control bits are on, otherwise it is an even number
ATCACG |index sequence

当然，上面的表格介绍的只是其中一个测序仪下机数据，如果是其它机器，产商可以自由定义标识符格式，因为fastq格式的第一行只需要以\@符号开头即可。

不过，也有一些时候fastq数据并不是测序仪直接下机的，而且他人上传到了NCBI的SRA中心，我们下载下来解压后一般就没有了测序仪相关的标识，例子如下：

```
@SRR001666.1 071112_SLXA-EAS1_s_7:5:1:817:345 length=36
GGGTGATGGCCGCTGCCGATGGCGTCAAATCCCACC
+SRR001666.1 071112_SLXA-EAS1_s_7:5:1:817:345 length=36
IIIIIIIIIIIIIIIIIIIIIIIIIIIIII9IG9IC
```

### 质量编码格式 {-}

质量评分指的是一个碱基的错误概率的对数值。
其最初在Phred拼接软件中定义与使用，其后在许多软件中得到使用。
其质量得分与错误概率的对应关系见下表：

PHRED QUALITY SCORE | PROBABILITY OF INCORRECT BASE CALL |BASE CALL ACCURACY
---|---|---
10| 	1 in 10|90 %
20 | 	1 in 100|99%
30 | 	1 in 1000|99.9%
40 | 	1 in 10000 |99.99%
50| 	1 in 100000|99.999%

```
Phred quality scores Q are defined as a property which is logarithmically related to the base-calling error probabilities P.
Q=-10lgP
```

除了Phred质量得分换算标准，还有就是Solexa标准：是把P换成p/(1-p)


对于每个碱基的质量编码标示，不同的软件采用不同的方案，目前有5种方案：

Sanger，Phred quality score，值的范围从0到92，对应的ASCII码从33到126，但是对于测序数据（raw read data）质量得分通常小于60，序列拼接或者mapping可能用到更大的分数。

Solexa/Illumina 1.0, Solexa/Illumina quality score，值的范围从-5到63，对应的ASCII码从59到126，对于测序数据，得分一般在-5到40之间。

Illumina 1.3+，Phred quality score，值的范围从0到62对应的ASCII码从64到126，低于测序数据，得分在0到40之间；

Illumina 1.5+，Phred quality score，但是0到2作为另外的标示。

Illumina 1.8+

下面是更为直观的表示：

```

  SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS.....................................................
  ..........................XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX......................
  ...............................IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII......................
  .................................JJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJ......................
  LLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLL....................................................
  !"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyz{|}~
  |                         |    |        |                              |                     |
 33                        59   64       73                            104                   126

 S - Sanger        Phred+33,  raw reads typically (0, 40)
 X - Solexa        Solexa+64, raw reads typically (-5, 40)
 I - Illumina 1.3+ Phred+64,  raw reads typically (0, 40)
 J - Illumina 1.5+ Phred+64,  raw reads typically (3, 40)
    with 0=unused, 1=unused, 2=Read Segment Quality Control Indicator (bold)
    (Note: See discussion above).
 L - Illumina 1.8+ Phred+33,  raw reads typically (0, 41)
 
```

来至于 wikipedia：
 
### 文件后缀 {-}

没有特别的规定，通常使用.fq, .fastq, .txt等。
但是要注意，这个文件格式主要指的是文本文件里面的每行每列的内容规则，并不是我们常见的计算机领域的mp3,mp4,avi,xls,doc等等。

 
其它注意事项: 

* 双端测序一般有两个文件（也可通过某种规则把两个文件合并成一个）。
* 第一个文件与第二个文件的行数完全一样，且测序序列的排列顺序完全一致。
* 在第一个文件中，描述信息的结尾是“/1”，表示是双端测序的一端；第二个文件中同样位置/行数的相对应的测序序列的描述信息则以“/2”结尾，表示是双端测序的另一端。（2.2.2的表2-5中有叙述）

参考链接:

https://en.wikipedia.org/wiki/FASTQ_format




## FASTA  

在生物信息学中，FASTA格式，是一种基于文本用于表示核苷酸序列或氨基酸序列的格式。
在这种格式中碱基对或氨基酸用单个字母来编码，且允许在序列前添加序列名及注释。
fasta序列格式是blast组织数据的基本格式，无论是数据库还是查询序列，大多数情况都使用fasta序列格式。 
它要比上一小节介绍的FASTQ格式简明很多。

### 定义和示例 {-}

总的来说，Fasta格式开始于一个标识符：">"，然后是一行描述，下面是的序列，直到下一个">",表示下一条。

下面是一个来源于NCBI的fasta格式序列：

```
>gi|187608668|ref|NM_001043364.2| Bombyx mori moricin (Mor), mRNA
AAACCGCGCAGTTATTTAAAATATGAATATTTTAAAACTTTTCTTTGTTTTTA
TTGTGGCAATGTCTCTGGTGTCATGTAGTACAGCCGCTCCAGCAAAAATACCT
ATCAAGGCCATTAAGACTGTAGGAAAGGCAGTCGGTAAAGGTCTAAGAGCCAT 
ATCAAGGCCATTAA
```

Fasta格式首先以大于号“>”开头，接着是序列的标识符“gi|187608668|ref|NM_001043364.2|”，然后是序列的描述信息。
换行后是序列信息，序列中允许空格，换行，空行，直到下一个大于号，表示该序列的结束。
下面简单给一个表格说明序列来源的数据库与对应的标识符

Database Name数据库名称 | Identifier Syntax 标识符
------------------------|-------------------------
GenBank                 |  ```gb|accession|locus```
EMBL Data Library | 	```emb|accession|locus```
DDBJ, DNA Database of Japan | ```	dbj|accession|locus```
NBRF PIR | 	```pir||entry```
Protein Research Foundation | ```	prf||name```
SWISS-PROT | ```sp|accession|entry name```
Brookhaven Protein Data Bank | ```pdb|entry|chain```
Patents| 	```pat|country|number```
GenInfo Backbone Id | ```	bbs|number```
General database identifier | ```	gnl|database|identifier```
NCBI Reference Sequence | ```	ref|accession|locus```
Local Sequence identifier|	```lcl|identifier```


通常情况下序列的标识符不会像上面的例子那样复杂，再复杂的标识符也是有规则的，上面的标识符是NCBI定义的，可以去其官网了解详情。

自己总结了一条速记：
```
*大于号来表开头，*
*描述紧跟在后头，*
*ABCDEFG换行组成序列呦~*
```

#### 序列中字母代表的含义 {-}

FASTA格式支持的核苷酸代码如下：

核苷酸代码|意义
---|---
A | Adenosine
C| Cytosine
G| Guanine
T | Thymidine
U | Uracil
R |G A (puRine)
Y | T C (pYrimidine)
K | G T (Ketone)
M | A C (aMino group)
S | G C (Strong interaction)
W | A T (Weak interaction)
B |G T C (not A) (B comes after A)
D| G A T (not C) (D comes after C)
H| A C T (not G) (H comes after G)
V| G C A (not T, not U) (V comes after U)
N|A G C T (aNy)
X | masked
-| gap of indeterminate length

FASTA格式支持的氨基酸代码如下：

氨基酸代码 | 意义
---|---
A | Alanine
B | Aspartic acid or Asparagine
C| Cysteine
D| Aspartic acid
E| Glutamic acid
F | Phenylalanine
G | Glycine
H | Histidine
I|Isoleucine
K |Lysine
L | Leucine
M | Methionine
N| Asparagine
O | Pyrrolysine
P| Proline
Q| Glutamine
R |Arginine
S| Serine
T |Threonine
U | Selenocysteine
V | Valine
W |Tryptophan
Y |Tyrosine
Z | Glutamic acid or Glutamine
X |any
*| translation stop
- |gap of indeterminate length


参考链接
https://en.wikipedia.org/wiki/FASTA_format

注意事项：

对于自己构建的序列数据库（序列不是来源与NCBI或其他数据），可以采用“gnl|database|identifier”或者“lcl|identifier”格式，以保证可以使用blast的所有功能。
database或者identifier是需要指定的数据库的标识和序列标识，指定的名称可以用大小写字母、数字、下划线“_”、破折号“-”或者点号“.”。
注意名称是区分大小写的，同时不能出现空格，空格表示序列标识符结束。

数据库中的序列标识符必须保证唯一，许多时候格式数据库是formatdb报告错误，就是因为标示符重复，还有一点需要强调的是序列不能为空，否则也会报错。

## SAM格式

SAM是一种序列比对格式标准，由sanger制定，是以TAB为分割符的文本格式。
主要应用于测序序列mapping到基因组上的结果表示，当然也可以表示任意的多重比对结果。
SAM的全称是sequence alignment/map format。

### 定义和示例 {-}

SAM分为两部分，注释信息（header section ）和比对结果部分 （alignment section）。
通常是把FASTQ文件格式的测序数据比对到对应的参考基因组版本得到的。
注释信息并不是SAM文件的重点，是该SAM文件产生以及被处理过程的一个记录，规定以@开头，用不同的tag表示不同的信息，主要有：

* \@HD，说明符合标准的版本、对比序列的排列顺序；
* \@SQ，参考序列说明；
* \@RG，比对上的序列（read）说明；
* \@PG，使用的程序说明；
* \@CO，任意的说明信息。

一个简单的SAM文件例子如下：

```
@HD VN:1.0 SO:unsorted
@SQ SN:chr1 LN:249250621
@SQ SN:chr2 LN:243199373
@PG ID:Bowtie VN:1.0.0 CL:"bowtie genome/hg19 -q reads/SRR3101251.fastq -m 1 -p 4 -S"
SRR3101251.1 0 chr19 9486878 255 49M * 0 0 NTACTCCCACTACTCTCAGATTCAAGCAATCCTCCCACCCTAGCCCACC #1=DDDFFHHHHHIHHIJJJHIJIIJIHIFHJIIJJJJJJJIIJJJJJJ XA:i:1 MD:Z:0A48 NM:i:1
SRR3101251.5 16 chr2 240279787 255 49M * 0 0 CCTGAATCCATCAGAGCAGCCGGGCTGTGACACTCACTGTCATGATGTT JIJJIHIIIIJJJJJJJJJGHJJJJIIHJHICJIGCHHHHHFFFFFCCC XA:i:0 MD:Z:49 NM:i:0
SRR3101251.6 4 * 0 0 * * 0 0 NATTCCCACCTATGAGTGAGAATATGCGGTGTTTGGTTTTTTGTTCTTG #1=DDDFFHHHHHJJJGHIJJJJJJJJJJCGGIIJJIIJJJIJHJIIJJ XM:i:1
```

前四行是注释信息，后面是比对结果，下面对比对结果进行解释，它是SAM格式文件的精华部分。

### 比对结果详解 {-}

```
SRR035022.2621862 163 16 59999 37 22S54M = 60102 179 CCAACCCAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCGACCCTCACCCTCACCC >AAA=>?AA>@@B@B?AABAB?AABAB?AAC@B?@AB@A?A>A@A?AAAAB??ABAB?79A?AAB;B?@?@<=8:8 XT:A:M XN:i:2 SM:i:37 AM:i:37 XM:i:0 XO:i:0 XG:i:0 RG:Z:SRR035022 NM:i:2 MD:Z:0N0N52 OQ:Z:CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCBCCCCCCBBCC@CCCCCCCCCCACCCCC;CCCBBC?CCCACCACA@
```

所以在我们的例子中，每一个字段的说明如下：

```
QNAME	SRR035022.2621862
FLAG	163
RNAME	16
POS	59999
MAQ	37
CIGAR	22S54M
MRNM	=
MPOS	60102
ISIZE	179
SEQ	CCAACCCAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCGACCCTCACCCTCACCC
QUAL	>AAA=>?AA>@@B@B?AABAB?AABAB?AAC@B?@AB@A?A>A@A?AAAAB??ABAB?79A?AAB;B?@?@<=8:8
TAG	XT:A:M
TAG	XN:i:2
TAG	SM:i:37
TAG	AM:i:37
TAG	XM:i:0
TAG	XO:i:0
TAG	XG:i:0
TAG	RG:Z:SRR035022
TAG	NM:i:2
TAG	MD:Z:0N0N52
TAG	OQ:Z:CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCBCCCCCCBBCC@CCCCCCCCCCACCCCC;CCCBBC?CCCACCACA
```

比对结果部分（alignment section），每一行表示一个片段（segment）的比对信息，包括11个必须的字段（mandatory fields）和一个可选的字段，字段之间用tag分割。

必须的字段有11个，顺序固定，不可用时，根据字段定义，可以为’0‘或者’*‘，这是11个字段包括：

1. QNAME，比对片段的（template）的编号；

2. FLAG，位标识，template mapping情况的数字表示，每一个数字代表一种比对情况，这里的值是符合情况的数字相加总和；
（picard专门有一个工具解读sam的flag:http://broadinstitute.github.io/picard/explain-flags.html）

```
1	The read is one of a pair  read是pair中的一条（read表示本条read，mate表示pair中的另一条read）
2	The alignment is one end of a proper paired-end alignment  pair一正一负完美的比对上
4	The read has no reported alignments  这条read没有比对上
8	The read is one of a pair and has no reported alignments  mate没有比对上
16	The alignment is to the reverse reference strand  这条read反向比对
32	The other mate in the paired-end alignment is aligned to the reverse reference strand   mate反向比对
64	The read is the first (#1) mate in a pair  这条read是read1
128	The read is the second (#2) mate in a pair  这条read是read2
```

3. RNAME，参考序列的编号，如果注释中对SQ-SN进行了定义，这里必须和其保持一致，另外对于没有mapping上的序列，这里是’*‘；

4. POS，比对上的位置，注意是从1开始计数，没有比对上，此处为0；

5. MAPQ，mappint的质量；

6. CIGAR，简要比对信息表达式（Compact Idiosyncratic Gapped Alignment Report），其以参考序列为基础，使用数字加字母表示比对结果，比如3S6M1P1I4M，前三个碱基被剪切去除了，然后6个比对上了，然后打开了一个缺口，有一个碱基插入，最后是4个比对上了，是按照顺序的；
```
M”表示 match或 mismatch；
“I”表示 insert；
“D”表示 deletion；
“N”表示 skipped（跳过这段区域）；
“S”表示 soft clipping（被剪切的序列存在于序列中）；
“H”表示 hard clipping（被剪切的序列不存在于序列中）；
“P”表示 padding；
“=”表示 match；
“X”表示 mismatch（错配，位置是一一对应的）；
```

7. RNEXT，下一个片段比对上的参考序列的编号，没有另外的片段，这里是’*‘，同一个片段，用’=‘；

8. PNEXT，下一个片段比对上的位置，如果不可用，此处为0；

9. TLEN，Template的长度，最左边得为正，最右边的为负，中间的不用定义正负，不分区段（single-segment)的比对上，或者不可用时，此处为0；

10. SEQ，序列片段的序列信息，如果不存储此类信息，此处为’*‘，注意CIGAR中M/I/S/=/X对应数字的和要等于序列长度；

11. QUAL，序列的质量信息，格式同FASTQ一样。read质量的ASCII编码。

12. 可选字段（optional fields)，格式如：TAG:TYPE:VALUE，其中TAG有两个大写字母组成，每个TAG代表一类信息，每一行一个TAG只能出现一次，TYPE表示TAG对应值的类型，可以是字符串、整数、字节、数组等。


#### SAM要处理好的问题 {-}

- 非常多序列（read)，mapping到多个参考基因组（reference）上
- 同一条序列，分多段（segment）比对到参考基因组上
- 无限量的，结构化信息表示，包括错配、删除、插入等比对信息


要注意的几个概念，以及与之对应的模型：


* reference
* read
* segment
* template（参考序列和比对上的序列共同组成的序列为template）
* alignment
* seq

参考链接:

https://en.wikipedia.org/wiki/SAM_(file_format)


## BAM 格式

> 本质上就是二进制压缩的SAM文件，大部分生物信息学流程都需要这个格式，为了节省存储空间已经方便索引。

```{r,warning=F,cache=T,message=F}
# BiocInstaller::biocLite('Rsamtools')
library(Rsamtools) 
test_bam_file <- 'data/CHIP-seq.bam' 
#fileter bam
filter <- FilterRules(list(MinWidth = function(x) width(x$seq) > 35))
res <- scanBam(test_bam_file, filter=filter)[[1]]
sapply(res, head)

``` 

从上面的例子可以看到BAM文件需要用特殊的方法来读取，可以是R里面的Rsamtools包，也可以是linux环境下安装好的samtools软件，因为它是二进制文件，不能像普通的文本文件那样来打开。

我们用R里面的head函数查看了该BAM文件的前6行，比对的flag分别是```16  0 16 16  0  0```,说明有3条序列没有成功比对到基因组。width信息说明该序列长度都是36bp。序列的碱基以及对应的碱基质量也如上所述。



## VCF

> Variant Call Format（VCF）是一个用于存储基因序列突变信息的文本格式。
可以表示单碱基突变, 插入/缺失, 拷贝数变异和结构变异等。
通常是对BAM文件格式的比对结果进行处理得到的。
BCF格式文件是VCF格式的二进制文件。我们就不再介绍BCF格式啦。

提到vcf就必须提到千人基因组计划，因为千人计划组才产生的vcf。生信菜鸟团有一篇博客《居然可以下载千人基因组计划的所有数据bam，vcf数据》
http://www.bio-info-trainee.com/1339.html
专门讲了千人计划的数据下载。

当然，所有的数据格式定义，都推荐大家看原汁原味的英文介绍，那个是金标准，我们的翻译只是为了促进大家的理解，如果有模棱两可的地方，以英文原文为准：https://samtools.github.io/hts-specs/VCFv4.2.pdf 

### 定义和示例 {-}

![vcf文件可以记录的基因组变异类型](image/C2/vcf_variation_class.jpg)

如上图所示，vcf记录的即为各类型的变异。例如：点突变，拷贝数变异，插入，缺失等结构变异。


VCF分为两部分，注释信息和变异位点记录信息。

注释信息通常以#开头，会描述该VCF版本，目前以4.2居多，然后会一行行记录变异位点信息里面会出现的所有TAG。

下面这个是NCBI的dbSNP数据库里面的人类的vcf文件的部分截取：

```
##fileformat=VCFv4.0
##fileDate=20160601
##source=dbSNP
##dbSNP_BUILD_ID=147
##reference=GRCh37.p13
##phasing=partial
##variationPropertyDocumentationUrl=ftp://ftp.ncbi.nlm.nih.gov/snp/specs/dbSNP_BitField_latest.pdf      
##INFO=<ID=RS,Number=1,Type=Integer,Description="dbSNP ID (i.e. rs number)">
##INFO=<ID=RSPOS,Number=1,Type=Integer,Description="Chr position reported in dbSNP">
##INFO=<ID=RV,Number=0,Type=Flag,Description="RS orientation is reversed">


#CHROM  POS     ID      REF     ALT     QUAL    FILTER  INFO
1       10177   rs201752861     A       C       .       .       RS=201752861;RSPOS=10177;dbSNPBuildID=137;SSR=0;SAO=0;VP=0x050000020005000002000100;GENEINFO=DDX11L1:100287102;WGT=1;VC=SNV;R5;ASP
1       10177   rs367896724     A       AC      .       .       RS=367896724;RSPOS=10177;dbSNPBuildID=138;SSR=0;SAO=0;VP=0x050000020005170026000200;GENEINFO=DDX11L1:100287102;WGT=1;VC=DIV;R5;ASP;VLD;G5A;G5;KGPhase3;CAF=0.5747,0.4253;COMMON=1
1       10228   rs143255646     TA      T       .       .       RS=143255646;RSPOS=10229;dbSNPBuildID=134;SSR=0;SAO=0;VP=0x050000020005000002000200;GENEINFO=DDX11L1:100287102;WGT=1;VC=DIV;R5;ASP
1       10228   rs200462216     TAACCCCTAACCCTAACCCTAAACCCTA    T       .       .       RS=200462216;RSPOS=10229;dbSNPBuildID=137;SSR=0;SAO=0;VP=0x050000020005000002000200;GENEINFO=DDX11L1:100287102;WGT=1;VC=DIV;R5;ASP
1       10230   rs775928745     AC      A       .       .       RS=775928745;RSPOS=10231;dbSNPBuildID=144;SSR=0;SAO=0;VP=0x050000020005000002000200;GENEINFO=DDX11L1:100287102;WGT=1;VC=DIV;R5;ASP
1       10231   rs200279319     C       A       .       .       RS=200279319;RSPOS=10231;dbSNPBuildID=137;SSR=0;SAO=0;VP=0x050000020005000002000100;GENEINFO=DDX11L1:100287102;WGT=1;VC=SNV;R5;ASP
1       10234   rs145599635     C       T       .       .       RS=145599635;RSPOS=10234;dbSNPBuildID=134;SSR=0;SAO=0;VP=0x050100020005000002000100;GENEINFO=DDX11L1:100287102;WGT=1;VC=SNV;SLO;R5;ASP
1       10235   rs540431307     T       TA      .       .       RS=540431307;RSPOS=10235;dbSNPBuildID=142;SSR=0;SAO=0;VP=0x050000020005040024000200;GENEINFO=DDX11L1:100287102;WGT=1;VC=DIV;R5;ASP;VLD;KGPhase3;CAF=0.9988,0.001198;COMMON=0
1       10247   rs796996180     T       C       .       .       RS=796996180;RSPOS=10247;dbSNPBuildID=146;SSR=0;SAO=0;VP=0x050100020005000002000100;GENEINFO=DDX11L1:100287102;WGT=1;VC=SNV;SLO;R5;ASP
```

限于文章篇幅限制，我只是截取了该VCF文件的部分注释信息，很明显可以看到注释信息刚刚开始的几行其实是没有规则的，只需要以##开头即可，描述一些必备信息，包括参考基因组版本，得到该VCF文件的命令是什么等等。

后面的都是以INFO=<ID=······>的形式来介绍一个个TAG，这些TAG都是会在VCF的正文，变异位点记录里面用到的。而且每个tag都很容易理解，就是对应的英文描述而已。



接下来我们看看比较复杂的正文部分，就是变异位点记录信息。

```
#CHROM  POS     ID      REF     ALT     QUAL    FILTER  INFO    FORMAT  sample1
1       858691  .       TG      T       222     .       INDEL;IDV=37;IMF=0.486842;DP=76;VDB=0.110516;SGB=-0.693139;MQSB=1;MQ0F=0;ICB=1;HOB=0.5;AC=1;AN=2;DP4=12,24,14,22;MQ=60  GT:PL   0/1:255,0,255
1       858801  .       A       G       222     .       DP=59;VDB=0.728126;SGB=-0.692717;RPB=0.748623;MQB=1;MQSB=1;BQB=0.963908;MQ0F=0;ICB=1;HOB=0.5;AC=1;AN=2;DP4=14,17,11,12;MQ=60    GT:PL   0/1:255,0,255
1       859404  .       C       G       222     .       DP=81;VDB=0.0896228;SGB=-0.693132;RPB=0.849598;MQB=1;MQSB=1;BQB=0.486963;MQ0F=0;ICB=1;HOB=0.5;AC=1;AN=2;DP4=17,15,18,16;MQ=60   GT:PL   0/1:255,0,255
1       859690  .       C       G       222     .       DP=75;VDB=0.0662538;SGB=-0.69312;RPB=0.959181;MQB=1;MQSB=1;BQB=0.962588;MQ0F=0;ICB=1;HOB=0.5;AC=1;AN=2;DP4=20,15,18,14;MQ=60    GT:PL   0/1:255,0,255
1       859701  .       C       G       222     .       DP=74;VDB=0.274853;SGB=-0.693127;RPB=0.97201;MQB=1;MQSB=1;BQB=0.717302;MQ0F=0;ICB=1;HOB=0.5;AC=1;AN=2;DP4=19,15,19,14;MQ=60     GT:PL   0/1:255,0,255
1       859913  .       A       G       222     .       DP=67;VDB=0.756546;SGB=-0.693139;RPB=0.950685;MQB=1;MQSB=1;BQB=0.662934;MQ0F=0;ICB=1;HOB=0.5;AC=1;AN=2;DP4=18,10,19,17;MQ=60    GT:PL   0/1:255,0,255
1       860416  .       G       A       222     .       DP=79;VDB=0.673886;SGB=-0.693144;RPB=0.11919;MQB=1;MQSB=1;BQB=0.992984;MQ0F=0;ICB=1;HOB=0.5;AC=1;AN=2;DP4=18,15,24,15;MQ=60     GT:PL   0/1:255,0,255

```

每一行代表一个Variant的信息。

CHROM 和 POS：代表参考序列名和variant的位置；如果是INDEL的话，位置是INDEL的第一个碱基位置。

ID：variant的ID。比如在dbSNP中有该SNP的id，则会在此行给出(这个需要自己下载dbSNP数据库文件进行注释才有的)。
若没有或者注释不上，则用’.'表示其为一个novel variant。

REF 和 ALT：参考序列的碱基 和 Variant的碱基。

QUAL：Phred格式(Phred_scaled)的质量值，表示在该位点存在variant的可能性；该值越高，则variant的可能性越大；
计算方法：Phred值 = -10 * log (1-p) p为variant存在的概率;
通过计算公式可以看出值为10的表示错误概率为0.1，该位点为variant的概率为90%。

FILTER：使用上一个QUAL值来进行过滤的话，是不够的。GATK能使用其它的方法来进行过滤，过滤结果中通过则该值为”PASS”;若variant不可靠，则该项不为”PASS”或”.”。

INFO： 这一行是variant的详细信息，内容很多，以下再具体详述。

FORMAT 和 sample1 ：这两行合起来提供了 sample1 这个sample的基因型的信息。’sample1′代表这该名称的样品，是由SAM/BAM文件中的@RG下的 SM 标签决定的。
(当然并不是所有的VCF都是由一个BAM文件产生，比如数据库dbSNP提供的vcf文件，就没有样本信息啦)

### 第8列的INFO {-}

该列信息最多了，都是以 “TAG=Value”, 并使用”;”分隔的形式 。 其中 很多的注释信息在VCF文件的头部注释中给出。以下是这些TAG的解释

AC，AF 和 AN：AC(Allele Count) 表示该Allele的数目；AF(Allele Frequency) 表示Allele的频率； AN(Allele Number) 表示Allele的总数目。对于1个diploid sample而言：则基因型 0/1 表示sample为杂合子，Allele数为1(双倍体的sample在该位点只有1个等位基因发生了突变)，Allele的频率为0.5(双倍体的 sample在该位点只有50%的等位基因发生了突变)，总的Allele为2； 基因型 1/1 则表示sample为纯合的，Allele数为2，Allele的频率为1，总的Allele为2。

DP：reads覆盖度。是一些reads被过滤掉后的覆盖度。

Dels：Fraction of Reads Containing Spanning Deletions。进行SNP和INDEL calling的结果中，有该TAG并且值为0表示该位点为SNP，没有则为INDEL。

FS：使用Fisher’s精确检验来检测strand bias而得到的Fhred格式的p值。该值越小越好。一般进行filter的时候，可以设置 FS < 10～20。

HaplotypeScore：Consistency of the site with at most two segregating haplotypes

InbreedingCoeff：Inbreeding coefficient as estimated from the genotype likelihoods per-sample when compared against the Hard-Weinberg expectation

MLEAC：Maximum likelihood expectation (MLE) for the allele counts (not necessarily the same as the AC), for each ALT allele, in the same order as listed

MLEAF：Maximum likelihood expectation (MLE) for the allele frequency (not necessarily the same as the AF), for each ALT alle in the same order as listed

MQ：RMS Mapping Quality

MQ0：Total Mapping Quality Zero Reads

MQRankSum：Z-score From Wilcoxon rank sum test of Alt vs. Ref read mapping qualities

QD：Variant Confidence/Quality by Depth

RPA：Number of times tandem repeat unit is repeated, for each allele (including reference)

RU：Tandem repeat unit (bases)

ReadPosRankSum：Z-score from Wilcoxon rank sum test of Alt vs. Ref read position bias

STR：Variant is a short tandem repeat

### 9和10列代表基因型 {-}

```
GT:AD:DP:GQ:PL 0/1:173,141:282:99:255,0,255 
GT:AD:DP:GQ:PL 0/1:1,3:4:25.92:103,0,26

```
看上面最后两列数据，这两列数据是对应的，前者为格式，后者为格式对应的数据。这些TAG也是可以在VCF的头文件找到的

GT：样品的基因型（genotype）。两个数字中间用’/'分开，这两个数字表示双倍体的sample的基因型。0 表示样品中有ref的allele； 1 表示样品中variant的allele； 2表示有第二个variant的allele。因此： 0/0 表示sample中该位点为纯合的，和ref一致； 0/1 表示sample中该位点为杂合的，有ref和variant两个基因型； 1/1 表示sample中该位点为纯合的，和variant一致。

AD 和 DP：AD(Allele Depth)为sample中每一种allele的reads覆盖度,在diploid中则是用逗号分割的两个值，前者对应ref基因型，后者对应variant基因型；
 
DP（Depth）为sample中该位点的测序深度。

GQ：基因型的质量值(Genotype Quality)。Phred格式(Phred_scaled)的质量值，表示在该位点该基因型存在的可能性；该值越高，则Genotype的可能性越大；计算方法：Phred值 = -10 * log (1-p) p为基因型存在的概率。

PL：指定的三种基因型的质量值(provieds the likelihoods of the given genotypes)。这三种指定的基因型为(0/0,0/1,1/1)，这三种基因型的概率总和为1。和之前不一致，该值越大，表明为该种基因型的可能性越小。 Phred值 = -10 * log (p) p为基因型存在的概率。

![VCF文件的官方描述](image/C2/vcf_file_format.jpg)

上图可以帮助很好的理解vcf格式。



注意事项：
　　

针对vcf格式有如bcftools等软件进行处理。

生信菜鸟团的博客和生信技能树里面都介绍了很多处理vcf的软件。
　　

强烈推荐去看《生信技能树》中的：我的基因组28 
https://en.wikipedia.org/wiki/Variant_Call_Format

## GTF和GFF

GFF全称为general feature format，这种格式主要是用来注释基因组。

GTF全称为gene transfer format，主要是用来对基因进行注释。

GTF和GFF格式是Sanger研究所定义，是一种简单的、方便的对于DNA、RNA以及蛋白质序列的特征进行描述的一种数据格式.
比如序列的哪里到哪里是基因，是转录本，是外显子，内含子或者CDS等等，已经成为序列注释的通用格式，许多软件都支持输入或者输出gff格式。


### 定义和示例 {-}

gff由tab键隔开的9列组成，以下是各列的说明：

```
Column 1: “seqid”
序列的编号，编号的有效字符[a-zA-Z0-9.:^*$@!+_?-|]
Column 2: “source”
注释信息的来源，比如”Genescan”、”Genbank” 等，可以为空，为空用”.”点号代替
Column 3: “type”
注释信息的类型，比如Gene、cDNA、mRNA等，或者是SO对应的编号
Columns 4 & 5: “start” and “end”
开始与结束的位置，注意计数是从1开始的。结束位置不能大于序列的长度
Column 6: “score”
得分，数字，是注释信息可能性的说明，可以是序列相似性比对时的E-values值或者基因预测是的P-values值。”.”表示为空。
Column 7: “strand”
序列的方向， +表示正义链, -反义链 , ? 表示未知.
Column 8: “phase”
仅对注释类型为 “CDS”有效，表示起始编码的位置，有效值为0、1、2。
Column 9: “attributes”
以多个键值对组成的注释信息描述，键与值之间用”=“，不同的键值用”;“隔开，一个键可以有多个值，不同值用”,“分割。注意如果描述中包括tab键以及”,=;”，要用URL转义规则进行转义，如tab键用 %09代替。键是区分大小写的，以大写字母开头的键是预先定义好的，在后面可能被其他注释信息所调用。
```

预先定义的键包括：
```
ID 注释信息的编号，在一个GFF文件中必须唯一；
Name 注释信息的名称，可以重复；
Alias 别名
Parent Indicates 该注释所属的注释，值为注释信息的编号，比如外显子所属的转录组编号，转录组所属的基因的编号。值可以为多个。
Target Indicates： the target of a nucleotide-to-nucleotide or protein-to-nucleotide alignment.（核苷酸对核苷酸或蛋白质至核苷酸比对的靶点。）
Gap：The alignment of the feature to the target if the two are not collinear (e.g. contain gaps).（如果两者不共线（例如包含间隙），则该特征与目标的对准）
Derives_from：Used to disambiguate the relationship between one feature and another when the relationship is a temporal one rather than a purely structural “part of” one. This is needed for polycistronic genes.（用于消除一个特征与另一个特征之间的关系，当关系是一个时间的关系，而不是纯粹的结构“一部分”时。 这是多顺反子基因所必需的。）
Note： 备注
Dbxref ：数据库索引
Ontology_term： A cross reference to an ontology term.
```

下面是一个简单的实例：

```
##gff-version 3
##sequence-region ctg123 1 1497228
ctg123 . gene 1000 9000 . + . ID=gene00001;Name=EDEN
ctg123 . TF_binding_site 1000 1012 . + . Parent=gene00001
ctg123 . mRNA 1050 9000 . + . ID=mRNA00001;Parent=gene00001
ctg123 . mRNA 1050 9000 . + . ID=mRNA00002;Parent=gene00001 ctg123 . mRNA 1300 9000 . + . ID=mRNA00003;Parent=gene00001 ctg123 . exon 1300 1500 . + . Parent=mRNA00003
ctg123 . exon 1050 1500 . + . Parent=mRNA00001,mRNA00002
ctg123 . exon 3000 3902 . + . Parent=mRNA00001,mRNA00003
ctg123 . exon 5000 5500 . + . Parent=mRNA00001,mRNA00002,mRNA00003
ctg123 . exon 7000 9000 . + . Parent=mRNA00001,mRNA00002,mRNA00003
ctg123 . CDS 1201 1500 . + 0 ID=cds00001;Parent=mRNA00001
ctg123 . CDS 3000 3902 . + 0 ID=cds00001;Parent=mRNA00001
ctg123 . CDS 5000 5500 . + 0 ID=cds00001;Parent=mRNA00001
ctg123 . CDS 7000 7600 . + 0 ID=cds00001;Parent=mRNA00001
ctg123 . CDS 1201 1500 . + 0 ID=cds00002;Parent=mRNA00002
ctg123 . CDS 5000 5500 . + 0 ID=cds00002;Parent=mRNA00002
ctg123 . CDS 7000 7600 . + 0 ID=cds00002;Parent=mRNA00002
ctg123 . CDS 3301 3902 . + 0 ID=cds00003;Parent=mRNA00003
ctg123 . CDS 5000 5500 . + 2 ID=cds00003;Parent=mRNA00003
ctg123 . CDS 7000 7600 . + 2 ID=cds00003;Parent=mRNA00003
ctg123 . CDS 3391 3902 . + 0 ID=cds00004;Parent=mRNA00003
ctg123 . CDS 5000 5500 . + 2 ID=cds00004;Parent=mRNA00003
ctg123 . CDS 7000 7600 . + 2 ID=cds00004;Parent=mRNA00003
```
可以看到第9列其实是可以无限扩展的，只需要以封号进行分割即可。

### GTF 与GFF的差异 {-}

GTF文件以及GFF文件都由9列数据组成，这两种文件的前8列都是相同的（一些小的差别），它们的差别重点在第9列。

GTF文件的第9列同GFF文件不同，虽然同样是标签与值配对的情况，但标签与值之间以空格分开,而不是GFF里面的=号
且每个特征之后都要有分号，（包括最后一个特征）.
下面看一个GTF的实例：

```
17	havana	five_prime_utr	7687377	7687427	.	-	.	gene_id "ENSG00000141510"; gene_version "16"; transcript_id "ENST00000503591"; transcript_version "1"; gene_name "TP53"; gene_source "ensembl_havana"; gene_biotype "protein_coding"; havana_gene "OTTHUMG00000162125"; havana_gene_version "10"; transcript_name "TP53-003"; transcript_source "havana"; transcript_biotype "protein_coding"; havana_transcript "OTTHUMT00000367399"; havana_transcript_version "2"; tag "cds_end_NF"; tag "mRNA_end_NF"; transcript_support_level "5";
17	havana	five_prime_utr	7677325	7677427	.	-	.	gene_id "ENSG00000141510"; gene_version "16"; transcript_id "ENST00000503591"; transcript_version "1"; gene_name "TP53"; gene_source "ensembl_havana"; gene_biotype "protein_coding"; havana_gene "OTTHUMG00000162125"; havana_gene_version "10"; transcript_name "TP53-003"; transcript_source "havana"; transcript_biotype "protein_coding"; havana_transcript "OTTHUMT00000367399"; havana_transcript_version "2"; tag "cds_end_NF"; tag "mRNA_end_NF"; transcript_support_level "5";
17	havana	five_prime_utr	7676595	7676622	.	-	.	gene_id "ENSG00000141510"; gene_version "16"; transcript_id "ENST00000503591"; transcript_version "1"; gene_name "TP53"; gene_source "ensembl_havana"; gene_biotype "protein_coding"; havana_gene "OTTHUMG00000162125"; havana_gene_version "10"; transcript_name "TP53-003"; transcript_source "havana"; transcript_biotype "protein_coding"; havana_transcript "OTTHUMT00000367399"; havana_transcript_version "2"; tag "cds_end_NF"; tag "mRNA_end_NF"; transcript_support_level "5";

```

速记口诀

```
* GTF,GFF 9列数据来组成*
*结果前8列都相同*
*GFF 9列间以tab 来分割，第9列用等号*
*GTF不服气，标签与值用空格分离，特征之间也用分号分开;要问特立独行谁最牛，GTF排第一！*
```

有趣小故事：

从前双胞胎兄弟，GTF,GFF。GFF是哥哥，GTF是弟弟，性别相同，都是由用来注释基因的，都是9个器官~~哥哥呢，比较随和，哥哥器官就用tab分开，然后第九个一对就划等号！

GTF是弟弟，天天心里想”WTF“! 不服气，就要特立独行！然后不服气，9个器官有八个都与哥哥相同，第9个子集偏不，就用了空格把它分成两块~~各个器官之间也是用分号分开！！预示子集跟各个不一样，自己的独立性！

目前两种文件可以方便的相互转化，比如:使用Cufflinks软件的 的gffread。

参考链接
http://genome.ucsc.edu/goldenPath/help/customTrack.html#GTF
http://blog.sina.com.cn/s/blog_8a4f556e0102yd3l.html

##  GenePred

> 这种格式主要用在基因浏览器中基因预测的track。如果有可变剪切的情况，那表格的每一行就是一个 transcript 的全部信息。

### 定义和示例 {-}

每一行的具体解释如下
```
table genePred
"A gene prediction."
    (
    string  name;               "Name of gene"
    string  chrom;              "Chromosome name"
    char[1] strand;             "+ or - for strand"
    uint    txStart;            "Transcription start position"
    uint    txEnd;              "Transcription end position"
    uint    cdsStart;           "Coding region start"
    uint    cdsEnd;             "Coding region end"
    uint    exonCount;          "Number of exons"
    uint[exonCount] exonStarts; "Exon start positions"
    uint[exonCount] exonEnds;   "Exon end positions"
    )
```

如果觉得抽象，我们可以用示例来进行一下对比。小编在这里首先将模式植物拟南芥的gtf文件转化为gpd格式。`head -n 1` 看一下gpd文件第一行的样式。

```
#gpd
AT1G01010.1	Chr1	+	3630	5899	3759	5630	6	3630,3995,4485,4705,5173,5438,	3913,4276,4605,5095,5326,5899
```

再来`grep`一下gtf文件中有**AT1G01010.1**信息的那些行是什么样

```
#gtf
Chr1	Araport11	5UTR	3631	3759	.	+	.	transcript_id "AT1G01010.1"; gene_id "AT1G01010"
Chr1	Araport11	exon	3631	3913	.	+	.	transcript_id "AT1G01010.1"; gene_id "AT1G01010"
Chr1	Araport11	start_codon	3760	3762	.	+	.	transcript_id "AT1G01010.1"; gene_id "AT1G01010"
Chr1	Araport11	CDS	3760	3913	.	+	0	transcript_id "AT1G01010.1"; gene_id "AT1G01010"
Chr1	Araport11	exon	3996	4276	.	+	.	transcript_id "AT1G01010.1"; gene_id "AT1G01010"
Chr1	Araport11	CDS	3996	4276	.	+	2	transcript_id "AT1G01010.1"; gene_id "AT1G01010"
Chr1	Araport11	exon	4486	4605	.	+	.	transcript_id "AT1G01010.1"; gene_id "AT1G01010"
Chr1	Araport11	CDS	4486	4605	.	+	0	transcript_id "AT1G01010.1"; gene_id "AT1G01010"
Chr1	Araport11	exon	4706	5095	.	+	.	transcript_id "AT1G01010.1"; gene_id "AT1G01010"
Chr1	Araport11	CDS	4706	5095	.	+	0	transcript_id "AT1G01010.1"; gene_id "AT1G01010"
Chr1	Araport11	exon	5174	5326	.	+	.	transcript_id "AT1G01010.1"; gene_id "AT1G01010"
Chr1	Araport11	CDS	5174	5326	.	+	0	transcript_id "AT1G01010.1"; gene_id "AT1G01010"
Chr1	Araport11	CDS	5439	5630	.	+	0	transcript_id "AT1G01010.1"; gene_id "AT1G01010"
Chr1	Araport11	exon	5439	5899	.	+	.	transcript_id "AT1G01010.1"; gene_id "AT1G01010"
Chr1	Araport11	stop_codon	5628	5630	.	+	.	transcript_id "AT1G01010.1"; gene_id "AT1G01010"
Chr1	Araport11	3UTR	5631	5899	.	+	.	transcript_id "AT1G01010.1"; gene_id "AT1G01010"
```

我们可以看出，在GTF中，AT1G01010.1这个transcript共有6个CDS，那么对应到相应gpd文件AT1G01010.1这一行的第8列就是6，而第9列和第10列则是这6个CDS对应的起始和终止位置。

细心的朋友可能会发现，GTF文件中CDS起始位置在GenePred table中统统少了1，这其实就是两种文件的起始位置从1开始还是从0开始计数的区别。

GTF的产生和流行有其历史的原因。但是从技术角度来讲，这个文件格式是个非常差劲的格式。

GTF格式非常冗余。以人类转录组为例，Gencode V22的GTF文件为1.2G，压缩之后只有40M。大家知道压缩软件的压缩比是和软件的冗余程度。很少有文件能够压缩到1/30的大小。可见GTF格式多么冗余。GTF格式（及其早期版本GFF等）有很好的替代格式。从信息量上来讲：GTF 等价于 GenePred （Bed12) + Gene_Anno_table。

GenePred是Jimmy Kent创建UCSC genome browser的时候建立的文件格式。UCSC的文件格式定义是非常smart的，包括之后我可能会讲到的2bit，bigwig格式。


### GTF vs GenePred {-}

- 从文件大小上来看，压缩前：GTF（1.2G) >> Genepred(23M) + Gene_Anno_table (2.8M)。压缩后：GTF(40M) >> GenePred(7.8M) +Gene_Anno_table (580K)
- 从可读性来讲，GTF是以gene interval 为单位（行），每行可以是gene，transcript，exon，intron，utr等各种信息，看起来什么都在里面，很全面，其实可读性非常差，而且容易产生各种错误。GenePred格式是以transcript为单位，每行就是一个transcript，简洁直观。
- 从程序处理的角度来讲：以GTF文件作为输入的程序，如果换成以GenePred格式为输入，编程的难度会降低一个数量级，运算时间会快很多，代码的可读性强很多。


GTF 转换成GenePred：

```
gtfToGenePred -genePredExt -ignoreGroupsWithoutExons -geneNameAsName2 test.gtf test.gpd
```

Gene_Anno_table: 其实就是把GTF的所有transcript行的第9列转换变成一个表格。


## BED

> BED 文件格式提供了一种灵活的方式来定义的数据行，以用来描述注释的信息。
跟GTF/GFF格式一样，也可以用来描述基因组特征。但没有GTF/GFF格式那么正规，通常用来描述任何人为定义的区间。
但没有GTF/GFF格式那么正规，通常用来描述任何人为定义的区间。
所以BED格式最重要的就是染色体加上起始终止坐标这3列。


### 定义和示例 {-}

BED行有3个必须的列和9个额外可选的列。 每行的数据格式要求一致。

#### 必须包含的3列是： {-}

1. chrom, 染色体或scafflold 的名字(eg chr3， chrY, chr2_random, scaffold0671 )

2. chromStart 染色体或scaffold的起始位置，染色体第一个碱基的位置是0

3. chromEnd 染色体或scaffold的结束位置，染色体的末端位置没有包含到显示信息里面。例如，首先得100个碱基的染色体定义为chromStart =0 . chromEnd=100, 碱基的数目是0-99
 
#### 9 个额外的可选列:
　
4. name 指定BED行的名字，这个名字标签会展示在基因组浏览器中的bed行的左侧。

5. score 0到1000的分值，如果在注释数据的设定中将原始基线设置为１，那么这个分值会决定现示灰度水平（数字越大，灰度越高）

6. strand 定义链的方向，''+” 或者”-”

7. thickStart 起始位置（The starting position at which the feature is drawn thickly）(例如，基因起始编码位置）

8. thickEnd 终止位置（The ending position at which the feature is drawn thickly）（例如：基因终止编码位置）　

9. itemRGB 是一个RGB值的形式, R, G, B (eg. 255, 0, 0), 如果itemRgb设置为'On”, 这个RBG值将决定数据的显示的颜色。

10. blockCount BED行中的block数目，也就是外显子数目

11. blockSize 用逗号分割的外显子的大小,这个item的数目对应于BlockCount的数目

12.  blockStarts- 用逗号分割的列表, 所有外显子的起始位置，数目也与blockCount数目对应.

一个简单的示例如下：

```
track name=pairedReads description="Clone Paired Reads" useScore=1
chr22 1000 5000 cloneA 960 + 1000 5000 0 2 567,488, 0,3512
chr22 2000 6000 cloneB 900 - 2000 6000 0 2 433,399, 0,3601
```

bed格式有相应的软件来处理这类格式的文件，如bedtools。

* 注意：用于在GBrowse上展示相关注释的bed格式通常第一行有一个关于track的描述信息。

 
 速记：
 * bed不是床，缺了主要3列就得黄~
 * 9列可选列，看了不会胡略略~~
       
 

### BED 与GFF的差异 {-}

BED文件中起始坐标为0，结束坐标至少是1,； 

GFF中起始坐标是1而结束坐标至少是1。


参考链接：

http://5527lok.blog.163.com/blog/static/64751582011530113134590/
http://blog.sciencenet.cn/home.php?mod=space&uid=442719&do=blog&id=721452
https://www.plob.org/article/3748.html
http://ju.outofmemory.cn/entry/193943
https://en.wikipedia.org/wiki/General_feature_format




## MAF

MAF格式本来并不是一个常见的文本文件格式，只是因为癌症研究实在是太热门了，对它的理解也变得需求旺盛起来了。

### MAF的说明  {-}

这些文件应该使用下面描述的突变注释格式（MAF）进行格式化。另外下文中有文件命名规范。

以下几种类型的体细胞突变会在MAF文件中出现：
*   错义突变及无义突变
*	剪接位点，其定义为剪接位点2 bp以内的SNP
*	沉默突变
*	与基因的编码区、剪接位点或遗传元件目标区域重叠的引物。
*	移码突变
*	调控区突变

大部分MAF提交提交的是原始数据。这些原始数据中在体细胞中标记的位点与已知的变异类型相重合的。为避免有可能出现的细胞系污染，MAF规定了一定的下细胞过滤标准。根据现行政策，可开放获取MAF资料应满足：

*	包括所有已验证的体细胞突变名称
*	包括与编码区域或剪接位点重叠的所有未验证的体细胞突变名称
*	排除所有其他类型的突变（即非体细胞突变、不在编码区域或剪接位点的未验证体的细胞突变以及未在dbSNP、COSMIC或OMIM中注释为体细胞的dbSNP位点）

我们提交给DCC MAF存档的数据包括两种：Somatic MAF（named .somatic.maf）的开放访问数据以及不经过筛选的包含原始数据的Protected MAF（named.protected.maf）。所有数据将使用MAF标准进行格式化。 


### MAF文件字段 {-}

MAF文件的格式是以制表符分隔的列。这些列都在表1中进行了详细注释，每个MAF文件中都必须按照相应格式进行处理，DCC将验证每列的顺序是否符合标注。每列的标题和值有时需要区分大小写。列中允许出现空值（即空白单元格）或枚举值。验证器将查找是否存在一个符合规范（例如#version 2.4）的标题，如果没有，验证将会失败。除了出现在表1中的列外，也可以选择附加其他列。可选列不经过DCC验证，可以按任何顺序进行。

MAF文件可能有两种格式 ，可能是47列，或者120列，第一行一般都是 头文件，注释着每一列的信息，的确，信息量有点略大。如下：

```
     1	Hugo_Symbol
     2	Entrez_Gene_Id
     3	Center
     4	NCBI_Build
     5	Chromosome
     6	Start_Position
     7	End_Position
     8	Strand
     9	Consequence
    10	Variant_Classification
    11	Variant_Type
    12	Reference_Allele
    13	Tumor_Seq_Allele1
    14	Tumor_Seq_Allele2
    15	dbSNP_RS
    16	dbSNP_Val_Status
    17	Tumor_Sample_Barcode
    18	Matched_Norm_Sample_Barcode
    19	Match_Norm_Seq_Allele1
    20	Match_Norm_Seq_Allele2
    21	Tumor_Validation_Allele1
    22	Tumor_Validation_Allele2
    23	Match_Norm_Validation_Allele1
    24	Match_Norm_Validation_Allele2
    25	Verification_Status
    26	Validation_Status
    27	Mutation_Status
    28	Sequencing_Phase
    29	Sequence_Source
    30	Validation_Method
    31	Score
    32	BAM_File
    33	Sequencer
    34	t_ref_count
    35	t_alt_count
    36	n_ref_count
    37	n_alt_count
    38	HGVSc
    39	HGVSp
    40	HGVSp_Short
    41	Transcript_ID
    42	RefSeq
    43	Protein_position
    44	Codons
    45	Hotspot
    46	cDNA_change
    47	Amino_Acid_Change
```

```
     1	Hugo_Symbol
     2	Entrez_Gene_Id
     3	Center
     4	NCBI_Build
     5	Chromosome
     6	Start_Position
     7	End_Position
     8	Strand
     9	Variant_Classification
    10	Variant_Type
    11	Reference_Allele
    12	Tumor_Seq_Allele1
    13	Tumor_Seq_Allele2
    14	dbSNP_RS
    15	dbSNP_Val_Status
    16	Tumor_Sample_Barcode
    17	Matched_Norm_Sample_Barcode
    18	Match_Norm_Seq_Allele1
    19	Match_Norm_Seq_Allele2
    20	Tumor_Validation_Allele1
    21	Tumor_Validation_Allele2
    22	Match_Norm_Validation_Allele1
    23	Match_Norm_Validation_Allele2
    24	Verification_Status
    25	Validation_Status
    26	Mutation_Status
    27	Sequencing_Phase
    28	Sequence_Source
    29	Validation_Method
    30	Score
    31	BAM_File
    32	Sequencer
    33	Tumor_Sample_UUID
    34	Matched_Norm_Sample_UUID
    35	HGVSc
    36	HGVSp
    37	HGVSp_Short
    38	Transcript_ID
    39	Exon_Number
    40	t_depth
    41	t_ref_count
    42	t_alt_count
    43	n_depth
    44	n_ref_count
    45	n_alt_count
    46	all_effects
    47	Allele
    48	Gene
    49	Feature
    50	Feature_type
    51	One_Consequence
    52	Consequence
    53	cDNA_position
    54	CDS_position
    55	Protein_position
    56	Amino_acids
    57	Codons
    58	Existing_variation
    59	ALLELE_NUM
    60	DISTANCE
    61	TRANSCRIPT_STRAND
    62	SYMBOL
    63	SYMBOL_SOURCE
    64	HGNC_ID
    65	BIOTYPE
    66	CANONICAL
    67	CCDS
    68	ENSP
    69	SWISSPROT
    70	TREMBL
    71	UNIPARC
    72	RefSeq
    73	SIFT
    74	PolyPhen
    75	EXON
    76	INTRON
    77	DOMAINS
    78	GMAF
    79	AFR_MAF
    80	AMR_MAF
    81	ASN_MAF
    82	EAS_MAF
    83	EUR_MAF
    84	SAS_MAF
    85	AA_MAF
    86	EA_MAF
    87	CLIN_SIG
    88	SOMATIC
    89	PUBMED
    90	MOTIF_NAME
    91	MOTIF_POS
    92	HIGH_INF_POS
    93	MOTIF_SCORE_CHANGE
    94	IMPACT
    95	PICK
    96	VARIANT_CLASS
    97	TSL
    98	HGVS_OFFSET
    99	PHENO
   100	MINIMISED
   101	ExAC_AF
   102	ExAC_AF_Adj
   103	ExAC_AF_AFR
   104	ExAC_AF_AMR
   105	ExAC_AF_EAS
   106	ExAC_AF_FIN
   107	ExAC_AF_NFE
   108	ExAC_AF_OTH
   109	ExAC_AF_SAS
   110	GENE_PHENO
   111	FILTER
   112	CONTEXT
   113	src_vcf_id
   114	tumor_bam_uuid
   115	normal_bam_uuid
   116	case_id
   117	GDC_FILTER
   118	COSMIC
   119	MC3_Overlap
   120	GDC_Validation_Status
```
 
 
```
重要标准
列表中每列的顺序最好与索引列相同。
标有“Case Sensitive“的列所有标题都需要区分大小写。
标有”Null“的列表示允许具有空值。
标有“Enumerated column”的列表示具有指定的值，比如“Enumerated value” 是"No"表示该列没有指定的值；其他值表示允许列出的具体值; “Set”表示该列的值来自指定的一组已知值（例如HUGO基因符号）
```
##### MAF文件检查 {-}

DCC 档案验证器将检查MAF文件的完整性。如果MAF文件中的任何一项出现错误，验证将失败：

```
1.	列标题文本（包括大小写）和顺序必须与表1完全一致
2.	表1中列出的列标题下的值不是空值时必须具有相应的值
3.	表1中指定为“Case Sensitive”的值必须区分大小写。
4.	如果列标题在规范中列出为具有枚举值（即“Enumerated”列的“Yes”），则这些列中的值必须来自“Enumerated”下列出的枚举值。
5.	如果规范中列标题具有设置值（即“Enumerated”列的“Set”），则那些列下的值必须来自该域的枚举值（例如，HUGO基因符号）。
6.	所有Allele-based列必须包含 –(deletion)或由以下大写字母组成的字符串：A，T，G，C。
7.	如果Validation_Status == "Untested" 那么Tumor_Validation_Allele1, Tumor_Validation_Allele2, Match_Norm_Validation_Allele1, Match_Norm_Validation_Allele2 可以是空值(由 Validation_Status决定).
a)	如果Validation_Status == "Inconclusive" 那么Tumor_Validation_Allele1, Tumor_Validation_Allele2, Match_Norm_Validation_Allele1, Match_Norm_Validation_Allele2 可以是空值(由 Validation_Status决定)
8.	如果Validation_Status == Valid, 那么Validated_Tumor_Allele1 and Validated_Tumor_Allele2一定要是 A, C, G, T, 或“-“中的一种

a)	如果Validation_Status  == "Valid" 那么Tumor_Validation_Allele1, Tumor_Validation_Allele2, Match_Norm_Validation_Allele1, Match_Norm_Validation_Allele2 不可以是空值

b)	 如果Validation_Status == "Invalid" 那么Tumor_Validation_Allele1, Tumor_Validation_Allele2, Match_Norm_Validation_Allele1, Match_Norm_Validation_Allele2 不可以是空值Tumor_Validation_Allelle1 == Match_Norm_Validation_Allele1 
Tumor_Validation_Allelle2 == Match_Norm_Validation_Allele2  (出现错误时，增加以替代8a)

9.	检查Mutation_Status的等位基因值:
检查Validation_status的等位基因值:

a)	如果Mutation_Status == "Germline" and Validation_Status == "Valid", 那么Tumor_Validation_Allele1 == Match_Norm_Validation_Allele1 Tumor_Validation_Allele2 == Match_Norm_Validation_Allele2.

b)	如果Mutation_Status == "Somatic" ，Validation_Status == "Valid", 那么Match_Norm_Validation_Allele1 == Match_Norm_Validation_Allele2 == Reference_Allele 且(Tumor_Validation_Allele1 or Tumor_Validation_Allele2) != Reference_Allele

c)	如果Mutation_Status == "LOH" and Validation_Status=="Valid", 那么Tumor_Validation_Allele1 == Tumor_Validation_Allele2 Match_Norm_Validation_Allele1 != Match_Norm_Validation_Allele2 and Tumor_Validation_Allele1 == (Match_Norm_Validation_Allele1 or Match_Norm_Validation_Allele2).
10.	检查 Start_position <= End_position
11.	根据Variant_Type检查Start_position和End_position: 
a)	如果Variant_Type == "INS", 那么(End_position - Start_position + 1 == length (Reference_Allele) 或End_position - Start_position == 1) 且length(Reference_Allele) <= length(Tumor_Seq_Allele1 and Tumor_Seq_Allele2)
b)	如果Variant_Type == "DEL", 那么End_position - Start_position + 1 == length (Reference_Allele), 
且length(Reference_Allele) >= length(Tumor_Seq_Allele1 and Tumor_Seq_Allele2)
c)	如果Variant_Type == "SNP", 那么length(Reference_Allele and Tumor_Seq_Allele1 and Tumor_Seq_Allele2) ==  1 且(Reference_Allele and Tumor_Seq_Allele1 and Tumor_Seq_Allele2) != "-"
d)	如果Variant_Type == "DNP", 那么length(Reference_Allele 和Tumor_Seq_Allele1 and Tumor_Seq_Allele2) ==  2 且(Reference_Allele and Tumor_Seq_Allele1 andTumor_Seq_Allele2) !contain "-"
e)	如果Variant_Type == "TNP", 那么length(Reference_Allele and Tumor_Seq_Allele1 and Tumor_Seq_Allele2) ==  3 且(Reference_Allele and Tumor_Seq_Allele1 and Tumor_Seq_Allele2) !contain "-"
f)	如果Variant_Type == "ONP", 那么length(Reference_Allele) == length(Tumor_Seq_Allele1) == length(Tumor_Seq_Allele2) > 3 且(Reference_Allele and Tumor_Seq_Allele1 and Tumor_Seq_Allele2) !contain "-"
12.	基于UUID的文件的验证: 
a)	列＃33必须包含肿瘤样本的BCR等分试样的UUID的Tumor_Sample_UUID
b)	列＃34必须是Matched_Norm_Sample_UUID，其中包含用于匹配正常样本的BCR等份的UUID
c)	由Tumor_Sample_Barcode和Matched_Norm_Sample_Barcode表示的元数据应分别对应于分配给Tumor_Sample_UUID和Matched_Norm_Sample_UUID的UUID
```

### MAF命名协议 {-}

在上传到DCC的档案中，MAF文件名应与以下方式与包含档案名称相关：
如果存档名称是：
```
<domain>_<disease_abbrev>.<platform>.Level_2.<serial_index>.<revision>.0.tar.gz
那么具有存档的公开的MAF文件命名依据为：
<domain>_<disease_abbrev>.<platform>.Level_2.<serial_index>[.<optional_tag>].somatic.maf
具有存档的受保护的MAF文件命名依据为：
<domain>_<disease_abbrev>.<platform>.Level_2.<serial_index>[.<optional_tag>].protected.maf
<optional_tag>可能包含字母数字字符、破折号或下划线，但不能有空格或句号。<optional_tag>可以省略。使用<optional_tag>的目的是给出一些简短的注释。
例如，对于文件
genome.wustl.edu_OV.IlluminaGA_DNASeq.Level_2.7.6.0.tar.gz
```
有效的MAF名称的为：
```
genome.wustl.edu_OV.IlluminaGA_DNASeq.Level_2.7.preliminary.somatic.maf
genome.wustl.edu_OV.IlluminaGA_DNASeq.Level_2.7.protected.maf
```
	
参考链接：

https://wiki.nci.nih.gov/display/TCGA/Mutation+Annotation+Format+%28MAF%29+Specification+-+v2.4

https://software.broadinstitute.org/software/igv/MutationAnnotationFormat

https://wiki.nci.nih.gov/display/TCGA/Mutation+Annotation+Format+%28MAF%29+Specification

 
## 其它格式  {-}


<!--chapter:end:02-filetype.Rmd-->

# 生物信息学数据库资源 {#database}

做数据分析常常会需要用到参考基因组和注释文件，还会需要分析公共数据，了解常见的生物信息学数据库资源也是非常有必要的！

## 基因ID

到目前为止，仅仅是人类研究，就有两万五左右的蛋白编码基因，这些基因可以合成十几万种蛋白质，还有近十万的编码lncRNA的基因，近万的miRNA等非编码基因。基因在生物信息学研究中具有中心地位，所以对于基因的命名也显得至关重要。

每个领域，每个地域，都有权威的科研单位，他们偏向于自己定义各种各样的基因命名系统，并没有一个统一的命名方式。

而且为了研究基因，还有产生探针捕获的技术，各个厂商的探针ID也是五花八门。

在疾病研究领域，也需要独特的ID。

一些功能数据库也会对自己的样本，基因重新编码ID。

常见的基础数据库也会提出自己的ID (entrez ID, Symbol, RefSeq, probeset, PubmedID,OminID,Accnum)，甚至，你自己整理发表一个数据库也可以提出基因命名系统，当然，不一定会那么受欢迎，也不会有那么多人去学习你的命名规则。


为什么要有这么多的基因ID呢？基因就像每个人一样，都是独特的个体。它在不同的地点扮演不同的角色。故自然有不同的ID，有的根据它的位置区分。比如：王总。有的根据它的特征区分比如：吝啬王。这样就会出现很多的称呼，即ID.，但他说的都是同一个人哦。

基因也是一样的，当在表达谱数据的时候，他的名字就是探针，当在ENSEMBL中时，就是ENSG开头加数字的格式，所以在不同的数据库中会有不同的命名故就会有很多的基因ID.

### ID种类繁多  {-}

其中GeneCards数据库里面列出了128种数据库ID，虽然很全面，但并非都是重点，希望大家把学习时间花在刀刃上，有一些就不要死记硬背了。
而且，我觉得大部分人看到了下面这些密密麻麻的ID，肯定是要疯掉的。
一般初学者常见的ID转换工具就是DAVID了，但其实可以自己用R编程的各种包来做转换，这样自己知道自己在做什么，也了解ID是如何定义的。

![密密麻麻的分子ID](image/C3/bioDBnet.jpg)

### 常见的数据库ID {-}

虽然有综合性的数据库收集整理了百余种ID，使得它们之间的对应及转换得以顺利进行，但大部分数据分析过程中并不会用到所有的数据ID，最常见的如下表。

|ID 示例|	ID 来源|
| -----|:-------------:|
|ENSG00000116717	|Ensemble ID|
|GA45A_HUMAN	|UniProtKB/Swiss-Prot, entry name
|A5PJB2_BOVIN	|UniProtKB/TrEMBL, entry name
|A2BC19, P12345, A0A022YWF9	|UniProt, accession number
|GLA, GLB, UGT1A1	|HGNC Gene Symbol
|U12345, AF123456	|GenBank, NCBI, accession number
|NT_123456, NM_123456, NP_123456	|RefSeq, NCBI, accession number|
|10598, 717|	Entrez ID,  NCBI|
|uc001ett, uc031tla.1	|UCSC ID|

Ensembl stable ID 的结构是根据不同物种设置的前缀, 加上数据所指的类型, 如基因蛋白质, 再加上一系列的数字. 有的时候可以有不同的版本, 则在 Ensembl ID 后面加上小数点和版本号.

#### 常用物种前缀 {-}

前缀 |学名
---|---
ENSCEL | Caenorhabditis elegans (Caenorhabditis elegans)
ENSCAF | Canis lupus familiaris (Dog)
ENSDAR | Danio rerio (Zebrafish)
FB | Drosophila melanogaster (Fruitfly)
ENS| Homo sapiens (Human)
ENSMUS |Mus musculus (Mouse)
ENSRNO |Rattus norvegicus (Rat)
ENSXET|Xenopus tropicalis (Xenopus)


#### 类型前缀  {-}

前缀 | 类型
---|---
E | exon
FM |Ensembl protein family
G | gene
GT| gene tree
P|protein
R |	regulatory feature
T| transcript

UniProt 中录入的数据都被分配了一个唯一的 entry name，叫做UniProtKB/Swiss-Prot entry name。它是最多有 11 位包含大写字母的字符串, 一般有着 "X_Y" 的形式, 其中 "X" 是最多五个便于记忆的蛋白质编号, "_" 是下划线, "Y" 是最多五个便于记忆的物种编号.

#### 蛋白质编号示例如下 {-}

Code(X)	 | Recommended protein name|Gene name
---|---|---|
B2MG | Beta-2-microglobulin|B2M
HBA | Hemoglobin subunit alpha|HBA1
INS | Insulin	|INS
CAD17 |Cadherin-17|CDH17

#### 物种编号示例如下 {-}

Code | Species
---|---
BOVIN | Bovine
CHICK | Chicken
ECOLI| Escherichia coli
HORSE| Horse
HUMAN | Homo sapiens
MAIZE|	Maize (Zea mays)
MOUSE| Mouse
PEA | Garden pea (Pisum sativum)
PIG| Pig
RABIT |Rabbit
RAT | Rat
SHEEP |Sheep
SOYBN | Soybean (Glycine max)
TOBAC | Common tobacco (Nicotina tabacum)
WHEAT |Wheat (Triticum aestivum)
YEAST|Baker’s yeast (Saccharomyces cerevisiae)
HUGO | Gene Nomenclature Committee


#### Gene Symbol  {-}

Gene Symbol 是用来表示基因的编码, 由大写字母构成, 或由大写字母和数字构成, 首字母均应该是字母，有点像是是基因的标准缩写。

如: 

* GLA这个symbol代表着"galactosidase, alpha"
* GLB这个代表着"galactosidase, beta"
* UGT1A1这个symbol代表着"UDP glycosyltransferase 1 family, polypeptide A1" 


#### GenBank Accession Number {-}

GenBank 的通用 accession number 通常是由一个大写字母加上 5 个数字的组合, 或者两个大写字母加上 6 个数字的组合.

#### RefSeq Accession Number {-}

RefSeq 有一套特殊的 Accesion Number. 形式是: [A-Z]{2}[_][0-9]{6:}, 两个大写字母, 一个下划线, 6 个或更多的数字.

Accession

前缀| 类型|说明
---|---|---
 AC_ | Genomic |Complete genomic molecule, usually alternate assembly
NC_	|Genomic |	Complete genomic molecule, usually reference assembly
NG_|	Genomi c|	Incomplete genomic region
NT_	|Genomic |	Contig or scaffold, clone-based or WGS
NW_	|Genomic|	Contig or scaffold, primarily WGS
NS_|	Genomic	|Environmental sequence
NZ_	|Genomic|	Unfinished WGS
NM_	|mRNA   |
NR_	|RNA    |
XM_	|mRNA   |	Predicted model
XR_	|RNA    |	Predicted model
AP_	|Protein|	Annotated on AC_ alternate assembly
NP_	|Protein|	Associated with an NM_ or NC_ accession
YP_|Protein	|
XP_	|Protein|	Predicted model, associated with an XM_ accession
ZP_ |Protein|	Predicted model, annotated on NZ_ genomic records

#### Entrez ID  {-}


Entrez 是 NCBI 使用的能够对众多数据库进行联合搜索的搜索引擎, 其对不同的 Gene 进行了编号, 每个 gene 的编号就是 entrez gene id. 由于 entrez id 相对稳定, 所以也被众多其他数据库, 如 KEGG 等采用. Entrez Gene ID 就是一系列数字, 也比较容易辨识. R 或网站都有众多的工具可以帮助从不同的 ID 转换为 entrez id 或者反向转换.
生信菜鸟团的博客《NCBI的基因entrezID相关文件介绍》讲解了Entrez ID主要的信息文件。可以直接搜索。
一个简单的Entrezid对应的别的ID,例子如下：

 gene_id  | symbol|chromosome
---|---|---
 352937  | dio2 |20

上表中geneid即为 Entrezid. 在ID转换中有重要的作用。

#### UCSC ID  {-}

由小写字母和数字构成, 起始均为 uc, 然后是三位数字, 接着又是三位小写字母, 最后有小数点和数字构成版本号.
如: uc010qfk.3, uc010qfk.3. 这个ID几乎被抛弃不用了，只是因为UCSC是三大数据库之一而已。

### ID 转换 {-}

最重要的就是怎么实现ID转换。常用的id有entrez gene ID, HUGO symbol, refseq ID, ensembl ID。他们之间进行转换，做一些后续的分析。一般初学者用的ID转换的工具就是DAVID，R里面关于ID转换常用的包为org.Hs.eg.db这一类的包。
在生信技能树论坛里，健明发的《ID转换大全》和《生信人必须了解的各种ID表示方式》以及《生信编程直播第8题-几个ID转换咯》里面有实战的代码，务必运行一遍。在论坛直接搜索即可。

方法1：直接在DAVID网站，粘贴转换

方法2：用R包。不管是什么ID转换，都是找到对应关系，然后match一下即可！--《生信编程直播第8题-几个ID转换咯》有完整代码

方法3：用R包，基于org.Xx.eg.db系列包，进行ID转换。--《ID转换大全》由完整代码

总之：

```
entrez gene ID 文盲不会写汉字，只能运用纯数字
Ensembl ID 有文化，身前物种做玉坠
refseq ID最懒惰，一躺在中间，字母在两边
Gene Symbol大写字母加数字，一生平庸最常见
```

直达车：

http://www.biotrainee.com/thread-941-1-1.html

http://www.biotrainee.com/thread-862-1-1.html


## 参考基因组版本

#### 不同版本对应关系 {-}

hg19，GRCH37和ensembl75是三种国际生物信息学数据库资源收集存储单位，即NCBI，UCSC和ENSEMBL各自发布的基因组信息。

hg系列，hg18/19/38来自UCSC也是目前使用频率最高的基因组。从出道至今我就只看过hg19了，但是建议大家都转为hg38，因为它是目前的最新版本。

基因组各种版本对应关系综合来看如下所示：

- GRCh36 (hg18): ENSEMBL release_52.
- GRCh37 (hg19): ENSEMBL release_59/61/64/68/69/75.
- GRCh38 (hg38): ENSEMBL  release_76/77/78/80/81/82.

ENSEMBL的版本特别复杂也很容易搞混，UCSC的版本就简单很多，常用的是hg19，最新版本为hg38。

看起来NCBI也是很简单，就GRCh36,37,38，但是里面水也很深！

```
Feb 13 2014 00:00    Directory April_14_2003
Apr 06 2006 00:00    Directory BUILD.33
Apr 06 2006 00:00    Directory BUILD.34.1
Apr 06 2006 00:00    Directory BUILD.34.2
Apr 06 2006 00:00    Directory BUILD.34.3
Apr 06 2006 00:00    Directory BUILD.35.1
Aug 03 2009 00:00    Directory BUILD.36.1
Aug 03 2009 00:00    Directory BUILD.36.2
Sep 04 2012 00:00    Directory BUILD.36.3
Jun 30 2011 00:00    Directory BUILD.37.1
Sep 07 2011 00:00    Directory BUILD.37.2
Dec 12 2012 00:00    Directory BUILD.37.3
```

从上面可以看到，有37.1， 37.2和 37.3 等等，不过这种版本一般指的是注释在更新而基因组序列一般不变。

**总之你需要记住，** **hg19基因组大小是3G，压缩后八九百兆**。

如果要下载GTF注释文件，基因组版本尤为重要。

##### **NCBI**：最新版（hg38） {-}

- ftp://ftp.ncbi.nih.gov/genomes/H_sapiens/GFF/ 

##### **NCBI**：其它版本 {-}

- ftp://ftp.ncbi.nlm.nih.gov/genomes/Homo_sapiens/ARCHIVE/    

##### **Ensembl** {-}

- ftp://ftp.ensembl.org/pub/release-75/gtf/homo_sapiens/Homo_sapiens.GRCh37.75.gtf.gz

> 变化上面链接中的**release**就可以拿到所有版本信息

- ftp://ftp.ensembl.org/pub/

##### **UCSC**  {-}

它本身需要一系列参数：

```
1. Navigate to http://genome.ucsc.edu/cgi-bin/hgTables
2. Select the following options:
clade: Mammal
genome: Human
assembly: Feb. 2009 (GRCh37/hg19)
group: Genes and Gene Predictions
track: UCSC Genes
table: knownGene
region: Select "genome" for the entire genome.
output format: GTF - gene transfer format
output file: enter a file name to save your results to a file, or leave blank to display results in the browser
3. Click 'get output'.
```
搞清楚版本关系了，接下来就是进行下载。UCSC里面下载非常方便，只需要根据基因组简称来拼接url：

```
http://hgdownload.cse.ucsc.edu/goldenPath/mm10/bigZips/chromFa.tar.gz
http://hgdownload.cse.ucsc.edu/goldenPath/mm9/bigZips/chromFa.tar.gz
http://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/chromFa.tar.gz
http://hgdownload.cse.ucsc.edu/goldenPath/hg38/bigZips/chromFa.tar.gz
```
或者用shell脚本指定下载的染色体号

```
for i in $(seq 1 22) X Y M;
do echo $i;
wget http://hgdownload.cse.ucsc.edu/goldenPath/hg19/chromosomes/chr${i}.fa.gz;done
gunzip *.gz
for i in $(seq 1 22) X Y M;
do cat chr${i}.fa >> hg19.fasta;
done
rm -fr chr*.fasta
```

## NCBI

NCBI (National Center for Biotechnology Information，美国国立生物技术信息中心）于1988年11月4日建立，是NIH（美国国立卫生研究院）的NLM（国立医学图书馆）的一个分支。目的是通过提供在线生物学数据和生物信息学分析工具来帮助人类更好的认知生物学问题。
目前有将近40个在线的文库和分子生物学数据库，包括：PubMed, PubMed Central, and GenBank等。网址： https://www.ncbi.nlm.nih.gov/

**一、任务**

1. 为储存和分析分子生物学、生物化学、遗传学知识创建自动化系统；
2. 从事研究基于计算机的信息处理过程的高级方法，用于分析生物学上重要的分子和化合物的结构与功能；
3. 促进生物学研究人员和医护人员应用数据库和软件；
4. 努力协作以获取世界范围内的生物技术信息。

**二、内容**

1.文献数据库

包括：PubMed,PubMed Central,Books等

2.序列资源库

包括人，小鼠，果蝇，线虫等各种物种的基因组数据库

包含DNA，RNA，蛋白等各种类型的数据

如：SNP,GEO,SRA等

3.常用序列分析工具

* Entrez -- 数据挖掘的工文本条件查询工具（Text Term Searching）
来自于超过10万个种物的核酸和蛋白序列数据，连同蛋白三维结构，基因组图谱信息和文献信息检索
网址：https://www.ncbi.nlm.nih.gov/gquery/

* BLAST -- 序列比对工具

https://blast.ncbi.nlm.nih.gov/Blast.cgi

4.数据下载与上传

数据下载接口：ftp://ftp.ncbi.nlm.nih.gov/

上传的工具有：Sequin，tbl2asn等，链接地址：https://www.ncbi.nlm.nih.gov/guide/data-software/

5.其他合作项目

我们比较常用的就是检索文献，检索序列，比对序列。了解更多内容可以参考官网手册：https://www.ncbi.nlm.nih.gov/books/NBK143764/

**参考资料**

https://baike.baidu.com/item/NCBI/3598184?fr=aladdin

https://www.ncbi.nlm.nih.gov/books/NBK143764/

### GEO

基因表达数据库(GEO,Gene Expression Omnibus database，https://www.ncbi.nlm.nih.gov/geo/ )是由NCBI负责维护的一个数据库，设计初衷是为了收集整理各种表达芯片数据，但是后来也加入了甲基化芯片，lncRNA，miRNA，CNV芯片等各种芯片，甚至高通量测序数据,是目前最大、最全面的公共基因表达数据资源。所有的数据均可以在ftp站点下载：ftp://ftp-trace.ncbi.nih.gov/geo/

首先，我们在GEO的主页（ https://www.ncbi.nlm.nih.gov/geo/ ）可以看到：

    Browse Content
    Repository Browser
    DataSets: 4348
    Series: 87717
    Platforms: 17572
    Samples: 2165066

截止到2017年8月17日，统计信息如上，可以看到数据量已经很恐怖了。


**一、GEO数据库基础知识 **

    GEO Dataset (GDS) 数据集的ID号
    GEO Series (GSE) study的ID号
    GEO Platform (GPL) 芯片平台
    GEO Sample (GSM) 样本ID号

这些数据都可以在ftp里面直接下载。

**二、数据上传**

上传的方式：

1. 网页
2. Excel表格
3. 软件
4. MINiML格式上传

详细上传方法，参见：https://www.ncbi.nlm.nih.gov/geo/info/submission.html

提交Affymetrix芯片数据到GEO数据库
http://www.biotrainee.com/thread-810-1-1.html

**三、数据挖掘**

1. Entrez GEO-DataSets

官网： http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=gds

收录整个实验数据，可以通过技术类型，作者，物种和实验变量等信息来进行搜索。一旦相关数据被查询到，可以通过提供上面的小工具做一些分析，比如：热电图分析，表达分析，亚群的影响等

2.Entrez GEO-Profiles

官网：https://www.ncbi.nlm.nih.gov/geoprofiles/

收录单个基因的表达谱数据。可以通过基因名字，GenBank编号，SAGE标签，GEO编号等来进行搜索

3.GEO BLAST

GEO Blast界面容许用户根据核酸序列的相似性来搜索相关的GEO-Profiles
所有的BLAST结果中“E”的标签代表这个数据跟GEO-Profiles表达数据相关。

4. 数据下载

我们一般是拿到了GSE的study ID号，然后直接把什么的url修改一下，就可以看到关于该study的所以描述信息，是用的什么测序平台(芯片数据，或者高通量测序)，测了多少个样本，来自于哪篇文章！
所有需要的数据均可以下载，而且都是在上面的ftp里面可以根据规律去找到的，甚至可以自己拼接下载的url链接，来做批量化处理！

例如：用GSE75528，则在https://www.ncbi.nlm.nih.gov/geo/  官网上直接搜索GSE75528
或直接输入 https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE75528 修改这个url最末尾的GSE号码就可以进入自己想去的任何研究的GEO页面。

如果是芯片数据，那么就需要自己仔细看GPL平台里面关于每个探针对应的注释信息，才能利用好别人的数据。
如果是高通量测序数据，一般要同步进入该GSE对应的SRA里面去下载sra数据，然后转为fastq格式数据，自己做处理！

**四、其他**

1. 联系方式

上传数据或查询数据有问题，可以联系 ``geo@ncbi.nlm.nih.gov``

2. 写一个Python脚本下载GEO数据

脚本逻辑很简单：

1. 根据GEO accession找到FTP地址
2. 用wget循环下载FTP地址下的数据

```
#!/bin/python3
import refrom urllib.request
import urlopen
import os
def main(geo):
# find the FTP address from [url=https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GEO]https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GEO[/url]
   response = urlopen("https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc={}".format(geo))
    pattern = re.compile("<a href=\"(.*?)\">\(ftp\)</a>")
    # use wget from shell to download SRA data
   ftp_address = re.search(pattern,response.read().decode('utf-8')).group(1)
    os.system(' wget -nd -r 1 -A *.sra ' + ftp_address)

if __name__ == '__main__':
    from sys import argv
    main(argv[1])
```

3. 数据分析

5行代码搞定GEO学习总结版：http://www.biotrainee.com/thread-612-1-1.html


参考资料：

http://www.bio-info-trainee.com/1835.html

https://www.ncbi.nlm.nih.gov/geo/info/GEOHandoutFinal.pdf

### SRA

跟GEO类似,NCBI的SRA（Sequence ReadArchive，https://www.ncbi.nlm.nih.gov/sra/ ）数据库是专门用于存储二代测序的原始数据，包括 454, IonTorrent, Illumina, SOLiD, Helicos and CompleteGenomics等。 除了原始序列数据外，SRA现在也存在raw reads在参考基因的aligment information。

该数据库也是International Nucleotide Sequence Database Collaboration (INSDC) 的一部分。INSDC包含：NCBI Sequence Read Archive (SRA), European Bioinformatics Institute (EBI), 和 DNA Database of Japan (DDBJ)。数据提交给其中任何一个数据库中后，数据都是共享的。

**一、数据库结构**

每个数据库都有自己最小的可发表单元。例如：PubMed最小可发表单元是一篇文献，SRA中最小可发表单元是一次实验（标签为：SRX#）。

NCBI中SRA数据结构的层次关系：Studies,Experiments, Samples，Runs:

```
Studies是就实验目标而言的，一个study可能包含多个experiment。
Experiments包含了样本，DNA source，测序平台，数据处理等信息。
一个experiment可能包含一个或多个runs。
Runs 表示测序仪运行所产生的reads.
```
SRA数据库用不同的前缀加以区分：ERP or SRP for Studies, SRS for samples, SRX for Experiments, and SRR for Runs。

**二、数据上传**

1. 登陆NCBI账号

2. 注册你的项目和生物样本

    注册项目：https://www.ncbi.nlm.nih.gov/bioproject/

    注册样本：https://www.ncbi.nlm.nih.gov/biosample/

3. 上传SRA数据

    上传SRA metadata (关于该项目、实验的等信息)

    上传序列数据

更详细的说明，参见 https://www.ncbi.nlm.nih.gov/sra/docs/submit/

**三、数据下载**

如果要下载每个study对应的runs的所有数据，我们需要下载安装SRA 
Toolkit！

链接地址： http://www.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=software

SRA toolkit常用命令的说明文档见：

http://www.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=toolkit_doc。

这里我们需要使用prefetch命令进行下载

```
$prefetch SRR776503  SRR776505  SRR776506
```
下载完成后，会在你的工作主目录下生成一个ncbi的文件夹。


Sra子文件夹中的.sra文件就是对应的runs文件。
‘.sra’的后缀是SRA数据库对fastq文件的特殊压缩。使用前，我们需要将其解压为fastq文件。SRA Toolkit 包含了解压函数fastq-dump :``$fastq-dump SRR776503.sra``


通过命令行来下载
```
for ((i=204;i<=209;i++)) ;
do 
wget ftp://ftp-trace.ncbi.nlm.nih.gov/sra/sra-instant/reads/ByStudy/sra/SRP/SRP017/SRP017311/SRR620$i/SRR620$i.sra;
done
ls *sra |while read id; do ~/biosoft/sratoolkit/sratoolkit.2.6.3-centos_linux64/bin/fastq-dump --split-3 $id;done
```

**四、其他**

1. 上传或下载数据遇到问题，可咨询：`` sra@ncbi.nlm.nih.gov``


**参考资料**

http://www.biotrainee.com/thread-800-1-1.html      

https://www.ncbi.nlm.nih.gov/sra/docs/


### dbSNP 

单核苷酸多态性（single nucleotide polymorphism，SNP） 主要是指在基因组水平上由单个核苷酸的变异所引起的DNA序列多态性。它是人类可遗传变异中最常见的一种，占所有已知多态性的90%以上。dbSNP(The Single Nucleotide Polymorphism database) 是一个公共的核酸多态性的数据库，它是关于单碱基替换以及短插入、删除多态性的资源库。网址：https://www.ncbi.nlm.nih.gov/projects/SNP。

**一、最新版本**

人的dbsnp目前已更新到150版本
150版本基本信息

|Organism| dbSNP Build| Genome Build| Number of  Submissions(ss#'s)|Number of RefSNP Clusters (rs#'s) ( # validated)|Number of (rs#'s) in gene|Number of (ss#'s) with genotype|Number of (ss#'s) with frequency
|-------|-------|-------|-------|-------|-------|-------|-------|
|Homo sapiens|	150|38.3|907,234,193|325,660,549 (107,926,145)|	191,665,918|73,917,935|130,169,906|

下载地址：ftp://ftp.ncbi.nlm.nih.gov/snp/

**二、目录结构**

点击：ftp://ftp.ncbi.nlm.nih.gov/snp/  则进入snp网址。

dbSNP包含了许多目录，其中最有用的是：

```
	organisms/
	database/
	specs/
```

相关详细信息见  ftp://ftp.ncbi.nih.gov/snp/00readme.txt

1. FTP的“specs/”目录包含重要的文件的格式，内容及其基本介绍。

2. FTP的“organisms/”目录包含了一列有SNP数据的生物体目录，其按通用名后接NCBI分类id号来组织的。（要知道，DBsnp不光包含人的数据哦，还有bacteria,tuna等物种的snp信息;人类物种ID是9606，可以看到variation位点信息有基于hg19和hg38的两种下载方式，如果还有其它需求，可以自己用基因组坐标转换工具。）

3. 点击特定的生物体子目录即可访问其中的ftp报表文件，你还可以选择同一个物种的不同版本。比如你点击了human_9606目录，那么你会发现人类数据目录包含以下子目录：ASN1_bin/、ASN1_flat/、XML/、VCF/、chr_rpts/、gene_report/、Genome_report/、rs_fasta/、ss_fasta/、genotype_by_gene/、genotype/、haplotypes/、database/、misc/、Enterz/


* ASN1_bin : ASN.1 二进制格式的RefSNP文档综述 （.bin）
* ASN1_flat :从ASN.1 二进制格式而来的按染色体排序的RefSNP docsum(.flat)
* XML： 提供refSNP簇的具体查询信息以及NCBI SNP交换格式的簇成员(.xml)
* chr_rpts ：包含特定染色体上的RefSNPs 的完整列表(.txt)
* genotype ：以基因型交换XML格式提供提交的SNPs的submitter和基因型信息(.xml)
* genome_reports ：包含生物体SNP密度分布的概要报告以及每个基因中的SNPs的概要报告(无后缀或.rpt或.log）
* ss_fasta ：包含FASTA格式的生物体的所有可用的submitted SNP（ss）序列数据（.fas)
* rs_fasta ：包含FASTA格式的人类所有可用的参考SNP （RS）序列数据（.fas)

**chr_rpts 中的txt文件内容：**

1. RefSNP id (rs#)rs代号

2. mapweight where 匹配个数

  * 1 = Unmapped
  * 2 = Mapped to single position in genome
  * 3 = Mapped to 2 positions on a single chromosome
  * 4 = Mapped to 3-10 positions in genome (possible paralog hits)
  * 5 = Mapped to >10 positions in genome.


3. snp_type where   snp类型

4. Total number of chromosomes hit by this RefSNP during mapping   匹配到的染色体个数

5. Total number of contigs hit by this RefSNP during mapping  匹配到的conting个数

6. Total number of hits to genome by this RefSNP during mapping  匹配到基因组的个数

7. Chromosome for this hit to genome   匹配到的染色体

8. Contig accession for this hit to genome  匹配到conting 序号

9. Position of RefSNP in contig coordinates  在conting中匹配到突变的位置

10. Position of RefSNP in chromosome coordinates (used to order report) 

在染色体中匹配到突变的位置

    * x, a single number, indicates a feature at base position x
    * x..y, denotes a feature that spans from x to y inclusive.
    * x^y, denotes a feature that is inserted between bases x and y
    
11. Genes at this same position on the chromosome    匹配到的基因名字

12. Genotypes available in dbSNP for this RefSNP   基因型是否可知

    * 1 = yes
    * 0 = no

**bed中包含各个染色体上的snp,如下：**

1. chrom:  The name of the chromosome (e.g. chr1, chr2, etc.).

2. chromStart:  The Reference SNP (rs) start position on the chromosome.

Note: The first base in a chromosome is numbered 0.

3. chromEnd:  The rs end position on the chromosome.

Optional Fields:

4. name:  The dbSNP Reference SNP (rs) ID

5. score:  dbSNP does not assign a score value, so this field will always
contain a 0 .    

6. strand:  This field defines strand orientation as either + or -.

**VCF**
这个是dbSNP数据库的精髓文件，需要仔细理解，内容节选如下：

```
    #CHROM POS  ID     REF  ALT  QUAL       FILTER      INFO
    1       948136    rs267598747   G      A      .        .         RS=267598747;RSPOS=948136;dbSNPBuildID=137;SSR=0;SAO=3;VP=0x050060000305000002100120;GENEINFO=NOC2L:26155;WGT=1;VC=SNV;PM;REF;SYN;ASP;LSD;CLNALLE=1;CLNHGVS=NC_000001.11:g.948136G>A;CLNSRC=.;CLNORIGIN=2;CLNSRCID=.;CLNSIG=1;CLNDSDB=MedGen:SNOMED_CT;CLNDSDBID=C0025202:2092003;CLNDBN=Malignant_melanoma;CLNREVSTAT=no_assertion_provided;CLNACC=RCV000064926.2
```

它包含的内容：染色体，突变的位置，rs代号，突变过程，info。

其中info包含了突变是否为同义突变？突变实在coding 区还是内含子或UTR？也包含了clinvar数据库的临床意义信息，CLNSIG（0 - Uncertain significance, 1 - not provided, 2 - Benign, 3 - Likely benign, 4 - Likely pathogenic, 5 - Pathogenic, 6 - drug response, 7 - histocompatibility, 255 - other）；CLNDSDB（Variant disease database name）；CLNDBN（Variant disease name）还有更多解释，直接看第二章的VCF格式介绍即可。

**三、查询**

 http://www.ncbi.nlm.nih.gov/SNP/
 是NCBI做好的一个网页版查询工具，因为下载一个 variation位点信息记录文件动辄就是十几个G，一般人也不会处理那个文件，不知道从里面应该如何提取需要的信息，这时候学习它的网页版查询工具也挺好的。

在UCSC里面也有对dbsnp数据库的介绍，主要是从数据库设计的角度来理解，里面详细介绍了每一列具体的意义，值得大家仔细学习。

http://genome.ucsc.edu/cgi-bin/hgTables?db=hg19&hgta_group=varRep&hgta_track=snp146&hgta_table=snp146&hgta_doSchema=describe+table+schema

http://genome.ucsc.edu/cgi-bin/hgTables?db=hg19&hgta_group=varRep&hgta_track=snp141&hgta_table=snp141&hgta_doSchema=describe+table+schema

但是如果真想从数据库语言的角度来理解，需要看它的数据库设计的schema了：很复杂：ftp://ftp.ncbi.nih.gov/snp/database/erd_dbSNP.pdf

 sql的代码也可以下载：
 ftp://ftp.ncbi.nlm.nih.gov/snp/organisms/human_9606/database/organism_schema/

还根据gene来分genotype：ftp://ftp.ncbi.nlm.nih.gov/snp/organisms/human_9606/genotype_by_gene/

**四、命名**

关于snp位点的命名其实并不统一，大家在文献中一般用的都是习惯或者说惯用名称。这里只介绍NCBI的rs号。NCBI里对所有提交的snp进行分类考证之后，都会给出一个rs号，也可称作参考snp，并给出snp的具体信息，包括前后序列，位置信息，分布频率等，应该说用这个rs号是比较容易确定搞明白的。 一般写法是这样: dbSNP后面跟featureID。featureID一般是rs/ss后跟7-8位数字， 比如: rs12345678或者dbSNP|rs12345678 。

最后值得一提的是，除了dbsnp对variation规定了ID号，还有几个其它偏门的ID号也可以来描述变异位点的。

	NCBI的dbsnp，以rs和ss开头
	illumina的kgp开头
	ESP的以esp开头
	kgp是illumina中华八芯片的

**五、其他**

有任何疑问可联系：snp-admin@ncbi.nlm.nih.gov

**参考资料：**

http://www.bio-info-trainee.com/1863.html

http://blog.sina.com.cn/s/blog_751bd9440102w6rm.html

https://www.ncbi.nlm.nih.gov/books/NBK21088/

### RefSeq

NCBI RefSeq (Reference Sequence，美国国立生物技术信息中心参考序列库) 是目前世界上最具有权威性的序列数据库。NCBI的参考序列计划（RefSeq）将为中心法则中自然存在的分子，从染色体到mRNA到蛋白提供参考序列标准。RefSeq标准为人类基因组的功能注解提供一个基础。它们为突变分析，基因表达研究，和多态发现提供一个稳定的参考点。

* 全面的，整合的，无冗余的序列
* 基因组DNA，RNA，蛋白产物
* 是医学、功能、多样性研究的一个基准
* 为基因组注释，基因鉴定和特性描述，突变和多态性分析，表达研究和比较分析提供稳定可靠的参考
* 由NCBI和其合作者维护

|Proteins|Transcripts|Organisms|
|------|------|------|
|88,385,530 |19,634,664|71,356|
-- 最新数据截止2017年7月21日

由于一些序列来自异常连接产生的转录物或由计算机推演产生的不正确内含子-外显子剪切，因此该数据库所收集的参考序列一直在不断地被修改中，尽管如此，NCBI RefSeq  仍是目前最可信赖的人类基因mRNA序列数据库。

**一、命名**

RefSeq一般的命名格式:前缀为两个字母，然后下横线（'_'）。区别于其它的GenBank的命名格式。

	Model RefSeq: XM_ (mRNA), XR_ (non-coding RNA), and XP_ (protein) 这个是首先被提交的
	Known RefSeq: NM_ (mRNA), NR_ (non-coding RNA), or NP_ (protein)  代表被人工检验过

1. 在Comment区域显示来源,说明数据可靠性。（GENOME ANNOTATION，INFERRED，MODEL，
PREDICTED，PROVISIONAL，WGS REVIEWED，VALIDATED）
2. 蛋白序列在DBSOURCE区域标示 ‘REFSEQ’

**blast结果中序列名的含义**

blast一般返回的结果序列开头的格式都如正下面所示：

	gi|4557284|ref|NM_000646.1|[4557284]

格式说明：

1. gi ：”GenBank Identifier的缩写”, 是序列的ID号，标识符。唯一的。
2. 4557284 就是该序列的gi号
3. ref :标示该序列是参考序列。
4. NM_000646.1 该序列的Accession号和版本号


**预测的，临时的，和检查过的RefSeq记录有什么区别？**

RefSeq记录是有三种可以获得的状态：预测的，临时的和检查过的（reviewd）。

1. 检查过的RefSeq记录代表了目前关于一个基因和它的转录子的知识的汇编。它们很多都来自于GenBank记录、人类基因组命名委员会、和OMIM。RefSeq标准为人类基因组的功能注解提供一个基础。

2. 预测的RefSeq记录是来自于那些未知功能的cDNA序列，它们有一个预测的蛋白编码区。

3. 临时的RefSeq记录还没有被检查过。它们是有自动的程序产生的。


**二、如何访问RefSeq**

1. BLAST

http://blast.ncbi.nlm.nih.gov/blast/

将序列跟已经注释的序列比对，寻找序列之间的差异

2. Clinical Remap

www.ncbi.nlm.nih.gov/genome/tools/remap

比较重新组织的序列跟RefSeqGene序列之间的差异

3. Variation Reporter

http://www.ncbi.nlm.nih.gov/variation/tools/reporter/

报到突变跟RefSeq序列的关系

4. 其他会检索RefSeq库的工具

mapview   https://www.ncbi.nlm.nih.gov/mapview/

ENTREZ GENE  https://www.ncbi.nlm.nih.gov/gene

ENTREZ GENOMES DIVISION   https://www.ncbi.nlm.nih.gov/genome

5. 数据下载

下载地址：ftp://ftp.ncbi.nlm.nih.gov/refseq/

其它物种： ftp://ftp.ncbi.nlm.nih.gov/refseq/release/

**三、讨论**

1. RefSeq和genbank的数据有什么区别？

genbank是一个开放的数据库，对每个基因都含有许多序列。很多研究者或者公司都可以自己提交序列，另外这个数据库每天都要和EMBL和DDBJ交换数据。genbank的数据可能重复或者不准。
而RefSeq数据库被设计成每个人类位点挑出一个代表序列来减少重复，是NCBI提供的校正的序列数据和相关的信息。数据库包括构建的基因组contig、mRNA、蛋白和整个染色体。refseq序列是NCBI筛选过的非冗余数据库，一般可信度比较高。

2. 为什么RefSeq记录中的基因符号（symbol）有时和相关的GenBank中的不一样？

RefSeq全部使用官方基因符号。而GenBank是一个公共的序列备份库，由数据发现者提供。有的作者会向相关的物种命名委员会取得官方基因符号，但有的作者没有，所以有时会产生别名。GenBank与Pubmed相同，通过display可以选择显示格式，常用的有GenBank和FASTA两种格式。如果要对基因序列作进一步分析，FASTA格式是很好的选择。FASTA格式仅包括该序列的简要特征，并以ATGC4种碱基列出核苷酸序列，简单明了。而GenBank格式可显示较完整的基因序列记录，反映核苷酸序列的详细信息


**参考资料**

http://www.ncbi.nlm.nih.gov/refseq/

http://liucheng.name/379/

http://yangl.net/2015/10/08/ncbi_refseq/

http://yangl.net/2015/10/08/ncbi-refseq-name-format/

http://www.biotrainee.com/thread-213-1-1.html

https://www.ncbi.nlm.nih.gov/books/NBK21091/

ftp://ftp.ncbi.nlm.nih.gov/refseq/H_sapiens/RefSeqGene/presentations/RefSeqGene.pptx

### Entrez

Entrez (http://www.ncbi.nlm.nih.gov/Entrez) 是美国国家生物技术信息中心所提供的在线资源检索器。该资源将GenBank序列与其原始文献出处链接在一起。 Entrez是由NCBI主持的一个数据库检索系统。


**一、Entrez系统数据库**

有将近38个库，这里仅列举了部分，具体请参见（ https://www.ncbi.nlm.nih.gov/gquery/gquery.fcgi ）

* Literature
    * Books: 在线生物医学图书
    * PubMed 生物医学文献数据库
* Health
    * OMIM : 人类孟德尔遗传数据库
* Genomes
    * SRA：二代测序的原始数据
    * SNP: 单核苷酸多肽性数据库
    * Taxonomy: GenBank 中的物种分类学数据库
* Genes
    * GEO: 基因表达数据库
* Proteins
    * Structure: 大分子三维结构数据库  
* Chemicals
    * BioSystems		跟基因，蛋白，化学分子关联的分子通路

**二、Entrez检索的方法**

1. 检索规则

* 词间默认逻辑关系为AND
* 短语检索加引号“”;
* 使用的逻辑运算符有AND、OR 和NOT, 但必须大写;
* 支持截词检索, 截词符用*表示;
* 定义词条类型：[ ]   
* 用：表示起始 

例子：

    从左到右的顺序，关联词大写
    promoters OR response elements NOT human AND mammals
    
    先执行括号里面的逻辑
    g1p3 AND (response element OR promoter)

    horse[Organism]
    neoplasms[MeSH Terms]
    prolactin[Protein Name]
    srcdb_refseq[Properties]
    2010/06[Publication Date]
    
    
    110:500[Sequence Length]
    2015/3/1:2016/4/30[Publication Date]

    PubMed: ("horses"[MeSH Terms] OR "horses"[All Fields] OR "horse"
    [All Fields] OR "equidae"[MeSH Terms] OR "equidae"[All Fields]) 
    AND ("receptors, dopamine d2"[MeSH Terms] OR ("receptors"[All Fields]
     AND "dopamine"[All Fields] AND "d2"[All Fields]) OR "dopamine d2 
    receptors"[All Fields] OR ("dopamine"[All Fields] AND "receptor"
    [All Fields] AND "d2"[All Fields]) OR "dopamine receptor d2"[All Fields])
    
    Protein: ("Equus caballus"[Organism] OR horse[All Fields]) AND (dopamine 
    receptor D2[Protein Name] OR (dopamine[All Fields] AND receptor[All Fields]
     AND D2[All Fields])  
    
    模糊匹配
    NC_0000*[Accession] AND Human[Organism]

2.搜索

a. 图形界面的搜索

在主页 https://www.ncbi.nlm.nih.gov/ 选择好数据库，进行检索。

NCBI上所有的资源见：https://www.ncbi.nlm.nih.gov/guide/all/

登陆NCBI以后会保留你的搜索记录。


进入单独的数据库搜索界面，会有advanced选项，更精细的搜索:

    Nucleotide: www.ncbi.nlm.nih.gov/nucleotide
    PubMed: www.ncbi.nlm.nih.gov/pubmed
    Gene: www.ncbi.nlm.nih.gov/gene/advanced

b. 直接输入网址

* 蛋白编号gi4557757，GenPept格式（默认）

    www.ncbi.nlm.nih.gov/protein/4557757

* 核酸编号，NM_000240和NM_000041，GenBank格式

    www.ncbi.nlm.nih.gov/nucleotide/NM_000240,NM_000041&report=genbank

* Gene编号348

    www.ncbi.nlm.nih.gov/gene/348

* Gene编号348，XML格式

    www.ncbi.nlm.nih.gov/gene/348?report=XML

* PubMed ID为9705509和19745054，abstract格式

    www.ncbi.nlm.nih.gov/pubmed/9705509,19745054?report=abstract&format=text

* 在nucleotide中搜索APOE基因，限制一页呈现200个结果
    
    www.ncbi.nlm.nih.gov/nucleotide/?term=APOE[gene]&dispmax=200

* 在PubMed中搜索Lipman DJ和PMID的格式呈现
    www.ncbi.nlm.nih.gov/pubmed/?term=Lipman+DJ&report=uilist

3. 命令行的搜索

可以通过E-utilities（Entrez Programming Utilities )来进行批量的下载或检索。

感兴趣的可以参考：https://www.ncbi.nlm.nih.gov/books/NBK25501/

bioython也带有相关的工具：http://biopython-cn.readthedocs.io/zh_CN/latest/cn/chr09.html

**参考资料**

《NCBI的Entrez系统检索技巧》

https://www.ncbi.nlm.nih.gov/books/NBK3837/

## Ensembl

[Ensembl](http://asia.ensembl.org/index.html)是由EBI和Sanger共同开发的真核生物基因组注释项目，它侧重于脊椎动物的基因组数据，但也包含了其他生物如线虫，酵母，拟南芥和水稻等，其中，[BioMart](www.biomart.org)是用户提取Ensembl基因组数据的强大工具。

[Ensembl](http://asia.ensembl.org/index.html)项目得到的数据均可以通过其基因组浏览器查看，用于支持脊椎动物基因组的比较基因组，进化，序列突变和转录调控方面研究。Ensembl注释基因，多重序列比对，预测结构和收集疾病数据。Ensembl工具包括：BLAST, BLAT, BioMart 和 Variant Effect Predictor (VEP)。

**一、简介**

Ensembl是由英国Sanger研究所Wellcome基金会（WTSI）和欧洲分子生物学实验室所属分部欧洲生物信息学研究所（EMBI-EBI）共同协作运营的一个项目。这些机构均位于英国剑桥市南部辛克斯顿的威康信托基因组校园（Wellcome Trust Genome Campus）内。

Ensembl计划开始于1999年，人类基因组草图计划完成前的几年。即使在早期阶段，也可明显看出，三十亿个碱基对的人工注释是不能够为科研人员提供实时最新数据的获取的。因此Ensembl的目标是自动的基因组注释，并把这些注释与其他有用的生物数据整合起来，通过网络公开给所有人使用。Ensembl数据库网站开始于July 2000，是一个真核生物基因组注释项目，其侧重于脊椎动物的基因组数据，但也包含了其他生物，如线虫，酵母，拟南芥和水稻等。近年来，随着时间推移，越来越多的基因组数据已经被添加到了Ensembl，同时Ensembl可用数据的范围也扩展到了比较基因组学、变异，以及调控数据。

目前Ensembl的组员有40到50个人，分成几个小组:

1. Genebuild小组负责不同物种的gene sets创建。他们的结果被保存在核心数据库中，该数据库由Software小组进行运维。Software小组还负责BioMart数据挖掘工具的开发和维护。

2. Compara、Variation以及Regulation小组分别负责比较组学、突变以及调控的数据相关工作。

3. Web小组的工作是确保所有的数据能够在网站页面上，通过清晰和友好用户界面呈现出来。

4. Production小组负责Ensembl数据的常规更行。

5. 最后，Outreach小组负责用户的答疑，以及提供全球范围内使用Ensembl的研讨会议或知识培训。

截止到2017年7月，Ensembl发发布了最新的Ensembl 90版本数据

包含的基因组的物种：http://asia.ensembl.org/info/about/species.html

**基因注释的数据来源**

1. 最新的基因组数据（大部分是动物）
2. UniProt/Swiss-Prot和UniProt/TrEMBL蛋白序列
3. NCBI RefSeq蛋白和核酸序列
4. EMBL cDNA序列

**二、Ensembl可以做什么**

* 查看基因在染色体上的注释
* 查看基因的选择性转录
* 探索某个基因的超过50个物种的同源性和进化树
* 比较物种的全基因组的比对和保守区域
* 查看比对到Ensembl上的芯片序列
* 查看染色体任何一区域的ESTs, clones, mRNA和proteins
* 检查染色体或基因上的SNPs (single nucleotide polymorphisms)
* 查看不同品种（rat,mouse）,种群，品种（狗）的SNPs
* 查看比对到Ensembl基因上的mRNA或蛋白的序列位置
* 上传自己的数据
* 通过BLAST或BLAT来搜索Ensembl基因组中相似的序列
* 通过BioMart导出序列和基因信息
* Variant Effect Predictor

**三、下载**

1. 少量的数据

大多数Ensembl 基因组数据的描述页有"export"功能，可以直接导出这一页的内容。

2. 大的数据集

PERL API http://www.ensembl.org/info/docs/api/index.html

如果不熟悉Perl语言，可以通过Ensembl REST API  http://rest.ensembl.org/

3. 复杂的交叉数据库

BioMart    http://www.ensembl.org/info/data/biomart/index.html

4. 全部的数据集

FTP site   http://www.ensembl.org/info/data/ftp/index.html 

**四、其他**

1. Ensembl genes命名

人的基因
```
	ENSG  Gene
	ENST  Transcript
	ENSE  Exon
	ENSP  Protein
	例如： ENST00000252723
```
其他物种的基因，例如老鼠(Mus musculus)
```
	ENSMUSG  Mouse Gene
	ENSMUST  Mouse Transcript
	ENSMUSE  Mouse Exon
	ENSMUSP  Mouse Protein
```

2. 常见问题

http://www.ensembl.org/Help/Faq

**参考资料**

http://asia.ensembl.org/info/about/index.html

http://www.ensembl.org/info/index.html

## UCSC

下面我们来介绍一下作为生信人必须掌握的三大数据库 NCBI-UCSC-ENSEMBL之一的UCSC。

**一、简介**

2000年6月22日，UCSC（University of California,Santa Cruz）和其他国际人类基因组计划的成员完成了人基因组组装的第一个草图，并承诺永久对外提供基因组信息。几个星期以后，在2000年7月22日，组装的基因组在网站 ttp://genome.ucsc.edu 呈现出来，并提供了一个在线的查询分析工具UCSC Genome Browser。接下来的几年里，该网站不断的发展，如今已包含大量的脊椎动物和模式生物的基因组组装和注释信息，并停工了一系列查看，分析，下载数据的工具。

站点地址：

*	http://genome.ucsc.edu/
*	Europe: http://genome-euro.ucsc.edu
*	Asia: http://genome-asia.ucsc.edu

数据库特点：

* 给浏览基因组数据提供了可靠和迅速的方式。
* 整合了大量的基因组注释数据，约有一半的注释信息是UCSC通过来自公开的序列数据计算得出，另外一半来自世界各地的科学工作者。本身并不下任何结论，而只是收集各种相关信息供用 户参考。
* 支持数据库检索和序列相似性搜索。

**二、UCSC可以干什么**

UCSC建立的初衷是为了更好的呈现基因组数据，方便人们查看与研究。因此在呈现基因组碱基序列的同时，也结合了注释信息，例如known genes, predicted genes, ESTs, mRNAs, CpG islands, assembly gaps and coverage, chromosomal bands, mouse homologies等等。所以用户既可以用他们提供的数据库里面的数据，也可以上传自己的数据来做研究。围绕着这样的初衷，他们设计

```
Genome Browser  整合基因组数据和各种注释数据的在线查看系统
Blat     序列比对工具
Table Browser  将文本文件转化为数据库可以识别的文件
Genome Graphs   上传和呈现基因组数据的工具，例如genome-wide SNP association studies, linkage studies 和homozygosity mapping
Gene Sorter    各种形式的呈现基因的表达，同源等信息以及相互关系
Gene Interactions  基因之间的交互关系
In-Silico PCR   查看一对引物在基因组中的位置
VisiGene 		查看基因在显微镜下的原位图
LiftOver   基因组版本的转换
```

**三、常用案例介绍**

1.如何搜索根据位置来快速获得序列

比如：获得chr17:7676091,7676196对应的序列

方法：
http://genome.ucsc.edu/cgi-bin/das/hg38/dna?segment=chr17:7676091,7676196

网页会返回 一个xml格式的信息，解析一下即可。
```
	This XML file does not appear to have any style information associated with it. The document tree is shown below.
	<DASDNA>
	<SEQUENCE id="chr17" start="7676091" stop="7676196" version="1.00">
	<DNA length="106">
	aggggccaggagggggctggtgcaggggccgccggtgtaggagctgctgg tgcaggggccacggggggagcagcctctggcattctgggagcttcatctg gacctg
	</DNA>
	</SEQUENCE>
	</DASDNA>
```

很明显里面的aggggccaggagggggctggtgcaggggccgccggtgtaggagctgctgg tgcaggggccacggggggagcagcctctggcattctgggagcttcatctg gacctg 就是我们想要的序列啦

hg38可以更换成hg19，dna?segment= 后面可以按照标准格式更换，既可以返回我们想要的序列了。

2.下载数据

首先是NCBI对应UCSC，对应ENSEMBL数据库：
GRCh36 (hg18): ENSEMBL release_52.
GRCh37 (hg19): ENSEMBL release_59/61/64/68/69/75.
GRCh38 (hg38): ENSEMBL  release_76/77/78/80/81/82.
可以看到ENSEMBL的版本特别复杂！！！很容易搞混！
但是UCSC的版本就简单了，就hg18,19,38, 常用的是hg19，但是我推荐大家都转为hg38

UCSC里面下载非常方便，只需要根据基因组简称来拼接url即可：

```
	http://hgdownload.cse.ucsc.edu/goldenPath/mm10/bigZips/chromFa.tar.gz
	http://hgdownload.cse.ucsc.edu/goldenPath/mm9/bigZips/chromFa.tar.gz
	http://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/chromFa.tar.gz
	http://hgdownload.cse.ucsc.edu/goldenPath/hg38/bigZips/chromFa.tar.gz
```

或者用shell脚本指定下载的染色体号：
```
	for i in $(seq 1 22) X Y M;
	do echo $i;
	wget http://hgdownload.cse.ucsc.edu/goldenPath/hg19/chromosomes/chr${i}.fa.gz;
	## 这里也可以用NCBI的：ftp://ftp.ncbi.nih.gov/genomes/M_musculus/ARCHIVE/MGSCv3_Release3/Assembled_Chromosomes/chr前缀
	done
	gunzip *.gz
	for i in $(seq 1 22) X Y M;
	do cat chr${i}.fa >> hg19.fasta;
	done
	rm -fr chr*.fasta
```

3. 基因组数据版本转化

染色体区域根据不同的版本（如hg19或者hg38）会有不同的显示，假设我现在有一段hg19上关注的区域（例如chr1：100-10000），我想知道在hg38版本上这段区域的位置，以便于可以更新注释信息，需要怎么做？

三大主流生物信息学数据库运营单位都出了自己的基因组坐标转换，它们分别是 (UCSC的liftOver, NCBI 的 Remap, Ensembl的API)，其中Ensembl的API是基于[crossmap](http://www.bio-info-trainee.com/1413.html)的，是一个python程序。而ucsc的[LiftOver](https://genome.ucsc.edu/cgi-bin/hgLiftOver)最出名，而且有可执行版本软件可以下载。 

4. Blat

* 针对DNA序列，BLAT是用来设计寻找95%及以上相似 至少40个碱基的序列。
* 针对蛋白序列，BLAT是用来设计寻找80%及以上相似 至少20个氨基酸的序列。
* 用法:
	- 查找mRNA或蛋白在基因组中的位置 
	- 决定基因外显子的结构
	- 显示全长基因的编码区域 
	- 分离一个物种他自己的EST
	- 查找基因家族
	- 从其他物种中查找人类基因的同源物

更多参考：https://genome.ucsc.edu/goldenpath/help/hgTracksHelp.html

**参考资料**

《人类基因组用户指南》

http://www.bio-info-trainee.com/1049.html

http://www.biotrainee.com/thread-29-1-1.html

https://genome.ucsc.edu/goldenpath/help/hgTracksHelp.html#What


## ENCODE

**一、简介**

人类基因组计划的主要目标是产生人类和主要模式生物（包括大肠杆菌（Saccharomyces cerevisiae）、线虫（Caenorhabditis elegans）、果蝇（Drosophila melanogaster）及小鼠（Mus musculus））的精确基因组序列。研究人员可以免费获取这项研究产生的数据公布，这又促进了人类基因组变异图谱的产生和发展（International HapMap Project）。然而，人们仍然不了解基因组如何编码产生多细胞有机体。这就需要精确阐明基因组上重要功能元件并且描绘出这些原件随着细胞种类及时间变化的动态变化情况。这些原件包括编码蛋白、非编码RNA、重要功能原件（如直接调控基因表达，DNA复制和染色体变异）的调控序列。大肠杆菌拥有较小规模基因组，因此首先被破译。对较为复杂的人、小鼠、果蝇及线虫基因组的破译工作仍然处在起始阶段。因此，美国国立人类基因组研究中心（National Human Genome Research Institute (NHGRI)）于2003年启动了ENCODE (Encyclopedia of DNA Elements)计划，该计划的最终目标是描绘出人类基因组的功能元件。在此基础上，对此项计划的扩展，内容包括将对人类基因组破译工作扩展到整个基因组，另外于2007年发起了对模式动物线虫和果蝇基因组破译工作--ENCODE (modENCODE)。

该项目吸引了来自美国、英国、西班牙、日本和新加坡五国32个研究机构的440多名研究人员的参与，经过了9年的努力，研究了147个组织类型，进行了1478次实验，获得并分析了超过15万亿字节的原始数据，确定了400万个基因开关，明确了哪些DNA片段能打开或关闭特定的基因，以及不同类型细胞之间的“开关”存在的差异。证明所谓“垃圾DNA”都是十分有用的基因成分，担任着基因调控重任。证明人体内没有一个DNA片段是无用的。

目前所有数据均全部公开(http://genome.ucsc.edu/ENCODE/ )，并以30篇论文在Nature、Science、Cell、JBC、Genome Biol、Genome Research同时发表(http://www.nature.com/encode )。成为一个互动的百科全书，并可以免费公开获得和利用这些全部的资料和数据。这是迄今最详细的人类基因组分析数据，是对人类生命科学的又一重大贡献。

```{r img1, fig.cap='caption', out.width='80%', fig.align='center', echo=FALSE}
    knitr::include_graphics("image/C3/encode.png")
```

更多信息见：

ENCODE主页 https://www.encodeproject.org/

modENCODE计划主页 http://www.modencode.org/

nature相关主题资源 http://www.nature.com/nature/focus/encode/index.html

ENCODE and modENCODE Data Listings http://www.ncbi.nlm.nih.gov/projects/geo/info/ENCODE.html

modENCODE 计划相关发表文献 http://blog.modencode.org/category/publications

NIH提供的ENCODE计划相关教程： 

* https://www.genome.gov/27553900/encode-tutorials/
* https://www.genome.gov/27562350/encode-workshop-april-2015-keystone-symposia/
* https://www.genome.gov/27561253/encode-workshop-tutorial-october-2014-ashg/
* https://www.genome.gov/27553901/encode-tutorial-may-2013-biology-of-genomes-cshl/
* https://www.genome.gov/27563006/encoderoadmap-epigenomics-tutorial-october-2015-ashg/
* https://www.genome.gov/27555330/encoderoadmap-epigenomics-tutorial-october-2013-ashg/
* https://www.genome.gov/27551933/encoderoadmap-epigenomics-tutorial-nov-2012-ashg/
* http://useast.ensembl.org/info/website/tutorials/encode.html
* https://www.encodeproject.org/tutorials/
* https://www.encodeproject.org/tutorials/encode-meeting-2016/
* https://www.encodeproject.org/tutorials/encode-users-meeting-2015/

**二、常见问题**

1. 6种方式下载ENCODE计划的所有数据

http://www.bio-info-trainee.com/1825.html

所有数据从raw data形式的原始测序数据到比对后的信号文件以及分析好的有意的peaks文件都可以下载。

2.ENCODE计划中enhance和promoter的确定

http://www.biotrainee.com/thread-298-1-1.html

**参考资料**

http://www.bio-info-trainee.com/1825.html

http://blog.sina.com.cn/s/blog_6a17628d0100vpba.html

## GENCODE

**一、介绍**

NHGRI（ National Human Genome Research Institute)于2003年9月启动了ENCODE计划（Encyclopedia Of DNA Elements），旨在发现人类基因组序列中的功能元件。随后，Sanger被授权启动GENCODE项目，旨在整合基因注释结果的整合，比如基因组每条染色体上面有哪些编码蛋白的基因，哪些假基因，哪些lncRNA的基因，它们坐标是什么，基因上面的外显子内含子坐标是什么，UTR区域坐标是什么。在2013年，GENCODE小组也启动了小鼠基因组的注释信息的整合工作。目前，GENCODE基因信息被ENCODE和1000 Genomes等其他项目使用。

**GENCODE 目标 **

1. 提高人类基因注释结构的覆盖度和准确性，特别是蛋白编码的可变剪切突变，非编码位置和假基因等的位置。
2. 建立老鼠基因，包含编码蛋白的可变剪切突变，有转录证据的非编码位置，假基因等。

通过比较小鼠注释的数据和人的基因注释结果可以提高注释结果的准确性。注释工作包括人工矫正，不同方法的计算分析和设计实验证明。有争议的位置会通过实验来验证。数据资源可以Ensembl和UCSC等上公开。

Version 26 (October 2016 freeze, GRCh38) - Ensembl90版本的统计数据

* Total No of Genes：58288
* Protein-coding genes：  19836
* Long non-coding RNA genes： 15778
* Small non-coding RNA genes： 7569
* Pseudogenes： 14694
    - processed pseudogenes: 10704
    - unprocessed pseudogenes: 3469
    - unitary pseudogenes: 206
    - polymorphic pseudogenes: 63
    - pseudogenes: 18
* Immunoglobulin/T-cell receptor gene segments
    - protein coding segments: 410
    - pseudogenes: 234
* Total No of Transcripts： 200401
* Protein-coding transcripts： 80930
    - full length protein-coding: 55406
    - partial length protein-coding: 25524
* Nonsense mediated decay transcripts： 14208
* Long non-coding RNA loci transcripts： 27908
* Total No of distinct translations：   60172
* Genes that have more than one distinct translations： 13546

**二、数据的下载**

FTP地址：ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/

可以下载该数据库的所有资料，而且整理的非常好，自己写脚本很容易处理得到自己想要的信息。

以GENCODE v24为例，在linux系统里面用shell代码即可批量下载所有metadata数据
```	
wget -c -r -np -k -L -A "*metadata*" ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_24/
```
再用代码检查里面的记录数：

```
ls *gz |while read id;do (echo -n $id;echo -n "    " ;zcat $id |wc -l ) ;done
```

我们看看meta data信息的记录数量，这些信息主要是GENCODE与其它主流数据库的对应关系

    gencode.v24.metadata.Annotation_remark.gz    40879
    gencode.v24.metadata.EntrezGene.gz    170466
    gencode.v24.metadata.Exon_supporting_feature.gz    19193542
    gencode.v24.metadata.Gene_source.gz    66206
    gencode.v24.metadata.HGNC.gz    182831
    gencode.v24.metadata.PDB.gz    94547
    gencode.v24.metadata.PolyA_feature.gz    84652
    gencode.v24.metadata.Pubmed_id.gz    209094
    gencode.v24.metadata.RefSeq.gz    75365
    gencode.v24.metadata.Selenocysteine.gz    119
    gencode.v24.metadata.SwissProt.gz    45067
    gencode.v24.metadata.Transcript_source.gz    217202
    gencode.v24.metadata.Transcript_supporting_feature.gz    87375
    gencode.v24.metadata.TrEMBL.gz    61924


还可以下载所有的gtf文件：
```
wget -c -r -np -nd -k -L -A "*gtf.gz" ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_26/
```

gtf文件特别重要，具体可参见第二章节数据格式的介绍。

还可以下载参考转录组及参考蛋白组，我这里还是拿hg19举例：
```
	## ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_26/GRCh37_mapping/gencode.v24lift37.transcripts.fa.gz
	## ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_26/GRCh37_mapping/gencode.v24lift37.lncRNA_transcripts.fa.gz
	## ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_26/GRCh37_mapping/gencode.v24lift37.pc_transcripts.fa.gz
```

其实你有gtf文件，也可以直接从参考基因组序列里面提取这个参考转录组及参考蛋白组，通常是gtf2fasta，随便搜索一下，一大堆方法。

**三、常见问题**

1. 根据gtf格式的基因注释文件得到人所有基因的染色体坐标

http://www.biotrainee.com/thread-472-1-1.html

2.常用的一些提取信息的命令

    获得基因名列表
    ``awk '{if($3=="gene"){print $0}}' gencode.gtf``

    获得所有的"protein-coding transcript"行
    ``awk '{if($3=="transcript" && $20=="\"protein_coding\";"){print $0}}' gencode.gtf``

    获得手动注释级别为1或2的注释结果
    ``awk '{if($0~"level (1|2);"){print $0}}' gencode.gtf``


**参考资料**

http://www.bio-info-trainee.com/1781.html

http://www.gencodegenes.org/about.html

The GENCODE v7 catalog of human long noncoding RNAs, 链接 http://genome.cshlp.org/content/22/9/1775.full

## TCGA

**一、简介**

肿瘤基因组图谱 (The Cancer Genome Atlas,TCGA)计划由美国 National Cancer Institute(NCI)和National Human Genome Research Institute（NHGRI）于2006年联合启动的项目。目前收录了来自11000个病人，33个癌症的数据，2.5P的数据量。 
但是TCGA只对授权的用户开放Level1-Level3数据访问的权限，而普通用户只能访问Level3的分析结果。即TCGA数据库的普通用户无法用Level1的数据进行个性化的高级分析。同时，这些用户也不能有效结合重要的临床信息进行数据的深入挖掘，严重限制用户对数据的有效利用。


收录的癌症类型，详见：https://cancergenome.nih.gov/cancersselected

**Platform Design **

* Applied Biosystems Sequence data
* Agilent 244K Custom Gene Expression
* Agilent SurePrint G3 Human CGH Microarray Kit
* Affymetrix Genome-Wide Human SNP Array
* Agilent 8 x 15K Human miRNA-specific microarray
* Illumina Genome Analyzer DNA Sequencing
等等

更多阅读：https://cancergenome.nih.gov/abouttcga/aboutdata/platformdesign
     
**TCGA数据类型 **

数据类型包括：Clinical Data，Images，Microsatellite Instability (MSI),DNA Sequencing,miRNA Sequencing,
Protein Expression,mRNA Sequencing,Total RNA Sequencing ,Array-based Expression ,DNA Methylation, Copy Number

更详尽的关于数据类型和数据等级，参见：https://cancergenome.nih.gov/abouttcga/aboutdata/datalevelstypes                


**癌症样本组织处理**

1. 癌症病人自愿捐赠肿瘤组织及正常组织样本，由人类癌症生物标本核心资源库承担癌症组织标本和正常组织标本的采集、处理和分配工作。

2. 组织样本经过严格标准处理（处理标准根据不同后续分析类型而异，具体标准请参见），确保质量可以用于进一步分析及测序，并由相关中心采用高通量测序技术进行基因和基因组排序。

3. 获得的临床资料中，可以识别病人身份的信息去掉。


**TCGA个部门分工**

1. TCGA 基因组分析中心（GCC）比对肿瘤和正常组织，寻找异常的基因重组现象。

2. 高通量测序中心（GSC）分析与各癌症或者亚型相关的基因突变、扩增或者缺失。

3. 资料分析中心（GDAC）进行资料的整理、汇总、并提供图表报告给全体研究团队。

**资料分享**

1. 资料综合中心（DCC）集中处理各个团队产生的资料，定期公开于网络上供全世界研究人员利用。

2. 提供公开的资料下载网站入口以方便进行资料搜索和下载

**二、数据下载**

虽然在TCGA中直接下载数据的方法较为繁琐，但是有多个网站提供TCGA数据（包括表达和临床等）完善的整理：GDAC， Cancer Browser和cBioportal是其中整理最为完整和可靠的。GDAC由美国MIT和Harvard共建的Broadinstitute运行，UCSC运行着Cancer Browser 和Xena, cBioportal由MemorialSloan-Kettering Cancer Cente建立，提供较为完善的TCGA数据为基础的各类信息检索服务。

**下载的数据分为两个权限：**

1. 公开的数据

这部分数据不涉及个人信息，下载这部分数据不需要用户认证，包括的数据

* De-identified clinical and demographic data
* Gene expression data
* Copy number alterations in regions of the genome
* Epigenetic data
* Summaries of data compiled across individuals
* Anonymized single amplicon DNA sequence data

2. 受控的数据

因为这部分信息设计到个人信息，所有需要用户申请，包括的数据：

* Primary sequence data (BAM and FASTQ files)
* SNP6 array level 1 and level 2 data
* Exon array level 1 and level 2 data
* VCFs
* Certain information in MAFs

**下载途径 **

1. GDC

自2016年7月15日起，TCGA(The Cancer Genomic Atlas) DATA PORTAL不再提供数据服务，所有数据将转入GDC(Genomic Data Commons) [DATA PORTAL](https://gdc-portal.nci.nih.gov/)。
GDC网站下载TCGA数据，图形界面，操作简单。

GDC提供两种数据下载方式：

（1）对于少量数据，在购物车内点击download，选择cart可以直接下载购物车内的数据

（2）对于大量数据，从购物车中直接下载易出现错误。我们可以点击download下的manifest，然后利用GDC Transfer Tool (gdc-client)，在Terminal内输入如下命令进行批量下载：  ``./gdc-client download -m manifest_xxx.txt ``

更多阅读：http://www.biotrainee.com/thread-821-1-1.html

2. gdac和firehose

网站是：https://gdac.broadinstitute.org/

客户端工具是firehose_get ，https://confluence.broadinstitute.org/display/GDAC/Download

这里的数据也来源于 portal.gdc.cancer.gov，经过了简单的合并，将每种癌症相同类型的数据合并到了一个文件中（例如443个胃癌样本的RNA表达量数据都合并到了一个文件中，非常适合用R进行后续的分析）

更多阅读：

http://www.biotrainee.com/thread-822-1-1.html

http://www.biotrainee.com/thread-822-1-2.html

3. cgdsR和cbioportal

cbioportal地址是：http://www.cbioportal.org/

他们给网站做了一个R包的API为cgdsR，整合和简化了包括TCGA，ICGC以及GEO等多个癌症基因组数据库的内容，提供友好可视化的界面，可供下载。

主要展示基因的somatic 突变谱，拷贝数变化，mRNA&miRNA表达量变化，DNA甲基化以及蛋白质表达的情况，并结合患者的临床资料，展示了KM生存曲线。

更多阅读：http://www.biotrainee.com/thread-824-1-3.html

4. Synapse

Synapse是需要注册的，但是是免费注册的，很简单，用谷歌账户注册即可。

https://www.synapse.org/#!Synapse:syn300013

这里面存放的就是一系列TCGA大文章的数据，一些人整理好的，所以非常方便的可以使用！
比如，我们可以获取 Lung Squamous Cell Carcinoma的生存分析数据

https://www.synapse.org/#!Synapse:syn1446127/version/3


**三、常用分析工具**

1. cBioPortal（cBio Cancer Genomics Portal）

是一个基于TCGA数据库，进行可视化分析的网页。

官网： http://cbioportal.org

a. 首先进入这个网页（http://cbioportal.org），然后可以看到下面这个界面，首先选择你想要分析的数据库和具体的数据

b. 接着勾选你要分析的数据到底都是啥，主要可以分析的是MUT（Mutation，突变），CNA(Copy Number Alterations,拷贝数变化），EXP（mRNA Expression，mRNA表达）和PORT/RPPA（Protein/ phosphoprotein level，蛋白表达或磷酸化变化）。但要注意的是，并不是所有数据都具备这四个选项，大多数只有MUT和CNA这两组数据，有些具有EXP数据和PORT数据。接着，要选择你要研究的基因，有一个下拉菜单可以给你参考，比如会有类似信号通路上的明星分子集合这类，你可以按照需要选择。当然，也可以自己输入基因名

c. 确认后就可以进入结果页面了，主要是显示样本中较为直观的变化，比如突变、缺失、RNA表达、磷酸化变化等等。

2. 用TANRIC来探索癌症中lncRNA功能

更多阅读：http://www.biotrainee.com/thread-999-1-2.html

3. TCGA2BED-可以从TCGA数据库提取数据成bed格式

官网：http://bioinformatics.mdanderson.org/main/TANRIC:Overview

更多阅读：http://www.biotrainee.com/thread-1056-1-1.html

4. TCGA可视化数据库GEPIA

官网：http://gepia.cancer-pku.cn/index.html

这个数据库可以分析有什么功能呢？

a. 给一个基因，告诉你在所有肿瘤组织里面的表达情况，同时还展示其在癌和癌旁的表达

b. 给一个基因，自动做生存分析

c. 给一个基因，告诉你他的共表达基因，或者叫表达模式相似的基因

d. 给两个基因，告诉你他在特定组织的相关性

e. 可以做编码基因，也可以做非编码基因

5. TCGA生存分析oncolnc

官网：http://www.oncolnc.org/

这是一个整合了TCGA的各种RNA数据和患者临床数据，提供生存分析的网站，灰常简单好用。

6. 基于TCGA的蛋白芯片分析神器TCPA

官网：http://www.tcpaportal.org/tcpa/

更多阅读：http://www.biotrainee.com/thread-1293-1-1.html

7. UCSC的cancer genome browser探索TCGA的level3数据

可以对任何癌种，根据任何临床指标进行分sub-group之后进行任何形式的生存分析，比较分析，还有相关分析。 

更多阅读： http://www.biotrainee.com/thread-1086-1-1.html

8. Immunophenogram

网站是：https://tcia.at/home

TCGA的数据挖掘大文章类型，从细胞群里里面区分各种免疫细胞

9. 基于TCGA的甲基化神器mexpress

官网：http://mexpress.be/

整合了TCGA中的DNA甲基化，表达量及临床数据，主要用来探索甲基化，基因表达和临床表型之间的关联

10. oncomine

Oncomine是目前最大的癌症基因芯片数据库
更多阅读：http://www.biotrainee.com/thread-1242-1-1.html

**参考资料**

http://www.biotrainee.com/thread-1080-1-1.html

http://www.biotrainee.com/thread-306-1-1.html

http://www.biotrainee.com/thread-307-1-1.html

http://www.biotrainee.com/thread-827-1-1.html

http://www.biotrainee.com/thread-1290-1-1.html

http://paper.dxy.cn/article/511878

https://cancergenome.nih.gov/abouttcga/overview

##1000 GENOME

**一、简介**

千人基因组计划（1000 Genomes，http://www.internationalgenome.org/ ）于2008-2015年开启的对人的基因组进行测序一个项目，目的是建立人类突变和分型的共同数据库。虽然这个项目已经结束了，EMBL-EBI的数据中心仍然获得了Wellcome Trust的基金资助来维护和扩充这个数据库。 IGSR（International Genome Sample Resource）想实现的目标：

* 保证公众能访问和使用1000 Genomes的数据
* 补充已发表的基因组其他信息
* 向1000 Genomes持续增加新的基因组数据

项目完成的三个阶段

| 阶段             | 目标                                       | 深度     | 策略              | 状态         | 
| -------------- | --------------------|-------------------- | ------ | --------------- |
| 1-low coverage | Assess strategy of sharing data across samples | 2-4X   | 180个样本全基因组测序    | 2008年9月完成  | 
| 2-trios        | Assess coverage and platforms and centres | 20-60X | 2个母亲-父亲-孩子的家系测序 | 2008年10月完成 |    
| 3-gene regions | Assess methods for gene-region-capture   | 50X    | 900个样本1000个基因   | 2009年6月完成  |      



5个大的人种（亚洲人、欧洲人等），25个亚人种。目前，新版共有NA编号开头的1182个人，HG开头的1768个人。

它的官方网站是：有一个ppt讲得很清楚如何通过官网做的data portal来下载数据：https://www.genome.gov/pages/research/der/ichg-1000genomestutorial/how_to_access_the_data.pdf 

**二、数据查询与下载**

**查询数据 **

千人基因组计划 -- 基因组浏览器： http://www.ncbi.nlm.nih.gov/variation/tools/1000genomes/

查询某个SNP的信息

	http://www.ncbi.nlm.nih.gov/projects/SNP/snp_ref.cgi?rs=rs35761398
	http://www.ncbi.nlm.nih.gov/SNP/snp_ref.cgi?rs=2501432
	http://www.ncbi.nlm.nih.gov/SNP/snp_ref.cgi?rs=2502992

在千人基因组计划里面看一个rs就能看到各种人群信息：
http://browser.1000genomes.org/Homo_sapiens/Variation/Population?r=1:24201420-24202420;v=rs2501432;vdb=variation;vf=1849472
这些人群信息，可以画一个网路图！ 只需要变化rs ID号即可，当然并不是所有的rs ID号都在千人基因组计划里面有显示的。

**下载数据 **

下载地址：

	ftp://ftp-trace.ncbi.nih.gov/1000genomes/ftp/
	ftp://ftp.sanger.ac.uk/pub/1000genomes/
	ftp://ftp.ebi.ac.uk/pub/databases/1000genomes/
	ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp
 

直接看最新版的数据，共有NA编号开头的1182个人，HG开头的1768个人！
ftp://ftp.ncbi.nlm.nih.gov/1000genomes/ftp/phase3/data/

也可以按照人种来查看这些数据：ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000_genomes_project/data/

每个人的目录下面都有 四个数据文件夹

	Oct 01 2014 00:00    Directory alignment
	Oct 01 2014 00:00    Directory exome_alignment
	Oct 01 2014 00:00    Directory high_coverage_alignment
	Oct 01 2014 00:00    Directory sequence_read

这些数据实在是太丰富了！

也可以直接看最新版的vcf文件，记录了这两千多人的所有变异位点信息！
可以直接看到所有的位点，具体到每个人在该位点是否变异！
ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/

不过它的基因型信息是通过MVNcall+SHAPEIT这个程序call出来的，具体原理见：http://www.ncbi.nlm.nih.gov/pubmed/23093610

而且网站还提供一些教程：ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000_genomes_project/working/


**表达相关的数据 **

1000G项目中比较重要的表达数据：

1. 465个个体（包括种群：CEU, TSI, GBR, FIN, YRI)的RNAseq数据（mRNA和miRNA）

http://www.geuvadis.org

http://www.ebi.ac.uk/arrayexpress/experiments/E-GEUV-1/samples.html

http://www.ebi.ac.uk/arrayexpress/experiments/E-GEUV-2/samples.html

2. 60个CEU个个体RNAseq

http://www.ebi.ac.uk/arrayexpress/experiments/E-MTAB-197

3. 800 HapMap个体的表达芯片

http://www.ebi.ac.uk/arrayexpress/experiments/E-MTAB-198

http://www.ebi.ac.uk/arrayexpress/experiments/E-MTAB-264

4. 69个YRI个体的RNAseq数据

http://www.ebi.ac.uk/arrayexpress/experiments/E-GEOD-19480

**三、讨论**

1. 1000G中的突变也在dbSNP中吗？

1000G中的SNP和插入缺失突变都提交到了dbSNP中，更长的结构突变被提交到了DGVa。
1000G的vcf文件中有一个ID列，对应的就是dbSNP的rs ID。
因为方法改进，phase 3为代表最终的结果

**参考资料**

http://www.bio-info-trainee.com/1841.html

http://www.internationalgenome.org/about

http://www.internationalgenome.org/category/dbsnp




<!--chapter:end:03-database.Rmd-->

# 统计及可视化 {#statistics}

数据分析过程中，检查要用到各种各样的统计学原理。甚至某个原理本身单独都可以讲解一本书。

## 基本统计概念

## 写在前面

了解生物信息，所有人都绕不开的一部分是统计基础知识和相关实现方式。在这一章中，我们将会简要介绍在实际处理生物数据问题过程中会常碰到统计学概念以及如何使用R语言进行计算和分析。


## 第一节 描述性统计量

什么是统计学问题呢？通常为了解决一类问题，我们会观察一组和该问题相关的样本，利用总体中的这部分样本来推断总体的情况进而得到结论。在通过样本推断总体之前，我们首先需用对已有样本数据进行简单的评估和描述，针对这一需求也就引出了**描述统计量**这一概念。进行描述性统计时，我们最关注的数据两个层面的问题，分别是数据的集中趋势和变异分散性。

### 数据的集中趋势 {-}

面对少则几十个多则上千个数字，第一步通常是观察平均水平。这里介绍三个计算数据平均水平的概念。分别是均值(mean)、中位数(median)和众数(mode)。

**均值**：所有观察值的和除以观察的个数。通常，算数平均是最自然和常用的测度，其问题在于对异常值(outliers)非常敏感。有极端值存在时，均值不能代表样本的绝大多数情况。

**中位数**：所谓中位数，是指所有样本观测值由小到大排序，位于中间的一个（样本数为奇数）或者两个数据的平均值（样本数为偶数）。

通常，当数据分布对称时，中位数近似等于算数平均数；当数据正倾斜时（画出的图像向右边倾斜），中位数小于算数平均数；当数据负倾斜时（画出的图像向左倾斜），中位数大于算数平均数。因此在很多情况下，我们可以通过比较样本的均值和中位数对数据的分布对称性进行一个初判断。

**众数**：在样本的所有观测值中，出现频率最大（出现次数最多）的数值称为众数。这里需要说明的是，当数据量很大而且数值不会多次重复出现时众数并不能给我带来太多的信息。比如当你计算上万个基因的表达量后，得到众数最可能的是0，因为每个基因的表达值或多或少都有一些不同，这时候出现最多的就是那些没有检测的表达基因的0了。

但是在遇到类别数据而非数值型数据时，众数有很大用处，或者说众数是唯一可以用于类别数据的平均数。

在R中，上述提到的均值和中位数可以通过`mean(data)`和`median(data)`函数进行计算,而中位数可以通过`modeest`包的`mfv(data)`函数得到。


### 数据的变异性（离散性）  {-}

平均数显然不能说明一切问题，因为在说明样本数据时我们还必须考虑数据是不是过于分散。例如在篮球队员的投篮平均得分相同的情况下，更重要的时知道他们哪些人发挥得更加稳定。

**极差(range)**指的是一个样本中最大值和最小值之间的差值。在统计学中也称为全距，它能够指出数据的“宽度”（范围）。但是，它和均值一样非常容易受到极端值得影响，而且会受到样本量的明显影响。

针对极差的缺点，统计学中又引入了**分位数(quantiles)**的概念，通俗讲就是把数据的“宽度”细分后再去进行比较从而更好地描述数据的分布形态。分位数用三个点将从小到大排列好的数据分为四个相等部分，而这三个点也就是我们常说的四分位数，分别叫做下四分位数，中位数和上四分位数。当然，除了四分位你也可以计算十分位或者百分位。

分位数的引进能够说明数值的位置，但是无法说明某个数值在该位置出现的概率。为了说明数据的稳定程度，我们可以考虑计算每个数据值到平均数的距离（此处，你可以脑补一个高瘦形的数据曲线和矮胖形的数据曲线），但是样本中所有观测值和均值的偏差和永远是0。为了解决这种正负距离相互抵消的问题，统计学中又引入了**方差(variance)**和**标准差(standard deviation)**的概念。

所谓**方差**是指数值与均值距离平方数的平均数，而**标准差**则是方差的平方根。标准差体现了数据的变异度，标准差越小，数值和均值越近。通常，均值用μ表示，而标准差用σ表示。

在R中，可以通过`quantile()`计算分位数，通过`var()`来计算方差，通过`sd()`来计算标准差。

有个**标准差**的概念，随之而来的问题是当两个样本标准差相同但是均值相差很大时该如何做出区分。统计学中随之引入了**变异系数(coefficient of variation, CV)**的概念，变异系数是指样本标准差除以均值再乘100%。变异系数不会受数据尺度的影响，因此常用来进行不同样本之间变异性的比较。

在实际的数据分析中，如果要比较不同数据集(均值和标准差都不同)之间的数值，通常会引入**z score**的概念，z score 的计算方法是用某一数值减去均值在除以标准差。通过对原始数据进行z变换，我们将不同数据集转化为一个新的均值为0，标准差为1的分布。

### 计算描述性统计量 {-}

在R中，使用`summary()`函数方便的得到一个data frame 的各种描述性统计量。当某一列是数值型变量时，你可以得到该列数据的均值、极值、方差和分位数。

下面我们使用R中内置的数据**Edgar Anderson's Iris Data**进行一些简单展示。

```{r iris}
summary(iris) 

#查看常用的描述统计量
```

### 形象化展示 {-}

所谓形象化展示就是用图示的方法来展示数据结果，比较常见的方法有条形图，箱线图，直方图等。


```{r}
boxplot(iris$Sepal.Length)

# 使用箱线图展示某一列数据的分布情况

```

```{r}
hist(iris$Sepal.Length)

# 使用直方图展示某一列数据的分布情况

```


```{r}
plot(ecdf(iris$Sepal.Length))

# 绘制简单的累积分布函数图展示某一列数据分布情况

```

## 第二节 概率相关

统计学中的大量内容源于概率，学习统计学也就必须要了解一些概率中的基本概念，其中尤为重要的是条件概率，以及延伸出的贝叶斯定理（概率论中最牛也是最难掌握的内容之一）。

### 几个需要理解的概念 {-}

**样本空间(sample space)**是所有可能结果的一个集合，**事件(event)**则是样本空间中所有感兴趣结果的一个子集。某事件的**概率(probability)**是该事件在无限次试验次数中的相对频率。

事件的交集并集和补集，概率的加法法则乘法法则在这里不做更加详细的介绍。

**条件概率(Conditional Probability)**用来描述与其他事件的发生相关的某个事件的概率，通常被描述成在A事件下发生事件B的概率。在书写过程中用“|”表示，例如P(A|B)就是在B发生的情况下A发生的概率。计算方式为AB同时发生的次数除以所有B发生的次数。

条件概率计算方法：$P(A|B)=P(A\cap B)/P(B)$

也可推出 $P(B|A)=P(A\cap B)/P(A)$ => $P(A\cap B)=P(A)\times P(B|A)$

全概率公式：$P(B)=P(A)\times P(B|A)+P(A')\times P(B|A')$ （贝叶斯定理的分母部分）

贝叶斯定理：$P(A|B)=\frac{P(A)\times P(B/A)}{P(A)\times P(B|A)+P(A')\times P(B|A')}$

得到的贝叶斯定理可以帮助我们计算逆条件概率。

在实际的生物学数据处理的过程中，我们还会接触到**灵敏度(sensitivity)**、**特异度(specificity)**、**假阳性(false negative)**和**假阴性(false  positive)** 这几个概念。在疾病方面，对于某一个症状，通俗地说灵敏度指发病后出现症状的概率，特异度不发病时不出现症状的概率。假阳性是指实验结果阳性但是实际为阴性，假阴性是指实验结果为阴性但是实际为阳性。基于灵敏度和特异度，我们经常会看到ROC曲线，通常来说在两个检验中，曲线下面积大的较好。

概率能够告诉我们事件发生的可能性，但如果想要利用概率预测未来的结果并且评估预测的确定性就需要引入概率分布的概念。

### 离散概率分布 {-}

**随机变量**： 样本空间中，对不同事件指定有相应概率的数值函数。

随机变量是可以等于一系列数值的变量，这些值又都和一个特定的概率关联。它的写法是P(X=x)，表示随机变量X取特定值为x时的概率。随机变量包括离散和连续两种形式，所谓离散指变量只能取一些确定值。连续指的是有无限多种可能取值。

**概率分布**也叫概率质量分布，$P(X=x)$。在描述统计量中，样本的频数分布描述每个取值以及对应发生次数，如果样本总数除以对应发生次数，得到的频率分布就类似于这里的概率分布。而所谓的“拟合优度检验”就是比较有限样本频率分布和概率分布的差异。

如果将随机变量和样本对应起来，那么样本中均值的概念在总体（随机变量）中称为**期望**，也叫作**总体均值**，表示为$\mu$（和均值一致）或者$E(X)$。计算方式是将每个可能值和概率相乘再把所有乘机相加。和均值类似，这里的期望无法描述相关数值的分散程度。

同样的，在随机变量中也有类似于样本方差的概念，称为**总体方差**（随机变量方差），用来表示分散程度。其计算公式为 $Var(X) = E(X-\mu)^{2}$。而概率分布的标准差σ同样是方差的平方根。

计算$E(X-\mu)^{2}$时，首先计算每个数值x的$(x-\mu)^{2}$,然后再将结果乘以概率，最后把所有结果相加。

在数据集中，方差和标准差表示的是数据和均值的距离，在概率分布中特定数值概率的分散情况。方差越小，结果越接近期望。

**累加分布函数(cumulative-distribution function, cdf)**:随机变量X，对于X的任一指定值x，概率值$P(X\leq x)$。即随机变量取值不大于指定值的概率。记作$F(x)$

### 常见的离散概率分布 {-}

**几何分布**：进行一组相互独立实验，每次实验有成功失败两种可能且每次试验概率相等，想知道第一次成功需要进行的试验次数。
$$P(X=r)=pq^{r-1}$$
$$P(X>r)=q^{r}$$
$$P(X \leq r)=1-q^{r}$$

期望$E(X)=\frac{1}{x}$;方差 $Var(X)=\frac{q}{p^{2}}$


**二项分布**：进行一组相互独立试验，每次实验有成功失败两种可能且每次试验概率相等且*试验次数有限*，想知道在有限次试验中成功的次数。
$$P(X=r)=C^{r}_{n}p^{r}q^{n-r}$$
期望$E(X)=np$;方差$Var(X)=npq$

二项分布和几何分布差别在于，试验次数固定求成功概率用二项分布，求第一次成功前试验次数用几何分布。

**泊松分布**：常与稀有事件相关，单独事件在给定区间（区间可以是时间或者空间）内随机独立发生，该区间内的事件平均发生次数已知且为有限值。这个值用$\lambda$表示。给定区间内发生r次事件个概率计算公式：$$P(X=r)=\frac{e^{-\lambda}\lambda^{r}}{r!}$$
期望是给定区间内能够期望的事件发生次数λ，方差也是λ。如果一个离散随机变量的一批数据计算后方差和均值近似相等，则可以推测样本符合泊松分布。

当二项分布的p很小且n非常大时，$npq\approx np$,方差和期望近似相等，可以用泊松分布来近似二项分布，从而使计算简化。通常，n大于50且p<0.1时为典型的近似情况。

### 连续概率分布 {-}

当数据是连续分布时，人们关心的是取得某一个特定范围的概率。

**概率密度函数(probability density function, pdf)**:本质是一个函数，用这个函数可以求出在一个范围内的某连续变量的概率，同时该函数可以指出该概率分布的形状。换句话讲，任意a,b两点之间及函数对应曲线下组成的面积等于随机变量X落在ab间的概率。曲线下面积总和是1。

所谓概率密度可以指出各种范围内的概率大小，用面积来表示。只是表示概率的一种方法而非概率本身。

**累加分布函数**a点上的值等于随机变量X取值$\leq a$的概率，也是概率密度函数a左边曲线下的面积。

**正态分布**是连续数据的一种理想状态。正态分布的概率密度函数是一条对称的钟形曲线，均值具有最大的概率密度，偏离均值概率密度越小。参数μ是均值，也是曲线的中央位置，$\sigma$表述曲线的胖瘦。

连续随机变量X符合均值为μ，标准差为$\sigma$的正态分布时写作$X\sim N(\mu,\sigma^{2})$.

线性变换$$aX+b \sim N(a\mu +b,a^{2}\sigma^{2})$$
当X和Y**相互独立**（彼此之间没有影响）时：
$$X+Y \sim N(\mu_{x}+\mu_{y},\sigma_{x}^{2}+\sigma_{y}^{2})$$
$$X-Y \sim N(\mu_{x}-\mu_{y},\sigma_{x}^{2}+\sigma_{y}^{2})$$
期望$E(X_{1}+X_{2}+...+X_{n})=nE(X)$;方差$Var(X_{1}+X_{2}+...+X_{n})=nVar(X)$

当X和Y并不彼此独立时，使用**协方差(Covariance)**来描述两个随机变量间的关系，记做Cov(X,Y)
$$Cov(X,Y)=E[(X-\mu_{x})(Y-\mu_{y})]$$


**二项分布正态近似**：通常情况，当二项分布满足$np\geq 5$ (也有建议$npq\geq 5$)时，可以用正态分布代替二项分布。其中$\mu = np$,$\sigma^{2}=npq$。另外，当泊松分布的λ>15时，也可以用正态分布进行近似。


## 第三节 估计

在通常的试验中，我们获得的信息同时从样本中获得的。想要知道总体参数，只能通过以后样本的参数进行估计。

样本均值是总体均值的点估计。通常，样本均值是用$\overline x$表示，总体均值用$\mu$表示。

在估计总体方差$\sigma^2$时，计算公式为:$$\sigma^2=\frac{\Sigma(x-\overline x)^2}{n-1}$$

用样本方差估计总体方差会使得估计结果偏低，样本越小，两个方差的差别可能就越大。在这里，估计总体方差的公式除以$n-1$，更接近总体方差。另外，总体方差点估计公式通常记做$s^2$,写作：$$s^2=\frac{\Sigma(x-\overline x)^2}{n-1}$$

用样本均值估计总体均值时会产生误差，均值的标准误差是$\sigma/\sqrt{n}$，估计量是$s/\sqrt{n}$。标准误差表示样本均值的分散情况，从公式中我们可以看出，n越大，用样本均值估计总体均值越准去。当样本足够大（大于30）时，即便总体不符合正态分布，但从中取出的样本均值分布仍然近似于正态分布（中心极限定理）。$\overline X$~$N(\mu, \sigma^2/n)$

除了对总体进行点估计以外，我们往往还会对总体进行区间估计，即对通过点估计得到的结果加减一定范围的误差。

## 第四节 相关性分析

本节提到的相关性分析以及后面会提到的t-test, ANOVA 以及回归分析被称为参数检验，这些检验在进行时我们都默认数据符合某种分布，我们的数据符合一定前提条件，通常包括数据符合正态分布和方差相等。当样本数量大于30的时候，根据中心极限定理，我们通常认为数据符合正态分布。在进行t-test和ANOVA分析时，还需要满足样本方差相等条件。

在进行各种检验之前往往需要初步检验一下数据是否符合某种检验的前提条件，如果不符合则应该考虑使用非参数检验等其他方法。

### 正态分布评估 {-}

在评估数据集是否符合正态分布时通常会采用** Shapiro-Wilk’s test**和图示（Q-Q plot）结合的方法。使用Q-Q plot(quantile-quantile plot)的结果比较直观，使用Shapiro-Wilk’s test显著性检验的方法更加准确（相对而言）。

Shapiro-Wilk’s test 结果受样本量的影响非常大，当样本量很大时，即便数据符合正态分布也容易出现p值很小进而拒绝原假设的情况（该检验原假设是样本来自于正态分布）。样本量很小时，即便真实数据不符合正态分布，也可能接受原假设。

这里试举一例

```{r}
# 分别随机生成两组二项分布和指数分布随机数
set.seed(90)
x <- rbinom(15,8,0.7)
y <- rexp(15,0.5)
shapiro.test(x)
shapiro.test(y)


```

可以发现即便我们生成的两个样本都不是正态分布，但是检验的结果仍然没有拒绝原假设（没有拒绝不等于接受原假设）。

好在R中这个函数限制了检测的样本个数，3到5000。因此，同时结合图像来看还是很必要的。

一般使用qqplot来检验是否符合正态分布，R中默认的函数是`qqnorm()`，ggplot2中可以使用函数`qplot()`，qqpubr包是基于ggplot2的简易升级版，操作更加友好，可以使用函数`ggqqplot()`。

下面利用生成的数据绘图。

```{r}
# 生成符合正态分布的一组数据并绘图
z <- rnorm(50)
qqnorm(z)

library(ggplot2)
qplot(sample=z)

library(ggpubr)
ggqqplot(z)

```

### 相关性分析 {-}

Pearson相关系数、Spearman相关系数、Kendall相关系数都可以用来表示变量之间的相关性，一般情况下，使用pearson相关系数更多，如果比较明确样本不符合正态分布的时候可以使用kendall或者spearman相关系数。这三种相关系数都可以通过`cor()`函数来进行计算。下面通过R已有数据集cars，查看汽车车速和刹车距离之间的相关性。

pearson相关系数计算公式
$$r = \frac{\sum{(x-m_x)(y-m_y)}}{\sqrt{\sum{(x-m_x)^2}\sum{(y-m_y)^2}}}$$
其中m表示均值。

```{r}
cor(cars, method = "pearson")

```

### 相关性可视化展示 {-}

可以使用散点图进行两个变量之间的相关性展示。

```{r}
plot(cars)

ggplot(cars, aes(x=speed, y=dist))+ geom_point()

ggscatter(cars,x="speed", y="dist",add = "reg.line", conf.int = T,cor.coef = T)

```

### Pearson 相关性检验 {-}

前面我们只是计算了两个变量之间的相关性，还应该对相关进行显著性检验。原假设为变量之间没有相关性，使用函数为`cor.test()`

```{r}
cor.test(cars$speed,cars$dist,alternative = "two.side", method = "pearson")

```

统计结果中，t表示t检验值，df表示自由度，pvalue是t检验的显著性水平，conf.int表示95%的置信区间，sample estimates 是相关系数。相关系数越接近-1表示负相关，接近1表示正相关。


## 常用统计方法

### 差异分析
差异分析一般指的是寻找

### 生存分析

### 主成分分析

### 进化树

## 常用可视化工具

### 网页工具

### 本地软件

> IGV等

### R绘图系统

这部分内容主要从总体上介绍R的两大作图系统：传统绘图系统和grid绘图系统。尽管这部分不会详细介绍如何去绘制某一种具体的图形，但是了解这部分知识却能帮助你在日后作图时根据需求修改已有的绘图工具。

无论是传统绘图系统还是grid绘图系统，它们都是建立在`grDevices`包的基础上。`grDevices`被称之为绘图引擎，提供了一系列R中的基本绘图函数，负责绘图参数和图片输出，并且几乎所有的高级的绘图函数都是建立在它的基础上。虽然它如此重要，功能十分强大，但由于太过底层，一般只有R包开发者才会深入研究。建立在绘图引擎之上，有两套互不相容的绘图包：`graphics` 包和 `grid` 包，将R的绘图功能从主体上一分为二。

**传统绘图系统**：graphics包之所以称其为传统绘图系统，是因为它实现了很多S语言（S语言由贝尔实验室开发并投入商用）所使用的绘图工具，比如说能用于绘制散点图和条形图的`plot()`，能用于绘制饼图的`pie()`，能用于绘制条形图的`barplot()`，而且每一个作图函数都提供了大量的图形参数用于修改图形。我们可以通过`library(help="graphics")`查看有哪些绘图函数，使用`demo("graphics")`了解传统绘图系统能绘制哪些图形。


**GRID绘图系统**: grid包与传统绘图系统不同，它不负责提供完整的图形函数。它的强大之处在于提供了基于**视图概念**定位区域的强大能力，相当于将R变成了Photoshop，也就是意味着你可以将图形视为不同区域元素的叠加，也意味着你可以做出非常复杂的图形，比如说下图
```{r circle}
library(grid)
grid.newpage()
grid.circle(x=seq(0.1,0.9,length=100),
            y=0.5 + 0.4*sin(seq(0,2*pi,length=100)),
            r=abs(0.1*cos(seq(0, 2 *pi, length=100))))
# 图形来自于R会图形系统（第二版）
```

不过人们基本上不会直接使用grid绘制统计图形，一般是采用基于**画笔系统**的`lattice`包或基于**图形语法**的`ggplot2`。 你或许会问，“知道`lattice`和`ggplot2`是基于`grid`对今后作图由什么帮助吗？”。这里就谈及一点，你可以使用`grid`定制`ggplot2`输出，也就是将多幅图形集中在一起.
```{r multi-plot}
library(ggplot2)
grid.newpage()
pushViewport(viewport(x=0, width=1/3, just="left"))
print(ggplot(mtcars, aes(x=vs)) + 
        geom_bar(),
      newpage=FALSE)
popViewport()
pushViewport(viewport(x=1/3,width=2/3, just="left"))
print(ggplot(mtcars, aes(x=disp, y=mpg)) +
  geom_point(aes(color=drat)),
  newpage=FALSE)
  
```

以上仅仅R语言图形系统的简单介绍，如果你只是想要绘制简单的图形，你只要继续看[如何通过Google来使用ggplot2可视化](http://mp.weixin.qq.com/s/WN4TSMNjH4b6vZgYVjaRvQ) 这一篇就够用了。但是如果你希望用R，而不是PS和AI，去随心所欲绘制出任何图形，那么你需要至少看完如下3本书：

- [ggplot2：数据分析与图形艺术](https://book.douban.com/subject/24527091/)
- [R数据可视化手册](https://book.douban.com/subject/25873705/)
- [R绘图系统](https://book.douban.com/subject/26792674/)

## 可视化举例

### 表达矩阵可视化大全

无论是芯片数据，还是高通量测序，结果总能得到每个样本的基因表达量数据。而将这些数据导入到R语言，就能得到一组表达量矩阵。对于这组数据，至少可以绘制如下图形

- 箱形图(boxplot)
- 小提琴图(vioplot)
- 柱状图(histogram)
- 密度图(density)
- 配对图(gpairs)
- 聚类图(cluster)
- 主成分分析(PCA)
- 热图(heatmap)
- 火山图(volcano plot)

在绘制这些图形之前，首先需要先安装加载一系列包
```{r ,message=FALSE, warning=FALSE}
if (! require('corrplot')) {install.packages("corrplot"); require('corrplot') }
if (! require('gpairs')) {install.packages("gpairs"); require('gpairs') }
if (! require('vioplot')) {install.packages("vioplot"); require('vioplot') }
if (! require('tidyverse')) {install.packages("tidyverse"); require('tidyverse') }
if (! require('RColorBrewer')) {install.packages("RColorBrewer"); require('RColorBrewer') }
source("http://bioconductor.org/biocLite.R")
if (! require('CLL')) {biocLite("CLL"); require('CLL') }
if (! require('pheatmap')) {biocLite("pheatmap"); require('pheatmap') }
if (! require('limma')) {biocLite("limma"); require('limma') }
```

随后是加载绘图所需的表达矩阵数据。分析所用的表达矩阵数据对象(ExpressionSet obejct)由[Affymetrix](#Affymetrix)的 AffyBatch 对象经 gcrma 处理后获得。

原数据为22组样本的12,625个基因表达状况，根据疾病状态分为两组：stable 或 progressive，此处仅选取前8个样本用作演示。

**注**: gcrma是Bioconductor中一个与芯片数据处理相关的包,主要功能是利用序列信息调整背景.
```{r, message=FALSE, warning=FALSE}
data("sCLLex")
sCLLex <- sCLLex[,1:8] # 样本数过多，仅选取前8个
group_List <- sCLLex$Disease # 获取分组信息
exprSet <- exprs(sCLLex) # 获取表达矩阵
head(exprSet, n=3)
```

之后，还要利用`tidyr::gather`将原本的**宽数据**重塑成**长数据**。这一步被称之为数据规整化，是数据分析流程中至关重要的一步，详见Hadley所写的[Tidy data](http://r4ds.had.co.nz/tidy-data.html) 一章。当前数据存在问题是：列名(CLLxx.cCEL)应该是变量名，而这里却是变量的值。
```{r tidy-it}
exprSet <- as.data.frame(exprSet)
exprSet$probe <- row.names(exprSet)
exprSet_L <- tidyr::gather(exprSet, key='sample',value='value',CLL11.CEL:CLL18.CEL)
exprSet_L$group <- rep(group_List, each=nrow(exprSet))
rbind(head(exprSet_L,2), tail(exprSet_L,2))
```

经过这基本处理，便能得到以探针，样本，处理结果和组别信息为列名的数据框。

完成了上述准备工作后，先用箱形图，小提琴图，柱状图，密度图了解各个样本表达数据的分布情况。
```{r distribution}
library(ggplot2)
library(grid)
grid.newpage()
pushViewport(viewport(x=0, width=1/2, just="left"))
p1 <- ggplot(exprSet_L,aes(x=sample,y=value,fill=group)) + geom_boxplot() +
  theme(axis.text.x = element_text(angle=90, hjust=1, vjust=.5))
print(p1, newpage=FALSE)
popViewport()
pushViewport(viewport(x=1/2, width=1/2, just="left"))
p2 <- ggplot(exprSet_L,aes(x=sample,y=value,fill=group)) + geom_violin() + 
  theme(axis.text.x = element_text(angle=90, hjust=1, vjust=.5))
print(p2, newpage=FALSE)
```

其中小提琴图可以认为是箱形图与核密度图的结合体。

```{r density-plot}
library(ggplot2)
# 条形图
p3 <- ggplot(exprSet_L,aes(value,fill=group)) + geom_histogram(bins = 200)+facet_wrap(~sample, nrow = 4)
print(p3)
# 密度图
p4 <- ggplot(exprSet_L,aes(value,col=group)) + geom_density()+facet_wrap(~sample, nrow = 4)
print(p4)
```
这里使用了ggplot2的分面特性，`fact_wrao()`为每一个变量都绘制相应的图。如果不使用分面，还可以观察不同分组数据在同一幅图中的分布情况。
```{r}
p5 <- ggplot(exprSet_L,aes(value,col=group)) + geom_density() 
print(p5)
```

上面提及的四类图形描述的都是每一组样本各自的情况，而下面谈到的`gpairs`和`cluster`则是探索不同变量之间的关系。

**配对关系图**：`gpairs`能够产生分组变量两两之间的关系。要求的输入数据为**宽数据**，即每列代表不同的细胞，每行表示不同的基因。
```{r gpairs, warning=FALSE}
library(gpairs)
gpairs(exprSet[,1:8]
       #,upper.pars = list(scatter = 'stats') 
       #,lower.pars = list(scatter = 'corrgram')
      )
```

**聚类图**： 通过计算基因表达量之间的欧几里得距离，从而对不同样本进行分类。
```{r cluster}
out.dist=dist(t(exprSet[1:8]),method='euclidean')
out.hclust=hclust(out.dist,method='complete')
plot(out.hclust)
```

**热图(heatmap)**： 本质上类似于散点图，只不过它利用颜色深浅表征数据的大小，以矩阵替代点。如果颜色表示为两个样本之间的距离，可以结合上面的聚类图，用来探索不同处理样本之间的差异。
```{r heatmap1}
sampleDistMatrix <- as.matrix( out.dist )
rownames(sampleDistMatrix) <- paste(colnames(exprSet[1:8]), group_List,sep="-")
colnames(sampleDistMatrix) <- NULL
colors <- colorRampPalette( rev(brewer.pal(9, "Blues")) )(255)
pheatmap(sampleDistMatrix,
         clustering_distance_rows = out.dist,
         clustering_distance_cols = out.dist,
         col = colors)
```

当然，也能用热图揭示不同样本间基因表达模式的差异，这里颜色就用来表征基因表达量。
```{r heatmap-2}
# 选择表达量较高的基因
choose_gene <- names(sort(apply(exprSet[1:8], 1, mad),decreasing = T)[1:50])
choose_matrix <- exprSet[choose_gene, ][1:8]
choose_matrix <- scale(choose_matrix)

# 使用pheatmap绘图
rownames(choose_matrix) <- rownames(choose_matrix)
colnames(choose_matrix) <- group_List
colors <- colorRampPalette( rev(brewer.pal(9, "Blues")) )(255)
pheatmap(choose_matrix,
         col = colors,
         fontsize_row = 5)
```
除了`pheatmap`，`gplots`包的`heatmap.2()`和自带的`heatmap`也能绘制热图。热图是否美观主要取决于整体配色。下图直观的表达了一个观点，合理的配色是可视化效果的加分项。
```{r heatmap-3}
heatmap(choose_matrix)
```

**PCA**: PCA本质是散点图，X和Y轴表示不同主成分，不同点之间的距离表示样本之间在两个主成分下的差异。
```{r}
pc <- prcomp(t(exprSet[1:8]),scale=TRUE)
pcx <- data.frame(pc$x)
pcr <- cbind(samples = rownames(pcx),group_List, pcx) 
p <- ggplot(pcr, aes(PC1, PC2)) + geom_point(size = 5, aes(color = group_List)) +
  geom_text(aes(label = samples),hjust = -0.1, vjust = -0.3)
print(p)
```

**火山图**：火山图主要用在差异表达分析之后，用于直观地进行样本间差异基因筛选。本质上依旧是散点图，X轴表示取对数后的倍数变化(log2 Fold Change), Y轴则表示取对数后的显著性水平
```{r}
design <- model.matrix(~factor(group_List))
fit <- lmFit(exprSet[1:8],design)
fit <- eBayes(fit)
DEG <- topTable(fit,coef=2,n=Inf)
with(DEG, plot(logFC, -log10(P.Value), pch=20, main="Volcano plot"))   
```

火山图既然本质上是散点图，那么只要以log2FC和-log10 pvalue 构建数据框，就可以使用 ggplot2 进行绘图，对目标区域进行高亮显示。
```{r}
logFC_cutoff <- with(DEG,mean(abs( logFC)) + 2*sd(abs( logFC)) )
DEG$change <-  as.factor(ifelse(DEG$P.Value < 0.05 & abs(DEG$logFC) > logFC_cutoff,
                              ifelse(DEG$logFC > logFC_cutoff ,'UP','DOWN'),'NOT')
                       )
this_tile <- paste0('Cutoff for logFC is ',round(logFC_cutoff,3),
                    '\nThe number of up gene is ',nrow(DEG[DEG$change =='UP',]) ,
                    '\nThe number of down gene is ',nrow(DEG[DEG$change =='DOWN',])
)

g <-  ggplot(data=DEG, aes(x=logFC, y=-log10(P.Value), color=change)) +
  geom_point(alpha=0.4, size=1.75) +
  theme_set(theme_set(theme_bw(base_size=20)))+
  xlab("log2 fold change") + ylab("-log10 p-value") +
  ggtitle( this_tile  ) + theme(plot.title = element_text(size=15,hjust = 0.5))+
  scale_colour_manual(values = c('blue','black','red'))  ## corresponding to the levels(res$change)
print(g)
```

### 染色体结构图

### 转录本结构图

RNA转录翻译时存在可变剪切，因此同一个基因在不同时间和空间下会产生不同的转录本。[GTF和GFF](#GTF和GFF) 在数据存放类型一章介绍过，用于注释基因组上的基因。

对转录本结构进行可视化的本质在于理解，Y轴用于区分不同的基因，X轴则表示染色体距离，以矩阵长度表示基因元件的起始和结束。因此，可以使用ggplot2[根据GTF画基因的多个转录本结构](https://mp.weixin.qq.com/s/UySUZRIpfX0VhNqveTPHwQ)

```{r }
gtf <- read.table("data/ANXA1.gencode.v7.gtf",stringsAsFactors = F,header = F,comment.char = "#",sep = '\t')
gtf <- gtf[gtf[,2] =='HAVANA',]
gtf <- gtf[grepl('protein_coding',gtf[,9]),]               
gtf$gene <- sapply(as.character(gtf[,9]), function(x) sub(".*gene_name\\s([^;]+);.*", "\\1", x))
draw_gene <- 'ANXA1'
structure <- gtf[gtf$gene==draw_gene,c(1,3:5)]
names(structure) <- c("chr", "record", "start", "end")
idx <- which(structure$record == "transcript")
s <- idx+1
e <- c(idx[-1]-1, nrow(structure))
g <- lapply(seq_along(s), function(i) {
  x <- structure[s[i]:e[i],]
  x$transcript <- i  
  return(x)
}) %>% do.call(rbind, .)
g <- g[g$record == "exon",]
g$transcript <- factor(g$transcript)

library(ggplot2)
ggplot(g) + geom_segment(aes(x=start, xend=end, y=transcript, yend=transcript, color=transcript), size=5) + theme(legend.position="none") + labs(title="ANXA1")
# 代码来源biobabble
```

### reads覆盖图

由于染色体的物理特性，某些区域的测序深度会明显高于其他部分。reads覆盖图用于探索不同read在不同区域的覆盖情况。 其实本身原理很简单，就是把全基因组的每个坐标的depth都得到，然后得到depth的频数，然后画图。
我们可以对每条染色体单独来绘图，也可以针对全基因组来绘图。这里使用是我们存放在data文件下的wgs.bam。 脚本很简单：
```shell
samtools mpileup wgs.bam | perl -alne \ '{if($F[3]>100){$depth{"over100"}++}else{$depth{$F[3]}++}}END{print "$_\t$depth{$_}" foreach sort{$a <=> $b}keys %depth}' \
> wgs.depth.txt
```
得到数据分为两列, 第一列是测序深度，第二列是在该测序深度下有多少个位点。因此第二列加起来就会是染色体的禅读。
```{r,eval=FALSE}
a <- read.table('data/wgs.depth.txt', stringsAsFactors = F)
p1 <- ggplot(a, aes(x=V1, y=V2)) + geom_line(lwd=2)
p2 <- p1 + geom_vline(xintercept = 27) + xlab("测序深度") + ylab("位点数") 
print(p2)
```


### 其它






<!--chapter:end:04-statistics.Rmd-->

# 计算资源及编程{#computering}

## 硬件配置

理论上在**个人Windows电脑**上面做生物信息学数据分析是不实际的，因为太多的生物信息学相关软件的开发者对windows并不熟练，没办法提供完善的基于windows操作系统的软件。
而且**个人Windows电脑**配置肯定不会太高，一般的组学测序数据都是10~500G一个样本，而且很多软件运行的时候对内存要求很高，最后这些数据的分析过程会非常耗时，个人电脑在硬盘，内存，cpu方面均不足以承担这个重任。

所以一般建议使用配置比较高的服务器，而且建议给服务器安装linux系统，ubuntu及centos均可。

* 单人使用，人民币2万以内，可以配置16线程+64G内存+4T硬盘
* 1到5人课题组，人民币10~50万，可以配置64线程+512G内存+64T硬盘
* 5人以上的课题组，一般是学校的超算中心有专门的IT来负责服务器。

服务器主要用来做计算，数据分析的时候使用，并不需要直接接触它。所以大家会用个人电脑来远程登录到服务器，在上面执行各种各样的数据处理命令。
如果是windows电脑，那么建议安装winscp+xshell来连接服务器。
如果是MAC电脑，建议用自带的终端即可，还可以用FileZilla软件进行文件传输。

如果是超算中心提供账号即可使用，不需要看攻略了，自然会有专门的对应的培训。
如果是1到5人课题组，找到联想，IBM等商家自然会拿到详细报价，甚至他们会上门进行ppt讲解。
那么需要在本文详细讲解的就是个人服务器，预算2万左右，改如何配置。

生信领域所涉及的计算往往是非持续性的，我相对较熟悉的RNASeq中计算量较大的就是比对步骤了，而比对往往只需要一次就可以！
这导致配备了一台豪华服务器使用率缺很低，用更少的钱做更多的事，配置一台可用于生物分析的PC机！
这篇配置适用于生物信息实验室、学校、研究所这样的单位，没有专业机房和运维人员，服务器使用率不高，经费有限等请场景下，不差钱的豪门请回避。

以下数据来自2017年6月22日京东数据


| 配件          |配置            | 单价  |
| ------------- |:-------------:| -----:| 
| 主板     | [微星（MSI）X99A RAIDER 主板 （Intel X99/LGA2011-3）](https://item.jd.com/1638311.html )		 |  2299|  
| CPU      | [英特尔（Intel）Extreme系列 酷睿八核i7-6900K 2011-V3 ](https://item.jd.com/2997188.html)		 |7699|  
| 内存     | [金士顿(Kingston)骇客神条 Fury系列 DDR4 2400 16G ](https://item.jd.com/2551276.html)          |  999*8=7992|  
| 电源     | [安钛克（Antec）额定650W EAG650 PRO 模组电源](https://item.jd.com/1331794.html)      			 |   649| 
| 散热器   | [九州风神（DEEPCOOL）大霜塔 CPU散热器](https://item.jd.com/689273.html )     					 |   219| 
| 硬盘     | [西部数据(WD)红盘 8TB SATA6Gb/s 128Mb](https://item.jd.com/2907115.html  )                  |   2999|  
| 机箱     | [酷冷至尊(CoolerMaster)特警342U3版](https://item.jd.com/206853.html)                        |   209|    

总价

| 配置          |总价            | 优缺点  |
| ------------- |:-------------:| -----:| 
| 8核128G内存8T存储|22066| 适合小基因组de novo分析，有参比对分析，主要针对的是de novo需要大内存|
| 6核128G内存8T存储|18666| 比上一套速度慢些，性价比较高，适用于数据不多情况|
| 8核64G内存8T存储 |18070| 常规分析+小数据存储|
| 8核64G内存2T存储 |15520| 常规分析，存储能力几乎没有|
| 6核64G内存2T存储 |12120| 小数据分析，会有速度影响不过影响不大|
| 10核128G内存8T存储|29366| 速度相对快一些，性价比较低，比上不足比下有余|

其中存储是独立于服务器配置的，取决于课题组项目的多少，可以进行按需扩容，下面给出一个42T的raid5磁盘阵列的配置方案，可用于普通主板，配置简单，适用于数据备份存储！
磁盘阵列，[麦沃（MAIWO）K8FSAS 全铝 八盘位磁盘阵列柜](https://item.jd.com/3012307.html ) 单价4999 
硬盘选择[希捷(SEAGATE)酷鹰系列 6TB 7200转256M](https://item.jd.com/3727811.html)总计1599*8=12792 
最后总价是17791，当然，大部分实验室可能并没有这么多的数据，不需要配置这个存储。

## 软件安装

>大部分的数据分析最重要的就是学习使用各种各样的软件了，一般生物信息学软件发布的时候会提供多种种形式以供下载，比如[sratoolkit](https://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/2.6.3/)

```
sratoolkit.2.6.3-centos_linux64.tar.gz 2016-05-25 17:24   61M  
sratoolkit.2.6.3-mac64.tar.gz          2016-05-25 17:25   52M  
sratoolkit.2.6.3-ubuntu64.tar.gz       2016-05-25 17:25   61M  
sratoolkit.2.6.3-win64.zip             2016-05-25 17:23   27M 
```
又或者 NCBI的 [blast](ftp://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/)
```
ncbi-blast-2.6.0+-1.x86_64.rpm	172 MB	
ncbi-blast-2.6.0+-src.tar.gz	19.1 MB	
ncbi-blast-2.6.0+-src.zip	22.3 MB	
ncbi-blast-2.6.0+-win64.exe	79.7 MB 
ncbi-blast-2.6.0+-x64-linux.tar.gz	212 MB	
ncbi-blast-2.6.0+-x64-macosx.tar.gz	122 MB	
ncbi-blast-2.6.0+-x64-win64.tar.gz	79.5 MB 
ncbi-blast-2.6.0+.dmg	123 MB	
```
可以看到软件开发单位提供的有src后缀的源代码文件，还有适用于各个操作系统的预编译版本

### 二进制软件(预编译版本)

作为新手，建议大家直接根据自己的系统下载预编译版本软件，并且直接解压就可以使用啦。
例子如下：
```
cd ~/biosoft
mkdir sratoolkit &&  cd sratoolkit
wget http://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/2.6.3/sratoolkit.2.6.3-centos_linux64.tar.gz
tar zxvf sratoolkit.2.6.3-centos_linux64.tar.gz
~/biosoft/sratoolkit/sratoolkit.2.6.3-centos_linux64/bin/fastdump -h ## 
```
我的系统是linux，所以用上面的代码软件就安装成功可以使用啦，是不是非常简单呢。

### 源码软件

一般的开源软件发布的时候肯定会把源代码放出来，如果是在linux系统下以源代码方式安装软件，那么一般自己的linux系统要有gcc编译器，还需要有一些库文件，这也是大多数新手被坑的地方。

源代码安装三部曲是：

* step1:配置  ./configure 
* step2:编译  make 
* step3:安装  sudo make install 

这个时候就需要对计算机的操作系统有一定的了解了，比如第一个步骤可以设置--prefiex=安装路径，参数指定软件编译后的可执行文件放在具体哪个路径下，默认的路径需要有root权限。
而第二步经常会遇到的库文件缺失，比如安装bwa软件的zlib，安装samtools的 等等。
总之遇到的坑越多，学到的知识越多，只是对初学者来说，这些知识点是否有必要学习，是否应该这么早学习这些。
如果直接用bioconda来管理生物信息学软件，这些坑就可以避免啦。
例子如下：
```
cd ~/biosoft
mkdir samtools &&  cd samtools
wget https://github.com/samtools/samtools/releases/download/1.3.1/samtools-1.3.1.tar.bz2 
tar xvfj samtools-1.3.1.tar.bz2 
cd samtools-1.3.1 
./configure --prefix=/home/jianmingzeng/biosoft/myBin
make
make install
~/biosoft/myBin/bin/samtools --help
~/biosoft/myBin/bin/plot-bamstats --help
```

### 系统自带软件中心

大家都知道，操作系统只是一个生态环境而已，没有上面丰富多彩的软件，它的用处很有限，就好像购买之初的手机，不下载QQ,微信，音频视频软件，根本没办法玩。同样的，做生物信息学数据分析也是如此。
唯一比较麻烦的事情是我们想安装的软件不是QQ、微信这种高频软件，而是科研相关的生物信息学数据分析软，大部分软件都不在系统自带软件中心。不过还是需要了解一下。
首先，不同的系统，安装方式不一样，windows基本没有自带软件中心，MAC有appstore，但是生物信息学相关的很少，linux根据发行版不一样，安装命令不一样，ubuntu的用apt-get，centos的用yum，其余的自己去搜索了解即可。


### conda软件管理

正是因为软件安装的各种坑，有些软件所需环境的配置同样令人头疼，会不断报错提醒你那些东西没有安装。
而系统自带的软件中心又不太可能包含所有的软件，所以出现了conda这样的软件管理中心来弥补，详情请看[conda 官网(]https://bioconda.github.io/)

bioconda里面几乎涵盖了引用率较高的，好用的工具的打包资源，一键式安装，并且各自依赖的环境相互分隔。
每次使用source activate env_name 来激活，使用source deactivate 来退出。
具体软件列表见：https://anaconda.org/bioconda/repo 但是列表不支持搜索，可以去它的github里面去搜索  https://bioconda.github.io/ 
 

首先需要安装这个conda
在官网找到安装包：复制链接,在linux下执行如下代码：
```
wget https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh
sh Miniconda2-latest-Linux-x86_64.sh
```
![](image/C5/install-conda.png)
之后出现yes /no 一律选择yes，这样会默认在你的home目录下新建一个miniconda2的文件夹，然后一切默认即可~
而且默认修改了你的环境变量文件， 即bashrc文件：输入source ~/.bashrc这样conda命令就可以使用了.

然后就可以用conda来安装其它软件，比如cutadapt，如下图所示：
![](image/C5/conda-cutadapt.png)
然后进入miniconda2文件夹里面的pkgs文件夹下面找到安装好的cutadapt软件，直接使用即可：
python /home/jmzeng/miniconda2/pkgs/cutadapt-1.10-py27_0/bin/cutadapt --help
大功告成！！！

当然，也并不是所有的生物信息学相关软件都在conda的安装市场里面,如果要详细掌握它的用法，可以自己慢慢研究它的说明书，一些简单的命令如下：
```
conda search bwa查看可选版本 
在安装时输入conda install bwa=版本号
conda list 查看所有安装的软件
conda update 软件名       可以对软件进行升级:eg.    conda update bwa
conda remove 卸载已经安装的软件
```

### 语言类软件（包）

比如perl,R,python,java,matlab,ruby,C等等

* 其中C源码就是``./configure,make,make install``，也有的就是make，取决于readme，这个也是报错最多的，一般就是没有权限，缺库，很头疼。Bwa/samtools/perl/python
* 然后perl和python软件呢，主要就是模块依赖的问题。Htseq/macs/circos
* R，java,软件非常简单了。Haploview/fastqc/Trimmomatic
* matlab软件，你要是在windows界面用到还好，想去linux用，也折腾好几个星期。
* ruby其它我没有用过啦。

我曾经在论坛上面发过一千个生物信息学软件安装，http://www.biotrainee.com/thread-856-1-1.html 

## 环境变量

Linux是一个多用户的操作系统。每个用户登录系统后，都会有一个专用的运行环境。
通常每个用户默认的环境都是相同的，这个默认环境实际上就是一组环境变量的定义。
环境变量是全局的，设置好的环境变量可以被所有当前用户所运行的程序所使用。
用户可以对自己的运行环境进行定制，其方法就是修改相应的系统环境变量。

环境变量有很多，需要重点理解的就是PATH，很多时候大家看到教程某些软件的使用，比如 
```
cd tmp/chrX_Y/hg19/
wget http://hgdownload.cse.ucsc.edu/goldenPath/hg19/chromosomes/chrX.fa.gz  
wget http://hgdownload.cse.ucsc.edu/goldenPath/hg19/chromosomes/chrY.fa.gz 
gunzip chrX.fa.gz
gunzip chrY.fa.gz
~/biosoft/bwa/bwa-0.7.15/bwa index chrX.fa
~/biosoft/bwa/bwa-0.7.15/bwa mem -t 5 -M chrX.fa read*.fa >read.sam
samtools view -bS read.sam >read.bam
samtools flagstat read.bam
samtools sort -@ 5 -o read.sorted.bam read.bam
samtools view -h -F4 -q 5 read.sorted.bam |samtools view -bS|samtools rmdup - read.filter.rmdup.bam
samtools index read.filter.rmdup.bam
samtools mpileup -ugf ~/tmp/chrX_Y/hg19/chrX.fa read.filter.rmdup.bam |bcftools call -vmO z -o read.bcftools.vcf.gz
```
bwa软件就没有添加到环境变量，所以需要用全路径，指明使用电脑里面什么地方的bwa软件来做数据分析。

而把安装好的软件添加到环境变量的方法有：
   
#### 第一种方法{-}

```
export PATH=/usr/local/webserver/mysql/bin:$PATH  ## 先添加
echo $PATH        ### 再查看
```
上述方法的PATH 在终端关闭后就会消失。所以还是建议通过编辑/etc/profile来改PATH，也可以修改家目录下的.bashrc(即：~/.bashrc)。

#### 第二种方法 {-} 

```
vim /etc/profile
在最后，添加:
export PATH="/usr/local/webserver/mysql/bin:$PATH"
保存，退出，然后运行：           
source /etc/profile，不报错则成功。
```

当然，还有很多其它的环境变量，如下：
```
PATH：        决定了shell将到哪些目录中寻找命令或程序
ROOTPATH:     这个变量的功能和PATH相同，但它只罗列出超级用户（root）键入命令时所需检查的目录。
HOME：        当前用户主目录
USER:         查看当前的用户
LOGNAME：     查看当前用户的登录名。
UID：         当前用户的识别字，取值是由数位构成的字串。
SHELL：       是指当前用户用的是哪种Shell。
TERM ：       终端的类型。
PWD           当前工作目录的绝对路径名，该变量的取值随cd命令的使用而变化。
MAIL：        是指当前用户的邮件存放目录。
HISTSIZE：    是指保存历史命令记录的条数
HOSTNAME：    是指主机的名称，许多应用程序如果要用到主机名的话，通常是从这个环境变量中来取得的。
PS1：         是基本提示符，对于root用户是#，对于普通用户是$，也可以使用一些更复杂的值。
PS2：         是附属提示符，默认是“>”。可以通过修改此环境变量来修改当前的命令符，比如下列命令会将提示符修改成字符串“Hello,My NewPrompt :) ”。# PS1=" Hello,My NewPrompt :) "
IFS：         输入域分隔符。当shell读取输入时，用来分隔单词的一组字符，它们通常是空格、制表符和换行符。
```

## 编程语言

### linux的shell

Linux系统在生物信息学数据处理中的重要性就不用我多说了，鉴于一直有学生问我一些很显而易见的问题，对系统性的学习并理解了Linux系统操作的专业人士来说是显而易见的。我在这里仅以拥有多年处理生物信息学数据经验过来人的角度给大家总结一下，Linux该如何学，该学什么，该花多少工夫，学习重点是什么？

现可以把Linux的学习过程分成三个阶段，总结如下：

#### 第一阶段：把linux系统玩得跟windows系统一样顺畅{-} 

这一阶段的主要目的就是去可视化，熟悉黑白命令行界面。

如何远程连接服务器(使用Xshell，SecureCRT，Putty，VNC等等)，了解你在服务器上面有什么权限。

左右鼠标单击双击如何实现？磁盘文件浏览如何实现？文件操作如何实现？绝对路径和相对路径区别？

需要了解的常见的Linux命令：
```
pwd/ls/cd/mv/rm/cp/mkdir/rmdir/man/locate/head/tail/less/more 
cut/paste/join/sort/uniq/wc/cat/diff/cmp/alias 
wget/ssh/scp/curl/ftp/lftp/mysql/
```

大家可以搜**每天一个linux命令的博客**来跟着练习，或者看一些Linux视频(百度云盘(http://pan.baidu.com/s/1jIvwRD8 )共享了一大堆)，或者关注一些Linux学习相关公众号，加入一些linux社区，论坛，当然如果你只是简单了解，搞生物信息学其实没必要那么深入理解，跟着一本像样的入门书籍（建议看鸟哥Linux私房菜），完整的学习即可！

需要深度理解的概念有：
```
软硬链接区别 
文本编辑，文件权限设置 
打包压缩解压操作(tar/gzip/bzip/ x-j x-c vf) 
软件的快捷方式如何实现？ 
软件如何安装(源码软件，二进制可执行软件，perl/R/python/java软件) 
软件版本如何管理，各种编程语言环境如何管理，模块如何管理？(尤其是大部分没有root权限)
```

这些知识需要深度理解，所以一般初学者肯定会遇到问题，自己要多看教程和视频跟着了练习，但总会有一些不是你立即就能解决的，不要纠结，继续学习，不久之后回过头来就明白了。

翻译成生物信息学语言就是：

```
测序文件在哪里？测序文件有多大？测序文件的格式fastq/fasta是什么？
前几行怎么看，参考基因组如何下载？参考基因组如何建立比对索引？
blast软件如何安装以及使用？
比对结果如何看？结果如何过滤？两次结果如何比较？ 
```

建议自己安装bio-linux系统，里面会自带很多生物信息学测试数据(fastq,fasta,sam,bam,vcf,gff,gtf,bed,MAF……)，安装系统的过程也是熟悉linux的过程，熟悉这些数据格式既能加强生物信息学技巧，也能练习linux操作。

#### 第二阶段：shell脚本，类似于windows的bat批处理文件{-} 

```
懂很多预定义变量：.bashrc/env/HOME/ 
学会一些控制语句：while/if/for/ 批量执行命令 
开始自定义函数，避免重复造轮子。 
了解 awk/sed/grep等文件操作语言，短小精悍，很多时候可以不需要编程。 
正则匹配技巧,find函数使用 
了解编程技巧 ()[]{} $$ 等符合如何使用，技巧有哪些，加快你数据处理能力(建议看shell 13问) 
```

翻译成生物信息学语言就是：

要深度组合这些命令，并且通过shell脚本，把它们在实际生物信息学数据处理中应用起来，需要很多的实践操作，可以借鉴EMBOSS软件套件，fastx-toolkit等基础软件，实现并且模仿该软件的功能。 尤其是SMS2/exonerate/里面的一些常见功能,还有DNA2.0 Bioinformatics Toolbox的一些工具。


基本上要了解到这里才能勉强算是一个合格的生物信息学工程师。

#### 第三阶段：高级运维技巧{-} 

```
w/last/top/qsub/condor/apache/socket/IO/ps/who/uid/ 
磁盘挂载/格式化/重启系统/文件清理/IP查看/网络管理/用户管理/目录结构了解/计划任务/各种库文件了解。
```

这个强烈建议初学者不要过于纠结，稍微了解为佳。
 

对于以上生信相关的Linux三个的学习阶段介绍就到这里了，牢记“不懂的名词，感觉谷歌搜索，多记笔记”。在学习Linux基础知识的同时，就可以开始项目实战，在实战的过程中要随时思考记录如何应用Linux知识辅助生物信息数据处理，并整理学习笔记以及经验分享。
 
其它知识点
R最新版的安装
配置ssh供远程登录
网络服务器配置lamp或者namp
虚拟机屏幕及联网设置
配置shiny，shiny-server，R-studio

### R  {# R-language}

> R语言不仅在生物信息数据处理中发挥着重要作用，也是其它主流数据处理人士的首选工具。现在非常多自学生物信息学的小伙伴必须学的就是R，所以写一个R的系统性入门指导是非常有必要的。我作为老一辈的生信工程师，所以喜欢perl一点，排斥python。我也稍微看过一些python的语法，个人认为R和python几乎是一模一样的。R的特点就是内置了大量的函数，基本上你认识的英文单词都可以是一个函数，即使不是，你也可以自定义为函数。搞清楚了函数和变量，就可以看懂大部分的R代码了。

#### perl/python/R 的比较{-} 

python跟perl都是高级语言， 两个开发的目的不同， perl更面向过程一些，优势是严谨，快。 
python主流面向对象编程， 这个跟R类似， 数据结构等方面有些不同，但可以互相调用。 
实际上以上三者之间可以互相调用部分功能。python的语法并不是很严谨，个人感觉，越偏向自然语言的编程语言越通俗但不严谨，以上，是跟C比较的。
R本身起源于S语言，是主要针对统计的，也是面向对象的。本质上，是把一个比excel功能强大的软件归零化成了命令行吧。
excel高级应用也是要编程的，所以R的初级应用可以当成是没有用户交互界面的excel，细心一点， 把示例代码都打对，当功能强大但不好使版的excel吧， 这样至少心理上不会畏难跟抵触。
内部集成的越多，用户需要做的越少，你用C画个图累死你，用python得写几行，R一行就行了！

#### 六步系统入门R语言 {-} 

##### 第一步：掌握必须要会的基础{-} 

下载R语言的软件：https://cran.r-project.org/bin/windows/base/
下载Rstudio这个R编辑器：https://www.rstudio.com/products/rstudio/download/（在Rstudio里面写代码会比较方便） 

学习help函数（你必须要把help函数用一百次以上，不然你不可能入门的！)

R的特性就是有着大量的包，所以你必须学会安装包：
```
安装包 install.packages(" xxxxxx ") 
加载包 library( xxxxx ) 
查看包的帮助文档help("xxxxx") 或?xxxxx 
获取当前工作区间getwd() 
更改工作区间 setwd( "xxxxxx") 
清除当前对象rm() 
```

安装包你一定会遇到错误，请参考： 
R包终极解决方案！(http://www.biotrainee.com/thread-144-1-1.html) 
R的包（package）(http://www.bio-info-trainee.com/579.html)

你必须要自学R语言基础，或者看书，或者看视频，或者有人手把手教你，书的话，我推荐：《R in Action》、《The Art of_R Programming》。
这些书籍都会提供一些简单的测试代码，你跟着傻瓜式的敲代码就好.
但是实践的过程中，请务必注意一些英文单词（file文件路径/Description简述/Usage用法/Arguments参数/Details详细/value 数值/Examples例子/header 表标题/logical_value 逻辑值/delimiter 分隔符/object 对象/col列/row 行/vector向量/dimensions维度/data数据）。

##### 第二步：明白R中的变量{-} 

向量和因子：向量特简单,没什么好说的，因子太复杂了，我说不清楚，你们慢慢理解。
数据框：就像我们的表格,第一行就是每一列的名字,我们称之为字段,或者变量名。那么对应每列下面的数据就叫做记录或者观测。用data.frame( 字段1,字段2,…. )创建 )
列表：与数据框类似,区别就是每一列向量类型和长度可以不一致。用list( 字段1, 字段2,….. )创建
数组：其形式就像我们玩的模方,每一个面都是一个矩阵数据,用array(数据,各维度的最大值,各维度的名称)


##### 第三步：了解变量的基础操作函数{-} 

变量怎么来，对它们处理什么？ 

我们处理生物信息学数据一般很少会手动创建这些对象，都是从文本里面读取，比如kegg数据库文件，差异分析结果，RNA-seq的表达量矩阵.
但是读入之后，我们的重点就是知道它们变成了什么，该如何去一步步的转换它们。
数据的特性函数也必须要知道，无非就是一些英文单词而已，你经常的玩一下，就慢慢的熟练了。
主要需要熟练的函数有：str,class,names,row.names,col.names,length,unique,view,min,max,summay,table

##### 第四步：可视化你的变量{-} 

了解了R里面的基础变量和对象，也学会了对它们进行简单的转换，接下来就可以尝一尝R的甜头了，对任何数据都可以可视化，简简单单的就可以画一大堆的图。 
plot,boxplot,barplot,pie,hist,pair,它们每个绘图函数都有自己要求的输入数据，特定的可视化结果，请务必在还没熟练使用之前help一下它们，自己主动查看它们好玩的地方，好好自学。

```
dev.new()新建画板 
plot()绘制点线图,条形图,散点图. 
barplot( ) 绘制条形图 
dotchart( ) 绘制点图 
pie( )绘制饼图. 
pair( )绘制散点图阵 
boxplot( )绘制箱线图 
hist( )绘制直方图 
scatterplot3D( )绘制3D散点图. 
低级绘图函数: 
par()　可以添加很多参数来修改图形 
title( )　添加标题 
axis( )　调整刻度 
rug( )　添加轴密度 
grid( )　添加网格线 
abline( )　添加直线 
lines( )　添加曲线 
text( )　添加标签 
legend()　添加图例
```

它们还有一系列的绘图参数(坐标轴、图例，颜色，性状，大小，空白，布局)非常繁琐，想掌握，花费的时间会非常多.
但是很多人直接跳到ggplot的绘图世界了，不想搞那么多底层绘图代码。
但是我看过一个底层R绘图集大成者，就Combining gene mutation with gene expression data improves outcome prediction in myelodysplastic syndromes文章的作者的github里面有。但是对大部分人来说，生信的绘图，都是有套路的，其实都被别人包装成函数了，做好数据，一个函数就出了所有复杂的图。比如热图，cluster等等。

高级可视化不得不提ggplot2了，基本语法是需要学习的，但并不一定要死记硬背，最重要的是学会搜索，如下：

如何通过Google来使用ggplot2可视化
用谷歌搜索来使用ggplot2做可视化（下）

##### 第五步：数据对象的高级操作{-} 

前面我们对向量，数据框，数组，列表都了解了，也知道如何查看数据的特性，但是要进行高级转换，就需要一些时间来学习apply系列函数，aggregate，split等函数的用法。这是一个分水岭，用好了你就算是R入门了。也可以用一些包，比如reshape2，dplyr。

当然，R里面的字符串对象是另外完全不一样的操作模式，建议大家自行搜索学习。

##### 第六步：遨游R的bioconductor世界{-} 

这个是生物信息学特有的，也是为什么我要求搞生物信息学数据处理的人必须学习R，就是为了应用大量的bioconductor包。在这里面所有的对象都不在是基础的向量，数据框，数组，列表了，而是S3，S4对象，这个高级知识点我就不推荐了，你学会了前面的东西，就有了自己的学习经验了，后面的分分钟就搞定了。(其实你永远也搞不定的)
每学一个bioconductor的包，都是自己R水平的提升。
大家可以参考我的博客：http://www.bio-info-trainee.com/tag/bioconductor 我就是这样学习过来的。我还创建了bioconductor中国这个社区，可惜效果不好，有志者可以继续联系我，我们看看有没有可能做起来。
R语言的应用方向。

当然R肯定不只是应用在生物信息学啦，其实它在非常多的地方都有应用，尤其是金融和地理。
在如何一个方向学习R，就不仅仅是R本身的语法了，你需要学习的东西太多了。
我简单列出几个我接触过的方向吧：统计，科学计算，数据挖掘，文本挖掘，基础绘图，ggplot绘图，高级编程，都有着丰富的书籍和视频资料。
炼数成金的R七种武器系列。(强烈推荐，全套视频很容易找到) 

```
《A Handbook of Statistical Analyses_Using_R》 
《Modern Applied Statistics With S》 
《Introduction to Scientific Programming and Simulation Using R》 
《Mastering Scientific Computing with R》 
《Practical Data Science with R》 
《Data Mining explain using R》 
《ggplot2 Elegant Graphics for Data Analysis》 
《R Graphics Cookbook》 
《R Cookbook》 
《R in a Nutshell》 
《R Programming for Bioinformatics》 
《software for data analysis programming with R》
```

看完以上这些，你就是R大神了。当然，前提是你看懂了也会灵活应用。

有小伙伴建议我继续以送视频送书籍的方式来增加浏览量，比如我网盘里面有几千本R语言的PDF书籍，也有十几套视频，但是，我这一篇总结写的太好了，我不想被利益被污染了，希望你可以转发给有需要的人，你的朋友会感激你的转发，让他了解这么多生信前辈的经验分享公众号！

补充：R语言学习的网络资源

R语言官方站 http://www.r-project.org/

R-blogger http://www.r-bloggers.com/

R语言资源汇总 https://github.com/qinwf/awesome-R

R语言搜索引擎 http://www.rseek.org/

R函数在线帮助 http://www.rdocumentation.org/

一个入门级的R在线教程 http://tryr.codeschool.com/

交互式的R在线教程 https://www.datacamp.com 

各种cheatsheet适合打印出来随时查阅

http://cran.r-project.org/doc/contrib/Short-refcard.pdf 

http://www.rstudio.com/resources/cheatsheets/


### perl  {# perl-language}

> Perl是典型的脚本语言，短小精悍，非常容易上手，尤其适合处理文本，数据，以及系统管理。它在老一辈的生物信息学分析人员中非常流行，出于历史遗留原因大家肯定会或多或少地接触 Perl，即使你再怎么推崇Python或者GO等新兴编程语言。

#### 1 入门资料{-} 

两个半小时入门指导：https://qntm.org/files/perl/perl.html
21天学完 perl，自己搜索下载PDF书籍吧！
大小骆驼书，建议都看完，以囫囵吞枣的方式阅读，只看基础知识来入门，难点全部跳过。
官网：https://www.perl.org/
函数如何用：都可以在http://perldoc.perl.org/perl.html 查到
论坛：http://www.perlmonks.org/

#### 2 知识要点{-} 

在看书的同时，你必须记住和熟练使用的知识点是下面这些：

理解perl里面的三种变量表示方式
```
$ 表示单个变量 
用单双引号区别，q(),qq() 
@ 表示多个变量组成的数组，qw() 
% 表示关系型变量-hash 
变量不严格区分类型，没有int/float/double/char这样的概念
```

三种变量都有对应的操作技巧：

* 简单变量的操作函数:
	> Numerical operators:  <,  >, <=, >=, ==, !=, <=>, +, *
	> String operators:    lt, gt, le, ge, eq, ne, cmp, ., x
* 数组操作(pop/push/shift/unshift/splice/map/grep/join/split/sort/reverse) 
# hash操作方式(keys,values,each,delete,exists) 

具体需要在实战里面体会：http://www.biotrainee.com/forum-90-1.html  生信人必练的200个数据处理任务(欢迎大家去练习)

变量内容交换，字符型转为数值型，字符串转为字符数组，字符串变量，heredoc，字符串分割，字符串截取，随机数生成，取整，各种概率分布数，多维矩阵如何操作，进制转换，hash翻转，数组转hash

> 上下文环境 

这个比较复杂： http://www.perlmonks.org/?node_id=738558，就是需要理解你写的程序是如何判断你的变量的，你以为的不一定是你以为的。

> 正则表达式 

这也是一个非常重要的一块内容，基础用法就是m和s，一个匹配，一个替换，比较有趣的就是1,2等等捕获变量。

> 内建变量 

就是perl语言设计的时候定义了一大堆的全局变量($_ $, $0 $> $< $! $. @ARGV @F @_ @INC %ENV %SIG) 
。外表上看起来都是一个$ @ %符号后面加上一大堆的奇奇怪怪的字符，表示一些特殊变量，这也是perl语言饱受诟病的原因。但是有些非常重要，懂了它之后写程序会方便。下载一个表格，里面有近100个预定义变量需要学习的。

> 控制语句(循环/条件/判断) 

if ... elsif ... else ... 
unless/while/next/last/for/foreach
读写文件，脚本实战！
```
while(<>){
#do something !
}
```
这是我最喜欢的一个程序模板，读取文件，根据需要处理文件，然后输出。需要实现非常多的功能，然后就可以自己总结脚本技巧，也能完全掌握perl的各种语法。在生物信息学领域，需要实现的功能有！

> perl 单行命令 

我个人特别喜欢这个知识点，我也专门下载过一本书来学习，把这个[perl单行命令教程](http://www.catonmat.net/blog/perl-one-liners-explained-part-one/)看完就基本上能全明白。学习单行命令的前提是掌握非常多的奇奇怪怪的perl自定义变量和perl的基础语法，用熟练了之后就非常方便，很多生物信息学数据处理过程我现在基本不写脚本，都是直接写一行命令，完全代替了shell脚本里面的awk、sed/grep系列命令，就是熟悉`` -p -a -n -a -l -i -F -M`` 这些参数。

>预定义函数 

perl 是一个非常精简的语言，自定义的函数非常少，连min max这样常见的函数都没有，如果你需要使用这样的功能，要么自己写一个函数，要么使用加强版的包，perl的包非常多。 

下面列出一些，我常用的函数：
```
程序必备： use/die/warn/print/open/close/<>/ 
数学函数：sin/cos/log/abs/rand/srand/sqrt 
字符串函数 ：uc/lc/scaler/index/rindex/length/pos/substr/sprintf/chop/chomp/hex/int/oct/ord/chr/unpack/unencode 
defined/undef
```

> 系统操作相关 

perl语言是跨平台的，因为它的执行靠的是perl解释器，而perl的解释器可以安装在任何机器上面。所以可以用perl来代替很多系统管理工作。
```
系统命令调用 
文件句柄操作(STDIN,STDOUT,STDERR,ARGV,DATA,) 
系统文件管理(mkdir/chdir/opendir/closedir/readdir/telldir/rmdir/)
```

> 一些高级技巧 

```
自定义函数 sub , 参数传递，数组传递，返回值 
模块操作(模块安装，加载，模块路径，模块函数引用) 
引用（变量的变量） 
选择一个好的编辑器-编译器，editplus，notepad++，jEdit，编程习惯的养成。 
搞清楚perl版本的问题，还有程序编码的问题，中文显示的问题。 
```

> 程序调试

> perl常见模块学习 

perl和LWP/HTML做网络爬虫必备，重点是DOM如何解析； 
perl和CGI编程，做网站的神器，重点是html基础知识； 
DBI相关数据库，用perl来操作mysql等，当然，重点是mysql知识； 
GD and GD::Graph 可以用来画图，但是基本上没有人用了，除了CIRCOS画圈圈图火起来了； 
TK模块，可以编写GUI界面程序，但是也几乎没有人用了； 
XML/pdf/excel/Json 相关的模块可以用来读取非文本格式数据，或者输出格式化报告； 
socket通信相关，高手甚至可以写出一个QQ的模仿版本； 
最后不得不提的就是Bioperl了，虽然我从来没有用过，但是它的确对初学者非常有用，大多数人不提倡重复造轮子，但我个人觉得，对初学者来说，重复造轮子是一个非常好的学习方式。大家可以仿造bioperl里面的各个功能，用自己的脚本来实现！

#### 3 复习资料{-} 

如果你感觉学的差不多了，就可以下载一些复习资料，查漏补缺: 
http://michaelgoerz.net/refcards/perl_refcard.pdf 

https://www.cheatography.com/mishin/cheat-sheets/perl-reference-card/ 

http://www.catonmat.net/download/perl.predefined.variables.pdf 

http://www.erudil.com/preqr.pdf 

https://www.cs.tut.fi/~jkorpela/perl/regexp.html 

https://support.sas.com/rnd/base/datastep/perl_regexp/regexp-tip-sheet.pdf


### python  {# python-language}

> Python开发的方向太多了，有机器学习，数据挖掘，网络开发，爬虫等等。其实在生信领域，Python还显现不出绝对的优势，生信的大部分软件流程都是用shell或Perl写的，而且已经足够好用了。我选Python是因为我想顺便学点数据挖掘和机器学习的东西，而且Python这些年越来越火，发展势头远超其他脚本语言，所以学它肯定是没错的。

##### 一、入门标准  {-} 

入门比较难定义，什么程度才算入门呢？

1. 掌握基本的语法，熟练使用python的内置类型、内置函数和数据结构。 
2. 了解一些基本的模块的使用，能够实现一些简单的需求。

后面有一个实例，如果你能简单的做完，那我敢肯定你已经入门了。

##### 二、基本知识点  {-}

###### 1.基本语法  {-}

**缩进**：Python是通过代码缩进来决定代码层次逻辑的，一般约定使用4个空格

**版本问题**：主要包括2.x系列的和3.x系列的，两者语法不同且不兼容，有的模块只能在指定版本下安装。建议使用3.x Python，碰到特殊问题再去使用指定版本

**文件编码声明**：python会去环境变量里寻找python解释器。如果代码里有中文，则要以utf-8编码
```python
#!/usr/bin/env python
#-*- coding: utf-8 –*-
```
**变量定义**：使用前要先定义

dir()：列出一个数据类型或对象的所有方法，非常好用，同help()

**文件操作**：f = open()，f.close()；with open() as f: ，os.path.exists()，os.path.isfile()，os.path.abspath()

**目录操作**：os.mkdir()，os.rmdir()，os.listdir()，os.chdir()

**开发环境选择**：
- Sublime Text 对Python支持挺好，轻量级生化武器（推荐）
- Eclipse+Pydev比较厚重，大型开发比较适合
- Vim/Atom
- PyCharm
- IPython
- WingIDE

###### 2.处理数据  {-}

####### 2.1 基本数据类型：布尔；整型；浮点型；字符串  {-}

```
# 字符串的内置函数，都比较有用
'capitalize', 'casefold', 'center', 'count', 'encode', 'endswith', 'expandtabs', 'find', 'format', 'format_map', 'index', 'isalnum', 'isalpha', 'isdecimal', 'isdigit', 'isidentifier', 'islower', 'isnumeric', 'isprintable', 'isspace', 'istitle', 'isupper', 'join', 'ljust', 'lower', 'lstrip', 'maketrans', 'partition', 'replace', 'rfind', 'rindex', 'rjust', 'rpartition', 'rsplit', 'rstrip', 'split', 'splitlines', 'startswith', 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill'
```

#### 2.2 基本数据结构：列表、元组、字典、集合。  {-}

数据结构就是一种容器，用于在内存中存放我们的数据。

列表：任意元素组成的顺序序列，以位置为索引。
```
# 列表的内置函数
'append', 'clear', 'copy', 'count', 'extend', 'index', 'insert', 'pop', 'remove', 'reverse', 'sort'
```
元组：相当于不可变的列表，防止错误修改，节省内存开销。元组解包
```
# 元组的内置函数
'count', 'index'
```
字典：键值对，没有顺序，键必须是常量。
```
# 字典内置函数
'clear', 'copy', 'fromkeys', 'get', 'items', 'keys', 'pop', 'popitem', 'setdefault', 'update', 'values'
```
集合：没有顺序，元素之间没有重复，相当于舍弃了值的字典。集合操作（&，|，-，^，<，<=，>，>=）
```
# 集合内置函数
'add', 'clear', 'copy', 'discard', 'pop', 'remove', 'update'
'isdisjoint','issuperset','issubset','symmetric_difference','difference','union', 'intersection', 'symmetric_difference_update','intersection_update','difference_update',
```
#######2.3 控制语句   {-}

条件：if…else…

循环：for，while，break，continue

####### 2.4 模块使用  {-}

Python有着非常友好的模块安装方法，一个pip install命令几乎可以安装绝大多数的模块。建议使用模块前多看相关API文档。

最常用的模块有：sys，os，re，csv，gzip，fileinput，random，collections，time；百度上有很多很好的模块使用入门教程。
- 正则表达式 re
- 有序字典 collections.OrderedDict()
- 调用系统命令 subprocess.call()

##### 三、入门实例  {-}

题目：从大量FASTA文件中提取指定序列，并对提取到的序列做某些处理（如求反向互补序列）

描述：假设你有很多测序数据，分别存储在不同文件夹的不同文件里，现在给你一些序列名，要求你从众多数据中提取出特定的序列。

image

image

思路：遍历每一个文件夹；遍历每一个文件；读取文件，判断序列，输出序列（处理），关闭文件；处理数据，添加一个函数即可。

##### 四、精通标准  {-}

当然这只是个噱头，精通的道路是无止境的，下面只是罗列了一些常见的高级特性。

- 切片，推导式，生成器，异常处理

- 高级模块：threading(多线程)，ctypes(调用C程序优化性能)，logging(日志)

- 专业模块：pysam - 处理基因组数据(fasta/fastq/bam/vcf)的Python模块

- Biopython：Python的计算分子生物学和生物信息学工具包

- 编写自己的package：解决某个特定需求，上传到 PyPI，然后你就成为大神了

- 编程规范：写出规范化的代码 Google Python coding style

- 函数式编程：即使代码量暴增也不会影响代码的可读性，调试和Debug也会变得非常简单。

- 面向对象编程：最高级的编程方法，对函数进行分类和封装，让开发“更快更好更强...”

##### 五、最后  {-}

Python只是一门编程语言，一种实现工具，我们可以用很多种语言来替换它，我们之所以选择Python，是因为我们喜欢它给我们带来的便捷。如果你想深入某个领域，其实真正重要的是技术背后的算法。

##### 六、推荐资源  {-}

- Python教程 - 廖雪峰的官方网站

- python初级教程：入门详解

- Python 面向对象（初级篇）

- Python | Codecademy

- Google Python编码风格

- Python正则表达式指南

- 《Python学习手册》

- 《Python编程金典》

- 《Bioinformatics Programming Using Python》 
 
### R包安装终极解决方案  {# R-package}


##### 写在前面：  {-}

>我曾多次强调过R语言在生物信息学中的重要性，也激发了很多小伙伴学习的热情。
学习R语言必然会安装各种各样的包，很多人在这一步就遇到了困难。

刚开始学习R语言的时候我们经常会遇到各种包安装错误，比如`package ‘airway’ is not available (for R version 3.1.0)`等等，

这篇文章，我们就来系统性地整理一些新手可能遇到的问题以及解决方案。

当然，你不一定现在就会遇到，但是如果你遇到了，请记住，可以在这里得到答案！

---
**文章目录如下**：

- 查看已经安装了和可以安装哪些R包
- 如何安装旧版本的包
- 如何切换镜像以及为什么要切换
- 4种常见的R包安装方式

**说明**：该文首发于我的个人博客以及**生信技能树**论坛，请点击文末的**阅读原文**前往查看详细资料。

---
##### 总体思路 {-}

R语言里面的包其实是很简单的，因为它自带了一个安装函数`install.packages()`基本上可以解决大部分问题。

但是如果出问题你需要从如下角度进行分析思考:

- 你的R语言安装在什么机器什么？（linux(ubuntu?centos?),window,mac）
- 你的R是什么版本:(3.1 ? 3.2 ? http://www.bio-info-trainee.com/1307.html )
- 你的安装器是什么版本？（主要针对于bioconductor包的安装）
- 你的联网方式是什么？https ？http ？
- 你选择的R包镜像是什么？

#####自己的R包安在哪里，可以安装哪些R包？ {-}

**首先**在R里面输入`.libPaths()`即可查看当前的R把包安装到了机器的哪个地方，这样可以直接进入目录去查看有哪些包，每个包都会有一个文件夹。

**其次**你可以用`installed.packages()`查看你已经安装了哪些包。

**最后**你可以用`available.packages()`可以查看自己的机器可以安装哪些包！

```
>.libPaths()
[1] "C:/Users/jmzeng/Documents/R/win-library/3.1"
[2] "C:/Program Files/R/R-3.1.0/library"
colnames(installed.packages())
 [1] "Package"               "LibPath"               "Version"              
 [4] "Priority"              "Depends"               "Imports"              
 [7] "LinkingTo"             "Suggests"              "Enhances"             
[10] "License"               "License_is_FOSS"       "License_restricts_use"
[13] "OS_type"               "MD5sum"                "NeedsCompilation"     
[16] "Built"    
ap <- available.packages()
> dim(ap)
```

打开ap变量可以看出，我们想安装的 airway 包根本不在，当然，这肯定是不存在的。 因为 airway 是**bioconductor**的包，并非R默认。 

需要调整`contriburl`参数,如下：

```
> dim(available.packages(contriburl = "https://cran.rstudio.com/bin/windows/contrib/3.2/"))
[1] 8110   17
> dim(ap)
[1] 8155   17
> dim(available.packages(contriburl = "http://bioconductor.org/packages/3.1/bioc/bin/windows/contrib/3.2/"))
[1] 1000   17
> dim(available.packages(contriburl = "http://mirrors.ustc.edu.cn/bioc//packages/3.1/bioc/bin/windows/contrib/3.2/"))
[1] 1000   17
```
用这个参数，可以看不同仓库，甚至不同版本的R包共有哪些资源!

##### 如何安装旧版本的包？ {-}

>既然你点进来看，肯定是有需求。
一般来说，R语言自带的`install.packages`函数来安装一个包时，都是默认安装最新版的。 
但是有些R包的开发者他会引用其它的一些R包，但是它用的是旧版本的功能，自己来不及更新或者疏忽了。 而我们又不得不用他的包，这时候就不得不卸载最新版包，转而安装旧版本包。

**首先**你要用`remove.packages`这个命令把现在的包卸载掉！

**然后**去包的官网上面找到它的旧版本的下载链接：

我这里拿**ggplot2**举例：
http://cran.r-project.org/src/contrib/Archive/ggplot2/
```
#packageurl <- "http://cran.r-project.org/src/contrib/Archive/ggplot2/ggplot2_1.0.1.tar.gz"
install.packages(packageurl, repos=NULL, type="source")

#我这里安装它的1.0.1版本，而不是最新版！
#还有很多其它方法，我就不一一举例了，这个是我认为最方便，最直观的！
# install yesterday's version of checkpoint, by date
install.dates('checkpoint', Sys.Date() - 1)
# install earlier versions of checkpoint and devtools
install.versions(c('checkpoint', 'devtools'), c('0.3.3', '1.6.1'))
```

>很明显，我是在*StackOverflow**上面搜索得到的解决方案，O(∩_∩)O哈哈~ 
你可以参考：http://stackoverflow.com/questions/17082341/installing-older-version-of-r-package


##### 如何切换镜像 {-}

这个技巧很重要，一般来说，R语言自带的`install.packages`函数来安装一个包时，都是用的默认的镜像！ 

如果你是用的**Rstudio**这个IDE，默认镜像就是：https://cran.rstudio.com/ 

如果你直接用的R语言，那么就是：http://cran.us.r-project.org 

但是一般你安装的时候会提醒你选择,而我们需要更改成自己最方便的

```
install.packages(pkgs, lib, repos = getOption("repos"),
    contriburl = contrib.url(repos, type),
    method, available = NULL, destdir = NULL,
    dependencies = NA, type = getOption("pkgType"),
    configure.args = getOption("configure.args"),
    configure.vars = getOption("configure.vars"),
    clean = FALSE, Ncpus = getOption("Ncpus", 1L),
    verbose = getOption("verbose"),
    libs_only = FALSE, INSTALL_opts, quiet = FALSE,
    keep_outputs = FALSE, ...)
``` 
如果是在国内，`install.packages("ABC",repos="http://mirror.bjtu.edu.cn/ ")`,换成北大的镜像你会体验飞一般的感觉！

如果想永久设置，就用**options**修改即可。

如果你是**Rstudio**的IDE，只需要鼠标点击直接进入全局设置，一劳永逸的选择好镜像！ 

插图


你可以check一下每个镜像的包是不是一致的：
```
dim(available.packages(contriburl = "http://cran.rstudio.com/bin/windows/contrib/3.2/"))
```
更改镜像主页及包的版本即可查看所有镜像各提供哪些包！

当然，我们的**bioconductor**其实也是有镜像的，只是大部分人都不知道，也不会去用而已！
```
source("http://bioconductor.org/biocLite.R")
options(BioC_mirror="http://mirrors.ustc.edu.cn/bioc/")
biocLite("RGalaxy")
##这样就用中科大的镜像来下载包啦
##bioconductor还有很多其它镜像：https://www.bioconductor.org/about/mirrors/
##https://stat.ethz.ch/R-manual/R-devel/library/utils/html/chooseBioCmirror.html
```


##### 4种常见的R包安装方式 {-}

###### **R自带函数直接安装**  {-}

这个是最简单的，而且不需要考虑各种包之间的依赖关系。
 
对普通的R包，直接`install.packages()`即可，一般下载不了都是包的名字打错了，或者是R的版本不够。如果下载了安装不了，一般是依赖包没弄好，或者你的电脑缺少一些库文件，如果实在是找不到或者下载慢，一般就用`repos=`来切换一些镜像。

```
 > install.packages("ape")  ## 直接输入包名字即可
Installing package into ‘C:/Users/jmzeng/Documents/R/win-library/3.1’
(as ‘lib’ is unspecified)  ##一般不指定lib，除非你明确知道你的lib是在哪里
trying URL 'http://mirror.bjtu.edu.cn/cran/bin/windows/contrib/3.1/ape_3.4.zip'
Content type 'application/zip' length 1418322 bytes (1.4 Mb)
opened URL   ##根据你选择的镜像，程序会自动拼接好下载链接url
downloaded 1.4 Mb
package ‘ape’ successfully unpacked and MD5 sums checked  
##表明你已经安装好包啦
The downloaded binary packages are in  
##程序自动下载的原始文件一般放在临时目录，会自动删除
    C:\Users\jmzeng\AppData\Local\Temp\Rtmpy0OivY\downloaded_packages
```

对于bioconductor的包，我们一般是
```
source("http://bioconductor.org/biocLite.R") ##安装BiocInstaller
#options(BioC_mirror=”http://mirrors.ustc.edu.cn/bioc/“) 如果需要切换镜像
biocLite("ggbio")
#或者直接
BiocInstaller::biocLite('ggbio') 
## 前提是你已经安装好了BiocInstaller
#某些时候你还需要卸载
remove.packages("BiocInstaller") 
#然后安装新的
```

###### 进入主页找到包下载地址 {-}

可以选择用R自带的下载器来下载，也可以把下面的url拷贝到浏览器用浏览器来下载
```
packageurl <- "http://cran.r-project.org/src/contrib/Archive/ggplot2/ggplot2_0.9.1.tar.gz"
packageurl <- "http://cran.r-project.org/src/contrib/Archive/gridExtra/gridExtra_0.9.1.tar.gz"
install.packages(packageurl, repos=NULL, type="source")
#packageurl <- "http://www.bioconductor.org/packages/2.11/bioc/src/contrib/ggbio_1.6.6.tar.gz"
#packageurl <- "http://cran.r-project.org/src/contrib/Archive/ggplot2/ggplot2_1.0.1.tar.gz"
install.packages(packageurl, repos=NULL, type="source")
```
这样安装的就不需要选择镜像了，也跨越了安装器的版本！

###### 下载到本地后再安装 {-}

```
download.file("http://bioconductor.org/packages/release/bioc/src/contrib/BiocInstaller_1.20.1.tar.gz","BiocInstaller_1.20.1.tar.gz")
##也可以选择用浏览器下载这个包
install.packages("BiocInstaller_1.20.1.tar.gz", repos = NULL)
```

如果你用的**RStudio**这样的IDE，那么直接用鼠标就可以操作了。或者用`choose.files()`来手动选择把下载的源码`BiocInstaller_1.20.1.tar.gz`放到哪里。但这种形式大部分安装都无法成功，因为R包之间的依赖性很强！

###### 命令行版本安装 {-}

如果是**linux**版本，命令行从网上自动下载包如下：

```
sudo su - -c \
"R -e \"install.packages('shiny', repos='https://cran.rstudio.com/')\""
```
如果是linux，命令行安装本地包，在shell的终端
```
sudo R CMD INSTALL package.tar.gz
```
window或者mac平台一般不推荐命令行格式，可视化那么舒心，何必自讨苦吃呢？


### perl模块终极解决方案 {# perl-module}


> 这种细节问题问我，我当然无法直接给出答案咯。毕竟，我的知识积累都不是靠死记硬背的。所以需要取回过头查看一下我的博客，才意识到，

> 我在博客陆陆续续写了7篇教程，关于perl的模块。目录如下：

* [ubuntu服务器解决方案第七讲-perl安装模块](http://www.bio-info-trainee.com/571.html)
* [Perl用cpan在linux上面安装模块](http://www.bio-info-trainee.com/540.html)
* [Perl及R及python模块碎碎念](http://www.bio-info-trainee.com/581.html)
* [perl模块终极解决方案-上](http://www.bio-info-trainee.com/1474.html)
* [perl模块终极解决方案-下](http://www.bio-info-trainee.com/1476.html)
* [perl程序技巧-检验系统环境或模块安装](http://www.bio-info-trainee.com/1479.html)

首先需要自己确定已经安装了哪些模块，都安装在哪里？还有新的模块需要安装到哪里？
然后再学习如何安装新的模块。

一、装Perl模块有两种方法 

* 自动安装 (使用CPAN模块自动完成下载、编译、安装的全过程)
* 手工安装 (去CPAN网站下载所需要的模块，手工编译、安装)


二、使用CPAN模块自动安装 

安装前需要先联上网，有无root权限均可。

初次运行CPAN时需要做一些设置，运行下面的命令即可：

```
perl -MCPAN -e shell
```
> 如果你的机器是直接与因特网相联(拨号上网、专线，etc.)，那么一路回车就行了，只需要在最后一步选一个离您最近的 CPAN 镜像站点。例如我选的是位于国内的http://www.cnblogs.com/itech/admin/ftp://www.perl87.cn/CPAN/ 

> 如果你的机器位于防火墙之后，还需要设置ftp代理或http代理。

其实大部分人的机器都不需要走这一步的，肯定是用过了perl的cpan功能啦，除非你是新买的电脑。

> 下面是常用 cpan 命令。

```
cpan>help
cpan>m
cpan>install Net::Server
cpan>quit
```

我简单解释一下吧：
```
查询：cpan[1]> d /模块名字或者部分名字/
查询结果中会给出所有含有模块名字或者部分名字的模块，选择您所需要的模块进行下载
下载安装：cpan[1]>  install  模块名字
同时会自动安装很多依赖的模块，非常方便。
```

三、手工安装的步骤

> 一般情况下不推荐这种安装方式，但是总是会有迫不得已的时候，而且尝试这种方式，能加深对perl模块的理解。

比如从 [CPAN](http://search.cpan.org/)下载了Net-Server模块0.97版的压缩文件Net-Server-0.97.tar.gz，假设放在/usr/local/src/下。


```perl 
cd /usr/local/src
tar xvzf Net-Server-0.97.tar.gz
cd Net-Server-0.97
perl Makefile.PL
make test
```

如果测试结果报告**all test ok**，你就可以放心地安装编译好的模块了。 
安装模块前，先要确保你对你下载包的文件夹(例子里面是/usr/local/src/)有可写权限(通常以 su 命令获得).
当然，只有root用户才会/usr/local/src/有写入的权限，普通用户把模块文件下载到自己的文件夹即可。

测试自己的模块安装成功与否，用下面的命令，如果没有给出任何输出，那就没问题。
```perl 
perl -MNet::Server -e1
```  

上述步骤适合于 Linux/Unix下绝大多数的Perl模块。可能还有少数模块的安装方法略有差别，所以最好先看看安装目录里的 README 或 INSTALL。

有的时候如果是build.pl的需要以下安装步骤：（需要Module::Build模块支持）
```perl 
perl Build.PL
./Build
./Build test
./Build install
```  

四、cpan和root权限的关系 

> 前面我说过，是否有root权限，都可以调用cpan下载器的，但还是有些微区别的。

如果是root用户，模块其实没有问题，直接用cpan下载器，几乎能解决所有的模块下载安装问题！

但是如果是非root用户，那么就麻烦了，很难用自动的cpan下载器，总有一些模块用cpan下载失败。

这样只能下载模块源码，然后编译，但是编译有个问题，很多模块居然是依赖于其它模块的，你的不停地下载其它依赖模块，最后才能解决，特别麻烦！
但是我仍然不推荐大家用手工下载的方式安装perl模块。
这里我推荐所有的非root用户运行下面的代码获取自己的私人cpan下载器。
```perl 
wget -O- http://cpanmin.us | perl - -l ~/perl5 App::cpanminus local::lib
eval `perl -I ~/perl5/lib/perl5 -Mlocal::lib`
echo 'eval `perl -I ~/perl5/lib/perl5 -Mlocal::lib`' >> ~/.profile
echo 'export MANPATH=$HOME/perl5/man:$MANPATH' >> ~/.profile
```
 
就能拥有一个私人的cpan下载器，~/.profile可能需要更改为.bash_profile, .bashrc, etc等等，取决于你的linux系统！
然后你直接运行cpanm Module::Name，就跟root用户一样的可以下载模块啦！
或者用下面的方式在shell里面安装模块，其中ext是模块的安装目录，可以修改！

```perl 
perl -MTime::HiRes -e 1 > /dev/null 2>&1 || cpanm -v --notest -l ext Time::HiRes;
perl -MFile::Path -e 1 > /dev/null 2>&1 || cpanm -v --notest -l ext File::Path;
perl -MFile::Basename -e 1 > /dev/null 2>&1 || cpanm -v --notest -l ext File::Basename;
perl -MFile::Copy -e 1 > /dev/null 2>&1 || cpanm -v --notest -l ext File::Copy;
perl -MIO::Handle -e 1 > /dev/null 2>&1 || cpanm -v --notest -l ext IO::Handle;
perl -MYAML::XS -e 1 > /dev/null 2>&1 || cpanm -v --notest -l ext YAML::XS;
perl -MYAML -e 1 > /dev/null 2>&1 || cpanm -v --notest -l ext YAML;
perl -MXML::Simple -e 1 > /dev/null 2>&1 || cpanm -v --notest -l ext XML::Simple;
perl -MStorable -e 1 > /dev/null 2>&1 || cpanm -v --notest -l ext Storable;
perl -MStatistics::Descriptive -e 1 > /dev/null 2>&1 || cpanm -v --notest -l ext Statistics::Descriptive;
perl -MTie::IxHash -e 1 > /dev/null 2>&1 || cpanm -v --notest -l ext Tie::IxHash;
perl -MAlgorithm::Combinatorics -e 1 > /dev/null 2>&1 || cpanm -v --notest -l ext Algorithm::Combinatorics;
perl -MDevel::Size -e 1 > /dev/null 2>&1 || cpanm -v --notest -l ext Devel::Size;
perl -MSort::Key::Radix -e 1 > /dev/null 2>&1 || cpanm -v --notest -l ext Sort::Key::Radix;
perl -MSort::Key -e 1 > /dev/null 2>&1 || cpanm -v --notest -l ext Sort::Key;
perl -MBit::Vector -e 1 > /dev/null 2>&1 || cpanm -v --notest -l ext Bit::Vector;
perl -M"feature 'switch'" -e 1 > /dev/null 2>&1 || cpanm -v --notest -l ext feature;
```  

五、非root用户的另一个解决方案 

手动下载local::lib, 这个perl模块，然后自己安装在指定目录，也是能解决模块的问题！

下载之后解压，进入：
```  
perl Makefile.PL --bootstrap=~/.perl  ##这里设置你想把模块放置的目录
make test && make install
echo 'eval $(perl -I$HOME/.perl/lib/perl5 -Mlocal::lib=$HOME/.perl)' >> ~/.bashrc
```  

等待几个小时即可！！！


添加好环境变量之后，就可以用
```  
perl -MCPAN -Mlocal::lib -e 'CPAN::install(LWP)'
```  
这样的模式下载模块了，所有的模块都会存储在$HOME/.perl/lib/perl5 里面！！！
如果是新写的perl程序，需要在开头加入 use local::lib;   
这样才能sets up a local lib at ~/perl5，才能使用该模块！ 
 
当然每次写程序添加这个也实在是太麻烦了，其实你也可以直接打开 ~/.bashrc，然后写入下面的内容
```  
PERL5LIB=$PERL5LIB:/PATH_WHERE_YOU_PUT_THE_PACKAGE/source/bin/perl_module; 
#(笨蛋，这个里面的内容-路径-是需要你修改的，别直接拷贝粘贴哈)
export PERL5LIB
``` 
 
可以把perl模块安装在任何地方，然后通过这种方式去把模块加载到你的perl程序！
 

```
PATH="/home/jmzeng/perl5/bin${PATH:+:${PATH}}"; export PATH;
PERL5LIB="/home/jmzeng/perl5/lib/perl5${PERL5LIB:+:${PERL5LIB}}"; export PERL5LIB;
PERL_LOCAL_LIB_ROOT="/home/jmzeng/perl5${PERL_LOCAL_LIB_ROOT:+:${PERL_LOCAL_LIB_ROOT}}"; export PERL_LOCAL_LIB_ROOT;
PERL_MB_OPT="--install_base \"/home/jmzeng/perl5\""; export PERL_MB_OPT;
PERL_MM_OPT="INSTALL_BASE=/home/jmzeng/perl5"; export PERL_MM_OPT;

```

六、查看perl模块的安装目录 

> 这里指的是查看那些被添加到了环境变量的perl模块安装目录，理论上你可以在如何文件夹里面安装一个perl模块，但是如果不添加到环境变量，意义不大，因为大多数perl程序只会在环境变量里面搜索安装的perl模块，其它地方的模块它们无法调用。

主要就是\@INC这个默认变量 ，可以用下面的代码查看:
```
perl -e '{print "$_\n" foreach @INC}'
```
比如我其中一个服务器显示如下：
```
/home/jmzeng/perl5/lib/perl5/5.18.2/x86_64-linux-gnu-thread-multi
/home/jmzeng/perl5/lib/perl5/5.18.2
/home/jmzeng/perl5/lib/perl5/x86_64-linux-gnu-thread-multi
/home/jmzeng/perl5/lib/perl5
/etc/perl
/usr/local/lib/perl/5.18.2
/usr/local/share/perl/5.18.2
/usr/lib/perl5
/usr/share/perl5
/usr/lib/perl/5.18
/usr/share/perl/5.18
/home/jmzeng/perl5/lib/perl5/5.18.1
/usr/local/lib/site_perl
```

七、查看已经安装哪些perl模块 

不管你有没有root权限，进入 cpan 然后install ExtUtils::Installed模块
这样就可以执行 instmodsh 这个命令了，可以查看当前环境下所有的模块！
为什么可以直接使用呢，因为模块安装的时候就顺便把**instmodsh**给你添加到了环境变量，你可以用 **which instmodsh** 查看它被安装到哪里了。
```
/usr/bin/instmodsh
/home/jmzeng/perl5/bin/instmodsh
```

当然也可以写出脚本来利用这个模块查询其它模块安装信息，主要是写脚本校验用户电脑模块的时候用得着。
```perl
#!/usr/bin/perl
use strict;
use ExtUtils::Installed;
my $inst= ExtUtils::Installed->new();
my @modules = $inst->modules();
foreach(@modules)
{
	my $ver = $inst->version($_) || "???";
	printf("%-12s -- %s\n", $_, $ver);
}
exit 0; 
```


八、模块理论上可以安装到如何地方 

比如非root用户，使用 cpan ，那么一般会创建/home/yourname/.cpan这个隐藏目录下面存储个人的perl模块。
因为不是root用户，所以cpan并不是万能的，有些包是安装不成功的，比如GD模块
而且也可以直接下载模块文件，自己编译到任何目录，只需要在运行自己的脚本的时候加上下面一句话。
```perl
use lib '/home/your-home/perl_lib';
```
但是，大部分情况下，我们安装模块不是因为我们自己写脚本需要，而且一些生物信息学软件对模块有依赖，但是我们很少有能力修改那些生物信息学软件。
所以这条路一般是不走的。
如果有很多自己下载的包，统一安装到了一个目录，就可以把该目录添加目录到@INC。



### Python包安装的小结  {# python-package}

> 想当初刚学习Python的时候，就会用书本里面自带的一些package，用**sys**，**os**也用得很开心。
后来接触到biopython项目，发现原来Python有这么不同功能的包，简直琳琅满目。
不过这也是我痛苦的开始，在服务器上装个包怎么那么费劲呢，缺这少那的。
为了一个包的安装，我得花多少时间啊，还能不能让人好好做科研了。  

一、黑暗时代 

最开始的时候是从源码开始安装，一般`python setup.py install`就执行安装过程了，不过可怕的这些包之间的依赖关系。而且安装的时候，要选择安装目录。对于刚开始学习的我，都要搞晕了。后来还看到一个叫`easy_install`,可以自动解析package之间的依赖关系，生产效率感觉提上去了。不过经常出错，虽然比手动安装好多了，使用起来还是挺费劲的。

二、迎来曙光  

不知道当时从哪里看到说用`pip`会更好，看来没事上上网还是挺有好处的。
而且比`easy_install`什么的不知道高到哪里去了，具体差异可以看[pip vs easy_install](https://packaging.python.org/pip_easy_install/)和[why use pip over easy_install](https://stackoverflow.com/questions/3220404/why-use-pip-over-easy-install)，上面的链接都说得很详细了。对于一般的需求，`pip install --user <package>`就已经很受用了。如果再使用上豆瓣上的PyPi源，那使用体验简直不能太好。

```bash
# Linux/Mac用户修改
# $HOME/.config/pip/pip.conf
[global]
timeout = 60
index-url = https://pypi.doubanio.com/simple

## 注意： 如果使用http链接，需要指定trusted-host参数

[global]
timeout = 60
index-url = http://pypi.douban.com/simple
trusted-host = pypi.douban.com
```

常用的pip用法一般有：

```bash
# 在用户目录安装软件，不需要root 权限
pip install --user <package>
# 搜索package
pip search biopython
# 安装特定版本的package，版本号可以从search的结果中找到
pip install biopython=1.69
# 卸载package
pip uninstall biopython
# 导出已安装的包信息
pip freeze > requirements.txt
# 其他使用方法可以参考pip的帮助说明
pip -h
```

三、发现virtualenv  

如果你只是测试，或者电脑上同一个package安装了好几个版本，那么你一定会喜欢`virtualenv`。
有了它，现在可以在电脑上安装不同版本的package了。
使用方法也很简单，因为`virtualenv`也是Python包，可以直接用`pip`来进行安装。
现在可以用它在电脑上创建不同的虚拟环境了，各个虚拟环境互不干扰，而且对原有的环境不会造成影响，哪天不想玩了，直接把对应的目录删掉就可以了，非常方便 。

```bash
# 安装virtualenv
pip install --user virtualenv
# 创建一个新的环境
mkdir my_envs 
cd my_envs
# 创建一个env_test目录，把相关的package安装到该目录下
virtualenv env_test
# 如果系统上有多个python版本，可以通过参数来指定对应的python版本
virtualenv -p /usr/bin/python2.7 env2.7
# 激活虚拟环境，需要提供具体的虚拟环境安装目录
source env2.7/bin/activate
# 激活后就可以在终端中看到有对应提示，如果想关闭也很简单
deactivate
```

上面的都一些基本的用法，如果创建的虚拟环境比较多，可以借助`virtualenvwrapper`来进行管理，更多的信息可以参考[Python虚拟环境](http://pythonguidecn.readthedocs.io/zh/latest/dev/virtualenvs.html)。

四、大杀器anaconda

当时在学习**virtualenv**的时候，也发现**anaconda**这个东西，不过当时觉得软件太多，而且比较臃肿。
对我这种没装几个软件的来说，**virtualenv**已经够用了。直到有一天我需要安装**tensorflow**的时候，才发现这东西有多方便。
不仅帮你解决依赖关系，而且还会帮你把相关的系统依赖也解决了。相信大家在安装软件的时候，没少遇到missing 什么 libxxxx.so什么的信息。
有的时候为了安装这些系统依赖，真的能把人搞疯。**anaconda**不仅能解决这些问题，还可以安装R里面package啊， 虽然我没用过这个功能。
现在除了**anaconda**，还有精简的**miniconda**和专门为生物信息准备的**bioconda**，虽然名字不一样，只是默认安装时带的package不一样而已，使用方法没什么区别。
下面简单说明下**bioconda**的使用：

```bash
# 首先需要安装conda，我们下载minicoda，文件比较小，下载过程比较快
wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash Miniconda3-latest-Linux-x86_64.sh
```

这样在你的`$HOME`目录里面有一个miniconda，保存安装的软件使用，而且自动在环境变量配置文件`.bashrc`添加新的变量设置。
安装之后可以自己检查一下，是不是有新的不一样的东西。你需要要重新登录一下或者重新加载环境变量`source ~/.bashrc`。
因为网络环境问题，最好修改一下安装软件源，可以使用[清华anaconda开源镜像源](https://mirror.tuna.tsinghua.edu.cn/help/anaconda/)

```bash
# 请注意一上顺序哈
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/
conda config --add channels defaults
conda config --add channels r
conda config --add channels bioconda
```


```bash
# 搜索特定软件包
conda search package-name  
# 新建一个叫py3的环境，这个环境里面我们需要指定使用python3 
conda create -n py3 python=3.5.3
# 激活虚拟环境 
source activate py3   
# 关闭虚拟环境
source deactivate  
# 列出已经创建的虚拟环境  
conda info --evns
```


五、参考  

1. [pip使用豆瓣源](http://www.cnblogs.com/ZhangRuoXu/p/6370107.html)  

2. [Python虚拟环境](http://pythonguidecn.readthedocs.io/zh/latest/dev/virtualenvs.html)







<!--chapter:end:05-computering.Rmd-->

# 生物学基础知识{#biology}

## 中心法则

中心法则是指遗传信息经过DNA转录成信使RNA(mRNA),再由转运RNA（tRNA）和核糖体将mRNA翻译成蛋白质（Protein）,完成遗传信息经DNA—RNA—蛋白质的传递。
也可以由DNA自我复制成DNA,即完成遗传信息由DNA—DNA的传递。
这是所有具有细胞结构的生物所遵循的法则。另外在某些病毒中的RNA自我复制和在某些病毒中以RNA为模版逆转录成DNA的过程，是对中心法则的补充。
![](image/C6/genetic-central-dogma.png)

### DNA 与 复制

脱氧核糖核酸又称去氧核糖核酸，是一种生物大分子，可组成遗传指令，引导生物发育与生命机能运作。主要功能是信息储存，可比喻为“蓝图”或“食谱”。其中包含的指令，是建构细胞内其他的化合物，如蛋白质与核糖核酸所需。带有蛋白质编码的DNA片段称为基因。脱氧核糖核酸又称去氧核糖核酸，是一种生物大分子，可组成遗传指令，引导生物发育与生命机能运作。主要功能是信息储存，可比喻为“蓝图”或“食谱”。其中包含的指令，是建构细胞内其他的化合物，如蛋白质与核糖核酸所需。带有蛋白质编码的DNA片段称为基因。

DNA是一种长链聚合物，组成单位为四种脱氧核苷酸，即：
腺嘌呤脱氧核苷酸（dAMP ）、胸腺嘧啶脱氧核苷酸（dTMP ）、胞嘧啶脱氧核苷酸（dCMP ）、鸟嘌呤脱氧核苷酸（dGMP ）。

而脱氧核糖（五碳糖）与磷酸分子借由酯键相连，组成其长链骨架，排列在外侧，四种碱基排列在内侧。每个糖分子都与四种碱基里的其中一种相连，这些碱基沿着DNA长链所排列而成的序列，可组成遗传密码，指导蛋白质的合成。读取密码的过程称为转录，是以DNA双链中的一条单链为模板转录出一段称为mRNA（信使RNA）的核酸分子。多数RNA带有合成蛋白质的讯息，另有一些本身就拥有特殊功能，例如rRNA、snRNA与siRNA。

在细胞内，DNA能与蛋白质结合形成染色体，整组染色体则统称为染色体组。对于人类而言，正常的人体细胞中含有46条染色体。染色体在细胞分裂之前会先在分裂间期完成复制，细胞分裂间期又可划分为：G1期-DNA合成前期、S期-DNA合成期、G2-DNA合成后期。对于真核生物，如动物、植物及真菌而言，染色体主要存在于细胞核内；而对于原核生物，如细菌而言，则主要存在于细胞质中的拟核内。染色体上的染色质蛋白，如组织蛋白，能够将DNA进行组织并压缩，以帮助DNA与其他蛋白质进行交互作用，进而调节基因的转录。

DNA是高分子聚合物，DNA溶液为高分子溶液，具有很高的粘度，可被甲基绿染成绿色。DNA对紫外线（260nm）有吸收作用，利用这一特性，可以对DNA进行含量测定。当核酸变性时，吸光度升高，称为增色效应；当变性核酸重新复性时，吸光度又会恢复到原来的水平。较高温度、有机溶剂、酸碱试剂、尿素、酰胺等都可以引起DNA分子变性，即DNA双链碱基间的氢键断裂，双螺旋结构解开—也称为DNA的解螺旋。


### RNA 与 转录

[转录（Transcription）](https://en.wikipedia.org/wiki/Transcription_(biology) )是遗传信息由DNA转换到RNA的过程，即信使RNA（mRNA）以及非编码RNA（tRNA、rRNA等）的合成步骤。转录中，一个基因会被读取、复制为mRNA；这个过程由RNA聚合酶（RNA polymerase）和转录因子（transcription factor）所共同完成。

两个真核生物转录必备基础名词：
 - [顺式调控元件](https://en.wikipedia.org/wiki/Cis-regulatory_element)，Cis-regulatory elements (CREs) ：增强或抑制其附近基因转录活性的非编码区域。通常是转录因子结合位点（TFBS）: promoter,  enhancers, silencers, and insulators. 
 - [反式作用因子](https://en.wikipedia.org/wiki/Trans-regulatory_element)：能够特异结合顺式作用元件的因子，多数为蛋白质，如RNA聚合酶，能和RNA聚合酶结合稳定转录起始复合物的蛋白质等。

![transcription](https://upload.wikimedia.org/wikipedia/commons/thumb/9/9b/MRNA.svg/758px-MRNA.svg.png)




#### 1. 转录和转录调控 {-}

转录组即某个物种或特定细胞在某一功能状态下产生的所有RNA的总和，可以揭示基因组序列中哪些序列能够表达，而且还能揭示在何时何处表达，以及转录活跃程度。

在每个细胞中，只有一部分基因是能够转录，此处染色质多为开放状态，其余的基因则处于抑制状态，对应的染色质状态较为紧密。在细胞中，保持一类基因的关闭，而另一类基因开启的状态称为基因调控，基因调控是动态的，在生物生长发育中起重要作用。基因表达的调控包含诸多维度比如信号转导，转录前（包括染色质构象和表观调控等），转录调控，转录后调控（剪切、编辑、转运等等），翻译和翻译后调控。其中多个步骤都是围绕着基因序列（DNA序列—传统的认为是顺式元件Cis）和其结合因子包括蛋白和非蛋白因子（传统地认为反式作用因子Trans）而发生。wiki中给出了转录因子可能参与转录调控的所有途径，见下图：
![TF](https://upload.wikimedia.org/wikipedia/commons/thumb/8/80/Transcription_Factors.svg/1024px-Transcription_Factors.svg.png)


基因的表观和转录调控可以分为两个层面，顺式元件和反式因子，技术方法学也可以简单粗暴地分为以研究顺式元件或反式因子为主的两类方法学：

 1. 已知或候选反式因子（表观和转录因子）和顺式元件的研究手段较成熟，比如经典的EMSA，报告基因，ChIP等。 

 2. __如何研究在native状态下与之相互作用的未知反式因子？__ 基因组非编码区域含有大量的组织特异性的调控序列，包括基因转录增强子(enhancer)、沉默子（silencer）、绝缘子（insulators） 等。这些调控序列通常结合几个，几十个，甚至几百个调控因子（包括转录因子，染色质调控蛋白，组蛋白，RNA分子）以及他们形成的三维结构。用来分离纯化单个调控序列的传统的方法面临的最大的挑战是无法区分结合调控序列的特异性调控因子和细胞内大量的非特异性因子。现有的技术包括locked nucleic acids (LNAs)和transcription activator-like (TAL)蛋白只能用在分离纯化多拷贝的基因组重复序列，比如染色体端粒(telomere)。而其他的常规技术，比如ChIP-seq和ChIA-PE则依赖于单个调控因子或组蛋白修饰，而并不能纯化和分析单个调控序列所结合的多个调控因子以及三维结构。在今年8月刚被cell报道的In Situ capture of chromatin interactions by biotinylated dCAS9中，作者首次利用了“biotinylated dCAS9”的方法建立了高分辨率，位点特异原位DNA-蛋白质以及其他元件的互作网络，为攻克这一难题揭开了序幕 [^ref1]。
![](http://owxb9z5ea.bkt.clouddn.com/17-9-29/88002581.jpg) 


#### 2. 转录与NGS  {-}

转录组测序的分析流程大致可以分成三类，包括基因组比对（Genome mapping）、转录组比对（Transcriptome mapping）、转录组组装（Reference-free assembly。其中第三种主要是用于分析没有参考基因组和基因注释的物种，应用场合较少且不适合新手入门。对于人、小鼠、大鼠等模式物种，通常用前两种方法进行分析。

转录组测序一般是在你有了一部分生理生化的实验结果，如表型差异、生理指标发生明显变化或有效物质含量出现明显差异等等，在这个基础上你可能会问自己，这些现象内在的机制如何。所以，转录组测序核心回答的是那些基因组存在表达差异，这些存在差异的基因都涉及什么功能，是如何发挥作用的。可以根据实验的目的确定需要转录组测序还是表达谱测序。

##### 转录组测序和表达谱测序的区别{-}

 1. 转录组测序 RNA-seq（Transcriptome）
    - 定义： 通过RNA测序，既想得到样本中序列的信息，又需要对序列的表达进行定量和分析。
        - 没有参考基因组的物种，RNA-seq (Transcriptome) 需要进行de novo拼接，对拼接得到的Unigene进行注释；
然后计算de novo得到的Unigene的表达量。所以，这个分析就包含了核酸序列分析和核酸表达定量分析。
        - 对于已有参考基因组的物种，RNA-seq (Transcriptome) 会对测序结果进行基于参考基因组的比对和拼接，从而分析样本中转录本的可变剪切、基因融合、SNP变异等此类针对转录本序列的分析；在序列分析的基础上，在分析转录本的表达量。所以，这个分析也包含了核酸序列的分析和核酸表达量的分析。
    - 对测序的要求：因为涉及到序列的拼接和组装，所以转录组测序对数据量的要求较高（一般单个样本的测序量> 4G），同时一般要求使用双末端测序（Paired end）的数据。

 2. 表达谱测序 RNA-seq（Qualification）
    - 定义：Qualification，顾名思义，这个产品的定义就是只对样本中mRNA进行定量分析，而不需要分析mRNA序列的变化。通常此类分析，需要参考序列。参考序列可以是：基因组序列（有参考基因组的物种）或转录组序列（无参考基因组物种，转录组de novo拼接的结果）。
    - 对此类产品的分析要求仅仅是：将测序得到的数据比对到参考序列上，然后计算参考序列的在样本中的对应表达，而不需要去分析参考序列在样本中是否发生了序列变化（可变剪切、基因融合、SNP等）。一般单个样本的测序量2~3G足够，同时单末端测序（single End）和双末端测序（paired End）的数据均可以分析。


#### 3. 转录调控与NGS  {-}

##### 常用的转录调控测量技术 {-}

参考上文提及的转录因子的调控路径，2012 Shirley Liu 的[Minireview: Applications of Next-Generation Sequencing on Studies of Nuclear Receptor Regulation and Function](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3458226/) 总结得很好，主要有Gene Expression Profiling, Transcription Factor Cistrome Mapping, Epigenome Profiling, Interactions in Three Dimensions, 也直接上图表示：

![NGS4TransReg](http://owxb9z5ea.bkt.clouddn.com/17-9-29/71714242.jpg)

(Next-generation sequencing applications in studies of transcriptional regulation. Applications in red make use mainly of the quantification of abundance, whereas applications in blue make use of sequence-based observations.)

##### NGS分析手段 {-}

对于可以直接参考HOMER提供的教程，学会了就能分析`ChIP-Seq, GRO-Seq, RNA-Seq, DNase-Seq, Hi-C and numerous other types of functional genomics sequencing data sets`。
入门应不成问题，官网在此<http://homer.ucsd.edu/homer>。

#### 总结 {-}
转录是在DNA和蛋白质之间传递信息的关键，调控途径多样，分析手段因此也层出不穷，多结合生物学背景，多看文献，才能想到idea再找数据去分析，光会用却不明白相关关系还是不行的。 
 

### protein {#biology-protein}

### 其它 {#biology-others}

## 组学

介绍常见的6种组学技术，有待完善

### 基因组

基因组学是一个非常广的概念，研究的可以是全基因组的重测序，比如对人类来说，基因组大小是3G(30亿个碱基)，我的全基因组测序共8.9亿条150bp的reads，那么全基因组范围的平均测序深度就是8.9亿*150/30亿~45X。

当然并不是每个实验对象都需要测多达10亿条reads，可以只选择基因组上面的编码基因的外显子设计好特异性的探针后捕获它们进行集中测序，这个就是外显子测序。外显子测序一般测几千万条reads就可以了，虽然测序总量只有全基因组测序的1%，但是外显子区域(30M)也只有全基因组(3G)的1%，所以测序深度仍然可以达到50X以上，足以做大部分的数据分析。

即使是只测全外显子，单个样本的数据量的确不大，但是要推广到千千万万的实验样本，这个累积数据量就可观了，同时分析速度也是一个限制，而且所有的外显子包含的信息量也太多，并不是每一个人都需要。这个时候有针对性的靶向捕获测序就显示出它的优点了。可以是针对十几个基因，或者几百个有明确意义(一般是疾病等性状相关)的基因panel。当然，捕获测序的最终目标也是找到跟参考基因组不一致的位点，来解释为什么这个样本有这样的变异。

如果是癌症研究，那么测序策略也会稍有不同，上面提到的全基因组，全外显子组，靶向测序都可以应用到癌症研究。但是癌症研究有个特色，就是对每个个体不只是要测癌症部位的DNA信息，还需要提取该个体的正常组织进行对照。去除那些个体特异性的变异位点，或者那些没有太大的临床表型意义的germline mutation位点。

最后，值得一提的是，上面的分析都是建立在测序个体的物种参考基因组是已知的前提，所以只需要跟参考基因组进行比较，来找差异。但即使是在测序数据已经海量的今天，也不是每个物种都有了参考基因组，这个时候基因组的de novo测序就可以大展身手了。它的分析流程也完全不一样。

#### 全基因组重测序分析流程分享

> 这里选取的是[ GATK best practice](https://www.broadinstitute.org/partnerships/education/broade/best-practices-variant-calling-gatk-1) 是目前认可度最高的全基因组重测序分析流程，尤其适用于``人类``研究。


##### 流程介绍 {-}

 - [x] bwa(MEM alignment)
 - [x] picard(SortSam)
 - [x] picard(MarkDuplicates)
 - [x] picard(FixMateInfo)
 - [ ] GATK(RealignerTargetCreator) 
 - [ ] GATK(IndelRealigner)
 - [ ] GATK(BaseRecalibrator)
 - [ ] GATK(PrintReads)
 - [x] GATK(HaplotypeCaller)
 - [ ] GATK(GenotypeGVCFs)

在本文，我将会把我的``全基因组重测序``数据走完上面所有的流程，并给出代码和时间消耗情况。

##### 准备工作 {-}
###### 首先是软件安装 {-}
```shell
## Download and install BWA
cd ~/biosoft
mkdir bwa &&  cd bwa
#http://sourceforge.net/projects/bio-bwa/files/
wget https://sourceforge.net/projects/bio-bwa/files/bwa-0.7.15.tar.bz2 
tar xvfj bwa-0.7.15.tar.bz2 # x extracts, v is verbose (details of what it is doing), f skips prompting for each individual file, and j tells it to unzip .bz2 files
cd bwa-0.7.15
make

## Download and install samtools
## http://samtools.sourceforge.net/
## http://www.htslib.org/doc/samtools.html
cd ~/biosoft
mkdir samtools &&  cd samtools
wget https://github.com/samtools/samtools/releases/download/1.3.1/samtools-1.3.1.tar.bz2 
tar xvfj samtools-1.3.1.tar.bz2 
cd samtools-1.3.1 
./configure --prefix=/home/jianmingzeng/biosoft/myBin
make 
make install 
~/biosoft/myBin/bin/samtools --help
~/biosoft/myBin/bin/plot-bamstats --help
cd htslib-1.3.1
./configure --prefix=/home/jianmingzeng/biosoft/myBin
make 
make install
~/biosoft/myBin/bin/tabix 


## Download and install picardtools
## https://sourceforge.net/projects/picard/
## https://github.com/broadinstitute/picard
cd ~/biosoft
mkdir picardtools &&  cd picardtools
wget http://ncu.dl.sourceforge.net/project/picard/picard-tools/1.119/picard-tools-1.119.zip
unzip picard-tools-1.119.zip
mkdir 2.9.2 && cd 2.9.2 
wget https://github.com/broadinstitute/picard/releases/download/2.9.2/picard.jar

## GATK 需要自行申请下载，不能公开
```
###### 其次是必备数据的下载  {-}

```shell
cd ~/reference
mkdir -p  genome/human_g1k_v37  && cd genome/human_g1k_v37 
# http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/ 
nohup wget http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/human_g1k_v37.fasta.gz  &
gunzip human_g1k_v37.fasta.gz
wget http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/human_g1k_v37.fasta.fai
wget http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/README.human_g1k_v37.fasta.txt
java -jar ~/biosoft/picardtools/picard-tools-1.119/CreateSequenceDictionary.jar R=human_g1k_v37.fasta O=human_g1k_v37.dict

cd ~/reference
mkdir -p index/bwa && cd index/bwa   ~/reference/index/bwa/human_g1k_v37  ~/reference/genome/human_g1k_v37/human_g1k_v37.fasta 1>human_g1k_v37.bwa_index.log 2>&1   &
 
mkdir -p ~/biosoft/GATK/resources/bundle/b37
cd ~/biosoft/GATK/resources/bundle/b37
wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/b37/1000G_phase1.indels.b37.vcf.gz
wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/b37/1000G_phase1.indels.b37.vcf.idx.gz
wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/b37/Mills_and_1000G_gold_standard.indels.b37.vcf.gz
wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/b37/Mills_and_1000G_gold_standard.indels.b37.vcf.idx.gz
gunzip 1000G_phase1.indels.b37.vcf.idx.gz
gunzip 1000G_phase1.indels.b37.vcf.gz
gunzip Mills_and_1000G_gold_standard.indels.b37.vcf.gz
gunzip Mills_and_1000G_gold_standard.indels.b37.vcf.idx.gz

mkdir -p ~/annotation/variation/human/dbSNP 
cd ~/annotation/variation/human/dbSNP 
## https://www.ncbi.nlm.nih.gov/projects/SNP/
## ftp://ftp.ncbi.nih.gov/snp/organisms/human_9606_b147_GRCh38p2/
## ftp://ftp.ncbi.nih.gov/snp/organisms/human_9606_b147_GRCh37p13/
nohup wget ftp://ftp.ncbi.nih.gov/snp/organisms/human_9606_b147_GRCh37p13/VCF/All_20160601.vcf.gz &
wget ftp://ftp.ncbi.nih.gov/snp/organisms/human_9606_b147_GRCh37p13/VCF/All_20160601.vcf.gz.tbi

```

> 上述代码都是可以直接在linux/MAC里面运行的，只有当软件安装完毕，还有参考基因组等必备文件准备齐全了，才能正式进入全基因组重测序分析流程！
 

> 上面是我的全基因组数据fastq文件的截图，测序分成了5条lane，每条lane的数据量不一致。

##### 数据分析 {-}

数据分析的主线就是 fastq-->bam-->vcf  这样的数据转换，中间选择不同的工具已经使用不同的参数，所以流程得到的结果会略微不同。这里仅介绍我多年实践的最佳流程，希望对读者有所启发。

###### 首先是fastq2bam步骤 {-}
####### 代码 {-}

```shell
module load java/1.8.0_91
GENOME=/home/jianmingzeng/reference/genome/human_g1k_v37/human_g1k_v37.fasta
INDEX=/home/jianmingzeng/reference/index/bwa/human_g1k_v37
GATK=/home/jianmingzeng/biosoft/GATK/GenomeAnalysisTK.jar
PICARD=/home/jianmingzeng/biosoft/picardtools/2.9.2/picard.jar
DBSNP=/home/jianmingzeng/annotation/variation/human/dbSNP/All_20160601.vcf.gz
SNP=/home/jianmingzeng/biosoft/GATK/resources/bundle/b37/1000G_phase1.snps.high_confidence.b37.vcf.gz
INDEL=/home/jianmingzeng/biosoft/GATK/resources/bundle/b37/Mills_and_1000G_gold_standard.indels.b37.vcf.gz
TMPDIR=/home/jianmingzeng/tmp/software
## samtools and bwa are in the environment 
## samtools Version: 1.3.1 (using htslib 1.3.1)
## bwa Version: 0.7.15-r1140
 
: '
'
## please keep the confige in three columns format, which are fq1 fq2 sampe
cat $1 |while read id
do
	arr=($id)
	fq1=${arr[0]}
	fq2=${arr[1]}
	sample=${arr[2]}
	#####################################################
	################ Step 1 : Alignment #################
	#####################################################
	echo bwa `date`
	bwa mem -t 5 -R "@RG\tID:$sample\tSM:$sample\tLB:WGS\tPL:Illumina" $INDEX $fq1 $fq2 > $sample.sam
	echo bwa `date`
	#####################################################
	################ Step 2: Sort and Index #############
	#####################################################
	echo SortSam `date`
	java -Djava.io.tmpdir=$TMPDIR    -Xmx40g -jar $PICARD SortSam SORT_ORDER=coordinate INPUT=$sample.sam OUTPUT=$sample.bam 
	samtools index $sample.bam
	echo SortSam `date`

	#####################################################
	################ Step 3: Basic Statistics ###########
	#####################################################
	echo stats `date`
	samtools flagstat $sample.bam > ${sample}.alignment.flagstat
	samtools stats  $sample.bam > ${sample}.alignment.stat
	echo plot-bamstats -p ${sample}_QC  ${sample}.alignment.stat
	echo stats `date`

	#####################################################
	####### Step 4: multiple filtering for bam files ####
	#####################################################

	###MarkDuplicates###
	echo MarkDuplicates `date`
	java -Djava.io.tmpdir=$TMPDIR    -Xmx40g -jar $PICARD MarkDuplicates \
	INPUT=$sample.bam OUTPUT=${sample}_marked.bam METRICS_FILE=$sample.metrics  
	echo MarkDuplicates `date`


	###FixMateInfo###
	echo FixMateInfo `date`
	java -Djava.io.tmpdir=$TMPDIR    -Xmx40g -jar $PICARD FixMateInformation \
	INPUT=${sample}_marked.bam OUTPUT=${sample}_marked_fixed.bam SO=coordinate  
	samtools index ${sample}_marked_fixed.bam
	echo FixMateInfo `date`
	
	echo ${sample}_marked_fixed.bam >>files.bamlist
	
	rm $sample.sam $sample.bam ${sample}_marked.bam
done 
samtools merge -@ 5  -b files.bamlist  merged.bam
samtools index merged.bam
```
> 上面的代码有一点长，希望大家能用心的来理解，其实就是一个批量处理，对5条lane的测序数据循环处理。
其实正式流程里面我一般是并行的，而不是循环，这里是为了给大家秀一下时间消耗情况，让大家对全基因组重测序分析有一个感性的认知。
最后把所有lane的数据用 ``samtools merge `` 合并成一个非常大(60G)的bam文件 .

####### 时间消耗情况探索 {-}

对L1来说，时间消耗如下：
```
[main] Real time: 15870.794 sec; CPU: 77463.156 sec
picard.sam.SortSam done. Elapsed time: 45.60 minutes.
picard.sam.markduplicates.MarkDuplicates done. Elapsed time: 64.20 minutes.
picard.sam.FixMateInformation done. Elapsed time: 58.05 minutes.
```
> 总共耗时约7.2小时，仅仅是对10G的fastq完成比对压缩排序去PCR重复。

如果是其它文件大小的fastq输入数据，那么这个流程耗时如下：
```
[main] Real time: 9527.240 sec; CPU: 47758.233 sec
[main] Real time: 16000.325 sec; CPU: 80595.629 sec
[main] Real time: 29286.523 sec; CPU: 147524.841 sec
[main] Real time: 28104.568 sec; CPU: 141519.377 sec

picard.sam.SortSam done. Elapsed time: 29.02 minutes.
picard.sam.SortSam done. Elapsed time: 61.26 minutes.
picard.sam.SortSam done. Elapsed time: 98.39 minutes.
picard.sam.SortSam done. Elapsed time: 117.16 minutes.

picard.sam.markduplicates.MarkDuplicates done. Elapsed time: 35.52 minutes.
picard.sam.markduplicates.MarkDuplicates done. Elapsed time: 54.41 minutes.
picard.sam.markduplicates.MarkDuplicates done. Elapsed time: 90.40 minutes.
picard.sam.markduplicates.MarkDuplicates done. Elapsed time: 93.03 minutes.

picard.sam.FixMateInformation done. Elapsed time: 35.92 minutes.
picard.sam.FixMateInformation done. Elapsed time: 66.31 minutes.
picard.sam.FixMateInformation done. Elapsed time: 131.65 minutes.
picard.sam.FixMateInformation done. Elapsed time: 122.31 minutes.
```

前面我们说过，这5条lane的数据其实是可以并行完成这几个步骤的，最长耗时约12小时。
每个数据处理我都分配了``5个线程``，``40G的内存``。

###### GATK重新处理bam文件 {-}

> 主要是针对上一个步骤合并了5个lane之后的``merge.bam``文件
```
-rw-rw-r-- 1 jianmingzeng jianmingzeng  57G Jun  7 11:32 merged.bam
-rw-rw-r-- 1 jianmingzeng jianmingzeng 8.4M Jun  7 12:05 merged.bam.bai
```
###### merge后需要AddOrReplaceReadGroups处理 {-}

> 因为不同的lane出来的数据都是我本人的全基因组重测续数据，后续处理应该是当做一个样本的，所有需要AddOrReplaceReadGroups处理，代码是：

```
### AddOrReplaceReadGroups ###
java -Djava.io.tmpdir=$TMPDIR    -Xmx40g -jar $PICARD AddOrReplaceReadGroups \
INPUT=${sample}.bam OUTPUT=${sample}_tmp.bam   RGID=jmzeng  RGLB=lib_all  RGPL=illumina RGPU=x10  RGSM=jmzeng
mv ${sample}_tmp.bam ${sample}.bam
samtools index ${sample}.bam 
```
这里是直接跟着GATK官方的最佳实践写的代码，完成对bam文件的预处理。包括``RealignerTargetCreator --> IndelRealigner --> BaseRecalibrator --> PrintReads ``
```shell
module load java/1.8.0_91
GENOME=/home/jianmingzeng/reference/genome/human_g1k_v37/human_g1k_v37.fasta
INDEX=/home/jianmingzeng/reference/index/bwa/human_g1k_v37
GATK=/home/jianmingzeng/biosoft/GATK/GenomeAnalysisTK.jar
PICARD=/home/jianmingzeng/biosoft/picardtools/2.9.2/picard.jar
DBSNP=/home/jianmingzeng/annotation/variation/human/dbSNP/All_20160601.vcf.gz

KG_SNP=/home/jianmingzeng/biosoft/GATK/resources/bundle/b37/1000G_phase1.snps.high_confidence.b37.vcf.gz
Mills_indels=/home/jianmingzeng/biosoft/GATK/resources/bundle/b37/Mills_and_1000G_gold_standard.indels.b37.vcf
KG_indels=/home/jianmingzeng/biosoft/GATK/resources/bundle/b37/1000G_phase1.indels.b37.vcf

TMPDIR=/home/jianmingzeng/tmp/software
## samtools and bwa are in the environment 
## samtools Version: 1.3.1 (using htslib 1.3.1)
## bwa Version: 0.7.15-r1140

sample='merge'

###RealignerTargetCreator###
echo RealignerTargetCreator `date`
java -Djava.io.tmpdir=$TMPDIR   -Xmx40g -jar $GATK -T RealignerTargetCreator \
-I ${sample}.bam -R $GENOME -o ${sample}_target.intervals \
-known $Mills_indels -known $KG_indels -nt 5
echo RealignerTargetCreator `date`


###IndelRealigner###
echo IndelRealigner `date`
java -Djava.io.tmpdir=$TMPDIR   -Xmx40g -jar $GATK -T IndelRealigner \
-I ${sample}_marked_fixed_split.bam -R $GENOME -targetIntervals ${sample}_target.intervals \
-o ${sample}_realigned.bam -known $Mills_indels -known $KG_indels 
echo IndelRealigner `date`


###BaseRecalibrator###
echo BaseRecalibrator `date`
java -Djava.io.tmpdir=$TMPDIR   -Xmx40g -jar $GATK -T BaseRecalibrator \
-I ${sample}_realigned.bam -R $GENOME -o ${sample}_temp.table -knownSites $DBSNP
echo BaseRecalibrator `date`


###PrintReads###
echo PrintReads `date`
java -Djava.io.tmpdir=$TMPDIR   -Xmx40g -jar $GATK -T PrintReads \
-R $GENOME -I ${sample}_realigned.bam -o ${sample}_recal.bam -BQSR ${sample}_temp.table
samtools index ${sample}_recal.bam
echo PrintReads `date`

###delete_intermediate_files###
全基因组重测序分析
``` 
时间消耗如下：
```
INFO  15:50:24,097 ProgressMeter - Total runtime 1165.34 secs, 19.42 min, 0.32 hours 
INFO  17:21:00,917 ProgressMeter - Total runtime 4265.44 secs, 71.09 min, 1.18 hours
INFO  19:58:23,969 ProgressMeter - Total runtime 9436.69 secs, 157.28 min, 2.62 hours 
INFO  23:41:00,540 ProgressMeter - Total runtime 13349.77 secs, 222.50 min, 3.71 hours 
```
可以看到最耗费时间的步骤是最后一个``PrintReads``

###### variant calling by gatk hc  {-}
这个才是GATK工具的本职工作，就是找出测序数据跟参考基因组的不同之处，代码如下：

```shell
module load java/1.8.0_91
GENOME=/home/jianmingzeng/reference/genome/human_g1k_v37/human_g1k_v37.fasta
INDEX=/home/jianmingzeng/reference/index/bwa/human_g1k_v37
GATK=/home/jianmingzeng/biosoft/GATK/GenomeAnalysisTK.jar
PICARD=/home/jianmingzeng/biosoft/picardtools/2.9.2/picard.jar
DBSNP=/home/jianmingzeng/annotation/variation/human/dbSNP/All_20160601.vcf.gz

KG_SNP=/home/jianmingzeng/biosoft/GATK/resources/bundle/b37/1000G_phase1.snps.high_confidence.b37.vcf.gz
Mills_indels=/home/jianmingzeng/biosoft/GATK/resources/bundle/b37/Mills_and_1000G_gold_standard.indels.b37.vcf
KG_indels=/home/jianmingzeng/biosoft/GATK/resources/bundle/b37/1000G_phase1.indels.b37.vcf

TMPDIR=/home/jianmingzeng/tmp/software
## samtools and bwa are in the environment 
## samtools Version: 1.3.1 (using htslib 1.3.1)
## bwa Version: 0.7.15-r1140

fq1=P_jmzeng_DHG09057_AH33KVALXX_L1_1.clean.fq.gz
fq2=P_jmzeng_DHG09057_AH33KVALXX_L1_2.clean.fq.gz
sample='merge'

java -Djava.io.tmpdir=$TMPDIR   -Xmx40g -jar $GATK -T HaplotypeCaller  \
-R $GENOME -I ${sample}_recal.bam --dbsnp $DBSNP  \
-stand_emit_conf 10 -o  ${sample}_recal_raw.snps.indels.vcf

java -Djava.io.tmpdir=$TMPDIR   -Xmx40g -jar $GATK -T HaplotypeCaller  \
-R $GENOME -I ${sample}_realigned.bam --dbsnp $DBSNP  \
-stand_emit_conf 10 -o  ${sample}_realigned_raw.snps.indels.vcf
```

时间消耗如下：
```
INFO  20:40:49,063 ProgressMeter - Total runtime 39243.88 secs, 654.06 min, 10.90 hours 
INFO  08:53:17,633 ProgressMeter - Total runtime 43939.69 secs, 732.33 min, 12.21 hours 
```
可以看到对``recal.bam``的处理比 ``recal.bam``时间上要少2个小时，但是时间均消耗很长。

##### 流程探究 {-}

> 如果只给代码，那么这个教程意义不大，如果给出了input和output，还给出了时间消耗情况，那么这个教程可以说是中上水平了，读者只需要拿到数据就可以自己重复出来，既能估算硬件配置又能对大致的时间消耗有所了解。

> 但，这仍然不够，对我来说，我还可以介绍为什么要走每一个流程，以及每一个流程到底做了什么。可以这么说，你看完下面的流程探究，基本上就相当于你自己做过了一个全基因组重测序分析实战

> 我这里就对``L1``样本进行解析

###### 首先的BWA  {-}

这个没什么好说的，基因组数据的比对首选，耗时取决于fastq文件的reads数目。
```
CMD: bwa mem -t 5 -R @RG\tID:L1\tSM:L1\tLB:WGS\tPL:Illumina /home/jianmingzeng/reference/index/bwa/human_g1k_v37 P_jmzeng_DHG09057_AH33KVALXX_L1_1.clean.fq.gz P_jmzeng_DHG09057_AH33KVALXX_L1_2.clean.fq.gz
[main] Real time: 15870.794 sec; CPU: 77463.156 sec
```
###### 接下来是PICARD  {-}

共3个步骤用到了这个软件，消耗时间及内存分别如下：

```
picard.sam.SortSam done. Elapsed time: 44.15 minutes. Runtime.totalMemory()=13184794624
picard.sam.markduplicates.MarkDuplicates done. Elapsed time: 53.71 minutes. Runtime.totalMemory()=39832256512
picard.sam.FixMateInformation done. Elapsed time: 53.79 minutes. Runtime.totalMemory()=9425649664 

```
比对得到的都是sam格式数据，文件占硬盘空间太大，一般需要压缩成二进制的bam格式文件，用的是``SortSam``
至于``FixMateInformation``步骤仅仅是对bam文件增加了MC和MQ这两个tags
```
add MC (CIGAR string for mate) and MQ (mapping quality of the mate/next segment) tags 
```
而 ``markduplicates`` 步骤就比较复杂了，因为没有选择 REMOVE_DUPLICATES=True  所以并不会去除reads，只是标记一下而已，就是把sam文件的第二列改一下。
```
Read 119776742 records. 
INFO    2017-06-05 10:57:22     MarkDuplicates  Marking 14482525 records as duplicates.
INFO    2017-06-05 10:57:22     MarkDuplicates  Found 943146 optical duplicate clusters.
```
下面列出了部分被改变的flag值，可以去下面的[PICARD网页](https://broadinstitute.github.io/picard/explain-flags.html)去查看每个flag的含义。
```
# https://broadinstitute.github.io/picard/explain-flags.html
# diff  -y -W 50   |grep '|' 
163		      |	1187
83		      |	1107
99		      |	1123
163		      |	1187
147		      |	1171
83		      |	1107
99		      |	1123
99		      |	1123
147		      |	1171
147		      |	1171
99		      |	1123
147		      |	1171
163		      |	1187
83		      |	1107
```

###### 最后是GATK {-}

``SplitNCigarReads`` 这个步骤对基因组数据来说可以略去，主要是针对于转录组数据的

> 命令是：

```
Program Args: -T SplitNCigarReads -R /home/jianmingzeng/reference/genome/human_g1k_v37/human_g1k_v37.fasta \
-I L1_marked_fixed.bam -o L1_marked_fixed_split.bam \
-rf ReassignOneMappingQuality -RMQF 255 -RMQT 60 -U ALLOW_N_CIGAR_READS 
```

> 程序运行的log日志是：

```
INFO  13:04:52,813 ProgressMeter - Total runtime 2398.74 secs, 39.98 min, 0.67 hours 
INFO  13:04:52,854 MicroScheduler - 0 reads were filtered out during the traversal out of approximately 120614036 total reads (0.00%) 
INFO  13:04:52,854 MicroScheduler -   -> 0 reads (0.00% of total) failing BadCigarFilter 
INFO  13:04:52,854 MicroScheduler -   -> 0 reads (0.00% of total) failing MalformedReadFilter 
INFO  13:04:52,855 MicroScheduler -   -> 0 reads (0.00% of total) failing ReassignOneMappingQualityFilter 
```

可以看到，对全基因组测序数据来说，这个步骤毫无效果，而且还耗时40分钟，应该略去。
 
然后是indel区域的重排，需要结合 ``RealignerTargetCreator`` 和 ``IndelRealigner`` 

> 命令是：

```
Program Args: -T RealignerTargetCreator -I L1_marked_fixed_split.bam \
-R /home/jianmingzeng/reference/genome/human_g1k_v37/human_g1k_v37.fasta -o L1_target.intervals \
-known /home/jianmingzeng/biosoft/GATK/resources/bundle/b37/Mills_and_1000G_gold_standard.indels.b37.vcf \
-known /home/jianmingzeng/biosoft/GATK/resources/bundle/b37/1000G_phase1.indels.b37.vcf -nt 5 
```

> 程序运行的log日志是：

```
INFO  15:50:24,097 ProgressMeter - Total runtime 1165.34 secs, 19.42 min, 0.32 hours 
INFO  15:50:24,097 MicroScheduler - 22094746 reads were filtered out during the traversal out of approximately 120826819 total reads (18.29%) 
INFO  15:50:24,104 MicroScheduler -   -> 0 reads (0.00% of total) failing BadCigarFilter 
INFO  15:50:24,104 MicroScheduler -   -> 1774279 reads (1.47% of total) failing BadMateFilter 
INFO  15:50:24,104 MicroScheduler -   -> 14006627 reads (11.59% of total) failing DuplicateReadFilter 
INFO  15:50:24,104 MicroScheduler -   -> 0 reads (0.00% of total) failing FailsVendorQualityCheckFilter 
INFO  15:50:24,104 MicroScheduler -   -> 0 reads (0.00% of total) failing MalformedReadFilter 
INFO  15:50:24,104 MicroScheduler -   -> 0 reads (0.00% of total) failing MappingQualityUnavailableFilter 
INFO  15:50:24,105 MicroScheduler -   -> 6313840 reads (5.23% of total) failing MappingQualityZeroFilter 
INFO  15:50:24,105 MicroScheduler -   -> 0 reads (0.00% of total) failing NotPrimaryAlignmentFilter 
INFO  15:50:24,105 MicroScheduler -   -> 0 reads (0.00% of total) failing Platform454Filter 
INFO  15:50:24,105 MicroScheduler -   -> 0 reads (0.00% of total) failing UnmappedReadFilter 
```

> 命令是：

```
Program Args: -T IndelRealigner -I L1_marked_fixed_split.bam \
-R /home/jianmingzeng/reference/genome/human_g1k_v37/human_g1k_v37.fasta \
-targetIntervals L1_target.intervals -o L1_realigned.bam \
-known /home/jianmingzeng/biosoft/GATK/resources/bundle/b37/Mills_and_1000G_gold_standard.indels.b37.vcf \
-known /home/jianmingzeng/biosoft/GATK/resources/bundle/b37/1000G_phase1.indels.b37.vcf
```

> 程序运行的log日志是：

```
INFO  17:21:00,917 ProgressMeter - Total runtime 4265.44 secs, 71.09 min, 1.18 hours 
INFO  17:21:00,920 MicroScheduler - 0 reads were filtered out during the traversal out of approximately 120614036 total reads (0.00%) 
INFO  17:21:00,920 MicroScheduler -   -> 0 reads (0.00% of total) failing BadCigarFilter 
INFO  17:21:00,920 MicroScheduler -   -> 0 reads (0.00% of total) failing MalformedReadFilter 
```

最后是碱基质量的矫正，需要结合 ``BaseRecalibrator`` 和 ``PrintReads`` 
> 命令是：

```
Program Args: -T BaseRecalibrator -I L1_realigned.bam \
-R /home/jianmingzeng/reference/genome/human_g1k_v37/human_g1k_v37.fasta -o L1_temp.table \
-knownSites /home/jianmingzeng/annotation/variation/human/dbSNP/All_20160601.vcf.gz
```

> 程序运行的log日志是：

```
INFO  19:58:23,969 ProgressMeter - Total runtime 9436.69 secs, 157.28 min, 2.62 hours 
INFO  19:58:23,970 MicroScheduler - 21179430 reads were filtered out during the traversal out of approximately 120614036 total reads (17.56%) 
INFO  19:58:23,970 MicroScheduler -   -> 0 reads (0.00% of total) failing BadCigarFilter 
INFO  19:58:23,970 MicroScheduler -   -> 14073643 reads (11.67% of total) failing DuplicateReadFilter 
INFO  19:58:23,970 MicroScheduler -   -> 0 reads (0.00% of total) failing FailsVendorQualityCheckFilter 
INFO  19:58:23,970 MicroScheduler -   -> 0 reads (0.00% of total) failing MalformedReadFilter 
INFO  19:58:23,971 MicroScheduler -   -> 0 reads (0.00% of total) failing MappingQualityUnavailableFilter 
INFO  19:58:23,971 MicroScheduler -   -> 7105787 reads (5.89% of total) failing MappingQualityZeroFilter 
INFO  19:58:23,971 MicroScheduler -   -> 0 reads (0.00% of total) failing NotPrimaryAlignmentFilter 
INFO  19:58:23,971 MicroScheduler -   -> 0 reads (0.00% of total) failing UnmappedReadFilter 
```

> 命令是：

```
Program Args: -T PrintReads -R /home/jianmingzeng/reference/genome/human_g1k_v37/human_g1k_v37.fasta -I L1_realigned.bam -o L1_recal.bam -BQSR L1_temp.table 
```

> 程序运行的log日志是：

```
INFO  23:41:00,540 ProgressMeter - Total runtime 13349.77 secs, 222.50 min, 3.71 hours 
INFO  23:41:00,542 MicroScheduler - 0 reads were filtered out during the traversal out of approximately 120614036 total reads (0.00%) 
INFO  23:41:00,542 MicroScheduler -   -> 0 reads (0.00% of total) failing BadCigarFilter 
INFO  23:41:00,542 MicroScheduler -   -> 0 reads (0.00% of total) failing MalformedReadFilter 
```

可以看到这个步骤非常的耗时，而且bam文件的大小近乎翻倍了。

最后是GATK真正的功能，variant-calling 

我这里不仅仅是对最后recal的bam进行variant-calling 步骤，同时也对realign的bam做了，所以下面显示两个时间消耗的记录.
因为GATK的 ``BaseRecalibrator`` 步骤太耗费时间，而且极大的增加了bam文件的存储，所以有必要确认这个步骤是否有必要。

> 命令是：

```
Program Args: -T HaplotypeCaller -R /home/jianmingzeng/reference/genome/human_g1k_v37/human_g1k_v37.fasta -I L1_recal.bam --dbsnp /home/jianmingzeng/annotation/variation/human/dbSNP/All_20160601.vcf.gz -stand_emit_conf 10 -o L1_recal_raw.snps.indels.vcf 
```

> 程序运行的log日志是：

```
INFO  20:40:49,062 ProgressMeter -            done    3.101804739E9    10.9 h           12.0 s      100.0%    10.9 h       0.0 s 
INFO  20:40:49,063 ProgressMeter - Total runtime 39243.88 secs, 654.06 min, 10.90 hours 
INFO  20:40:49,064 MicroScheduler - 22384946 reads were filtered out during the traversal out of approximately 119776742 total reads (18.69%) 
INFO  20:40:49,064 MicroScheduler -   -> 0 reads (0.00% of total) failing BadCigarFilter 
INFO  20:40:49,064 MicroScheduler -   -> 13732328 reads (11.46% of total) failing DuplicateReadFilter 
INFO  20:40:49,065 MicroScheduler -   -> 0 reads (0.00% of total) failing FailsVendorQualityCheckFilter 
INFO  20:40:49,065 MicroScheduler -   -> 8652618 reads (7.22% of total) failing HCMappingQualityFilter 
INFO  20:40:49,066 MicroScheduler -   -> 0 reads (0.00% of total) failing MalformedReadFilter 
INFO  20:40:49,066 MicroScheduler -   -> 0 reads (0.00% of total) failing MappingQualityUnavailableFilter 
INFO  20:40:49,066 MicroScheduler -   -> 0 reads (0.00% of total) failing NotPrimaryAlignmentFilter 
INFO  20:40:49,067 MicroScheduler -   -> 0 reads (0.00% of total) failing UnmappedReadFilter 
```

> 命令是：

```
Program Args: -T HaplotypeCaller -R /home/jianmingzeng/reference/genome/human_g1k_v37/human_g1k_v37.fasta -I L1_realigned.bam --dbsnp /home/jianmingzeng/annotation/variation/human/dbSNP/All_20160601.vcf.gz -stand_emit_conf 10 -o L1_realigned_raw.snps.indels.vcf 
```

> 程序运行的log日志是：

```
INFO  08:53:17,633 ProgressMeter -            done    3.101804739E9    12.2 h           14.0 s      100.0%    12.2 h       0.0 s 
INFO  08:53:17,633 ProgressMeter - Total runtime 43939.69 secs, 732.33 min, 12.21 hours 
INFO  08:53:17,634 MicroScheduler - 22384946 reads were filtered out during the traversal out of approximately 119776742 total reads (18.69%) 
INFO  08:53:17,634 MicroScheduler -   -> 0 reads (0.00% of total) failing BadCigarFilter 
INFO  08:53:17,635 MicroScheduler -   -> 13732328 reads (11.46% of total) failing DuplicateReadFilter 
INFO  08:53:17,635 MicroScheduler -   -> 0 reads (0.00% of total) failing FailsVendorQualityCheckFilter 
INFO  08:53:17,635 MicroScheduler -   -> 8652618 reads (7.22% of total) failing HCMappingQualityFilter 
INFO  08:53:17,636 MicroScheduler -   -> 0 reads (0.00% of total) failing MalformedReadFilter 
INFO  08:53:17,636 MicroScheduler -   -> 0 reads (0.00% of total) failing MappingQualityUnavailableFilter 
INFO  08:53:17,636 MicroScheduler -   -> 0 reads (0.00% of total) failing NotPrimaryAlignmentFilter 
INFO  08:53:17,637 MicroScheduler -   -> 0 reads (0.00% of total) failing UnmappedReadFilter 
```

这个不同的bam文件首先大小就不一致，其次找变异所消耗的时间也不一样，找出的变异数目也不同的，至于具体差别在哪里，就不是本文探索范围了，我在我在直播基因组里面有提到过这一点。
 
我对realign和recal以及原始的bam都用了HaplotypeCaller找变异，得到的vcf文件如下：
```
1.1G Jun 21 02:29 jmzeng_merge_raw.snps.indels.vcf
1.1G Jun 21 02:15 jmzeng_realigned_raw.snps.indels.vcf
1.1G Jun 20 21:20 jmzeng_recal_raw.snps.indels.vcf
```
这是最原始的变异文件，我们需要进行过滤，拆分SNP和INDEL，这样才能更好的对它们两两比较。

###### 拆分SNP和INDEL并过滤低质量位点  {-}

> 代码如下：
```shell
module load java/1.8.0_91
GENOME=/home/jianmingzeng/reference/genome/human_g1k_v37/human_g1k_v37.fasta 
GATK=/home/jianmingzeng/biosoft/GATK/GenomeAnalysisTK.jar 
TMPDIR=/home/jianmingzeng/tmp/software 

sample=$1

## for SNP 
: '
'
java -Djava.io.tmpdir=$TMPDIR   -Xmx40g -jar $GATK -T SelectVariants  -R $GENOME  \
-selectType SNP \
-V ${sample}_raw.snps.indels.vcf -o ${sample}_raw_snps.vcf


java -Djava.io.tmpdir=$TMPDIR   -Xmx40g -jar $GATK -T VariantFiltration -R $GENOME  \
--filterExpression "QD < 2.0 || FS > 60.0 || MQ < 40.0 || MQRankSum < -12.5 || ReadPosRankSum < -8.0"  \
--filterName "my_snp_filter" \
-V ${sample}_raw_snps.vcf  -o ${sample}_filtered_snps.vcf   

java -Djava.io.tmpdir=$TMPDIR   -Xmx40g -jar $GATK -T SelectVariants -R $GENOME  \
--excludeFiltered \
-V ${sample}_filtered_snps.vcf  -o  ${sample}_filtered_PASS.snps.vcf


java -Djava.io.tmpdir=$TMPDIR   -Xmx40g -jar $GATK -T VariantEval -R $GENOME  \
-eval ${sample}_filtered_PASS.snps.vcf -o  ${sample}_filtered_PASS.snps.vcf.eval

## for  INDEL 
java -Djava.io.tmpdir=$TMPDIR   -Xmx40g -jar $GATK -T SelectVariants  -R $GENOME \
-selectType INDEL  \
-V ${sample}_raw.snps.indels.vcf   -o ${sample}_raw_indels.vcf


java -Djava.io.tmpdir=$TMPDIR   -Xmx40g -jar $GATK -T VariantFiltration -R $GENOME  \
--filterExpression "QD < 2.0 || FS > 200.0 || ReadPosRankSum < -20.0"  \
--filterName "my_indel_filter" \
-V ${sample}_raw_indels.vcf  -o ${sample}_filtered_indels.vcf   

java -Djava.io.tmpdir=$TMPDIR   -Xmx40g -jar $GATK -T SelectVariants -R $GENOME  \
--excludeFiltered \
-V ${sample}_filtered_indels.vcf  -o  ${sample}_filtered_PASS.indels.vcf

java -Djava.io.tmpdir=$TMPDIR   -Xmx40g -jar $GATK -T VariantEval -R $GENOME  \
-eval ${sample}_filtered_PASS.indels.vcf -o  ${sample}_filtered_PASS.indels.vcf.eval
```
要深度理解这个代码请点击自行[阅读文档](http://gatkforums.broadinstitute.org/gatk/discussion/2806/howto-apply-hard-filters-to-a-call-set)。
其实我本人不大喜欢用这个工具的各种参数，我比较喜欢自行写脚本来过滤vcf文件。

这样得到的文件如下：

```
     801242 jmzeng_merge_filtered_indels.vcf
     760890 jmzeng_merge_filtered_PASS.indels.vcf
    3812094 jmzeng_merge_filtered_PASS.snps.vcf
    4034168 jmzeng_merge_filtered_snps.vcf
     801240 jmzeng_merge_raw_indels.vcf
    4837892 jmzeng_merge_raw.snps.indels.vcf
    4034166 jmzeng_merge_raw_snps.vcf
     801963 jmzeng_realigned_filtered_indels.vcf
     761510 jmzeng_realigned_filtered_PASS.indels.vcf
    3812442 jmzeng_realigned_filtered_PASS.snps.vcf
    4034797 jmzeng_realigned_filtered_snps.vcf
     801961 jmzeng_realigned_raw_indels.vcf
    4839256 jmzeng_realigned_raw.snps.indels.vcf
    4034795 jmzeng_realigned_raw_snps.vcf
     793611 jmzeng_recal_filtered_indels.vcf
     754755 jmzeng_recal_filtered_PASS.indels.vcf
    3784343 jmzeng_recal_filtered_PASS.snps.vcf
    4010670 jmzeng_recal_filtered_snps.vcf
     793609 jmzeng_recal_raw_indels.vcf
    4806696 jmzeng_recal_raw.snps.indels.vcf
    4010668 jmzeng_recal_raw_snps.vcf
```

很容易理解：
* 对recal的bam得到的变异vcf文件来说，总共是480万位点，拆分成401万的SNP和79万的INDEL，然后经过过滤后剩下378万的SNP和75万的INDEL。
* 对原始的bam得到的变异vcf文件来说，  总共是483万位点，拆分成403万的SNP和80万的INDEL，然后经过过滤后剩下381万的SNP和76万的INDEL。
* 对重排的bam得到的变异vcf文件来说，  总共是483万位点，拆分成403万的SNP和80万的INDEL，然后经过过滤后剩下381万的SNP和76万的INDEL。

从位点数量级来说，原始的bam和重排的bam得到的变异vcf文件没区别，所以需要用专业的工具来具体比较它们里面的每一个位点。
我这里还是选择SnpEff套件里面的SnpSift工具。

###### 首先比较SNP位点 {-}

> 代码如下：
```shell
java -jar  ~/biosoft/SnpEff/snpEff/SnpSift.jar  concordance -v ../jmzeng_merge_filtered_PASS.snps.vcf ../jmzeng_realigned_filtered_PASS.snps.vcf 1>concordance.txt 2>SnpSift_Concordance.log
java -jar  ~/biosoft/SnpEff/snpEff/SnpSift.jar  concordance -v ../jmzeng_recal_filtered_PASS.snps.vcf  ../jmzeng_realigned_filtered_PASS.snps.vcf 1>concordance.txt 2>SnpSift_Concordance.log
java -jar  ~/biosoft/SnpEff/snpEff/SnpSift.jar  concordance -v ../jmzeng_recal_filtered_PASS.snps.vcf  ../jmzeng_merge_filtered_PASS.snps.vcf 1>concordance.txt 2>SnpSift_Concordance.log
```

比较的结果如下：
```
Number of samples:
	1	File ../jmzeng_merge_filtered_PASS.snps.vcf
	1	File ../jmzeng_realigned_filtered_PASS.snps.vcf
	1	Both files

# Errors:
	ALT field does not match	31
	
Number of samples:
	1	File ../jmzeng_recal_filtered_PASS.snps.vcf
	1	File ../jmzeng_realigned_filtered_PASS.snps.vcf
	1	Both files

# Errors:
	ALT field does not match	204
	
Number of samples:
	1	File ../jmzeng_recal_filtered_PASS.snps.vcf
	1	File ../jmzeng_merge_filtered_PASS.snps.vcf
	1	Both files

# Errors:
	ALT field does not match	208
```
可以看到对高质量的SNP位点来说，3种bam文件得到SNP信息差别很微弱，在可接受范围内。但是我们不能忽视原始的bam和重排的bam得到的变异vcf文件要比recal后的少了近两万这个事实！！！


###### 接着比较INDEL位点 {-}

> 代码如下：
```	shell
java -jar  ~/biosoft/SnpEff/snpEff/SnpSift.jar  concordance -v ../jmzeng_merge_filtered_PASS.indels.vcf ../jmzeng_realigned_filtered_PASS.indels.vcf 1>concordance.txt 2>SnpSift_Concordance.log
java -jar  ~/biosoft/SnpEff/snpEff/SnpSift.jar  concordance -v ../jmzeng_recal_filtered_PASS.indels.vcf  ../jmzeng_realigned_filtered_PASS.indels.vcf 1>concordance.txt 2>SnpSift_Concordance.log
java -jar  ~/biosoft/SnpEff/snpEff/SnpSift.jar  concordance -v ../jmzeng_recal_filtered_PASS.indels.vcf  ../jmzeng_merge_filtered_PASS.indels.vcf 1>concordance.txt 2>SnpSift_Concordance.log
```

比较的结果如下：
```
Number of samples:
	1	File ../jmzeng_merge_filtered_PASS.indels.vcf
	1	File ../jmzeng_realigned_filtered_PASS.indels.vcf
	1	Both files

# Errors:
	REF fields does not match	28
	ALT field does not match	45
	
	
Number of samples:
	1	File ../jmzeng_recal_filtered_PASS.indels.vcf
	1	File ../jmzeng_merge_filtered_PASS.indels.vcf
	1	Both files

# Errors:
	REF fields does not match	1295
	ALT field does not match	964
	
	
Number of samples:
	1	File ../jmzeng_recal_filtered_PASS.indels.vcf
	1	File ../jmzeng_realigned_filtered_PASS.indels.vcf
	1	Both files

# Errors:
	REF fields does not match	1276
	ALT field does not match	947
```
INDEL本身对参数非常敏感，这时候不仅仅是数量的差异，而且本身找到的位点突变情况的差异也不少。

所以我的结论是，GATK的BEST PRACTISE的每个步骤都是必须的！虽然他们非常耗时，但是对结果准确性的改进的确非常显著。
如果要想把测序数据在临床上面应用，那么每一个步骤都是非常有意义的。	


比如，如果我们来分析realign之后的高质量SNP为什么会有31个的ALT改变了呢？
```		
21	10716592
21	10701512
21	10700605
20	26317761
19	15787221
18	18515822
17	25266293
16	35230302
16	33975649
16	33972478
10	42400348
10	42393437
9	66455306
4	49111623
1	142537167
Y	58977742
Y	13478115
X	61909282
X	61730552
X	61721098
```
简单看了几眼， 发现都是在染色体的中心粒的地方

再次介绍一下我的这次基因组重测续数据共5条lane，都是单独的PE150的fastq文件。

###### 如果仅对L1这条lane数据进行处理  {-}

> 首先是BWA的比对耗时如下： 

```
[main] Real time: 15870.794 sec; CPU: 77463.156 sec
picard.sam.SortSam done. Elapsed time: 45.60 minutes.
picard.sam.markduplicates.MarkDuplicates done. Elapsed time: 64.20 minutes.
picard.sam.FixMateInformation done. Elapsed time: 58.05 minutes.
```
> 然后是GATK对bam文件的一些预处理，步骤是：
```
RealignerTargetCreator --> IndelRealigner --> BaseRecalibrator --> PrintReads 
```
后面我会讲到这些步骤是否是必须的。

> 耗时如下：
```
INFO  15:50:24,097 ProgressMeter - Total runtime 1165.34 secs, 19.42 min, 0.32 hours 
INFO  17:21:00,917 ProgressMeter - Total runtime 4265.44 secs, 71.09 min, 1.18 hours 
INFO  19:58:23,969 ProgressMeter - Total runtime 9436.69 secs, 157.28 min, 2.62 hours 
INFO  23:41:00,540 ProgressMeter - Total runtime 13349.77 secs, 222.50 min, 3.71 hours 
```
可以看到对L1这条lane数据来说，整个流程耗时才不到10个小时，还算是可接受范围内的。


###### 接下来用HaplotypeCaller找变异  {-}

> 这个步骤我对realign的bam和recal的bam分别处理了，耗时如下：
```
INFO  20:40:49,063 ProgressMeter - Total runtime 39243.88 secs, 654.06 min, 10.90 hours 
INFO  08:53:17,633 ProgressMeter - Total runtime 43939.69 secs, 732.33 min, 12.21 hours 
```
对bam文件找变异的时间取决于reads数量的多少以及这些reads覆盖的基因组区域大小，虽然一条lane的数据量很小，但它仍然是全基因组测序，如果是全外显子测序这个耗时是不一样的。

> 整个L1这条lane数据(120M的reads)处理后的文件大小如下所示：
```
3.0M Jun  7 01:14 L1.bam.bai 
8.8G Jun  7 02:33 L1_marked.bam
9.0G Jun  7 03:31 L1_marked_fixed.bam
3.3M Jun  7 03:36 L1_marked_fixed.bam.bai
2.6K Jun  7 02:33 L1.metrics
8.2M Jun  5 17:21 L1_realigned.bai
9.0G Jun  5 17:21 L1_realigned.bam
8.2M Jun  5 23:41 L1_recal.bai
 27G Jun  5 23:41 L1_recal.bam
8.1M Jun  5 23:53 L1_recal.bam.bai
 39M Jun  5 15:50 L1_target.intervals
320K Jun  5 19:58 L1_temp.table
```
因为我的这次基因组重测续数据共5条lane，这5条lane的数据其实是可以并行完成这几个步骤的，最长耗时约12小时。 
每个数据处理我都分配了 5个线程， 40G的内存。

如果merge后不进行AddOrReplaceReadGroups处理,意味着要把5条lane的数据当做是不同的样本，这样后续处理这5个lane的bam矫正以及找变异都是独立进行的，虽然最后生成的vcf文件只有一个，但是每条lane都有独立的基因型。
realign和recal耗时如下：
```
INFO  17:54:59,396 ProgressMeter - Total runtime 5194.10 secs, 86.57 min, 1.44 hours 
INFO  02:04:10,907 ProgressMeter - Total runtime 22558.84 secs, 375.98 min, 6.27 hours 
INFO  18:48:45,762 ProgressMeter - Total runtime 60267.18 secs, 1004.45 min, 16.74 hours 
INFO  21:30:54,519 ProgressMeter - Total runtime 96119.87 secs, 1602.00 min, 26.70 hours 
```
> 同样还是对realign的bam和recal的bam分别用HaplotypeCaller找变异
```
INFO  19:36:31,960 ProgressMeter - Total runtime 201031.47 secs, 3350.52 min, 55.84 hours 
INFO  22:59:21,693 ProgressMeter - Total runtime 184944.78 secs, 3082.41 min, 51.37 hours 
```
可以看到这个时候的耗时相比仅针对一条lane已经是非常恐怖了，不仅仅是因为这个时候reads数量增多，而且是因为5条lane独立样本处理。


如果merge后进行AddOrReplaceReadGroups处理，也就是正确的做法下，realign和recal耗时如下：
```
INFO  15:52:21,739 ProgressMeter - Total runtime 3573.62 secs, 59.56 min, 0.99 hours 
INFO  22:46:39,615 ProgressMeter - Total runtime 24853.46 secs, 414.22 min, 6.90 hours 
INFO  16:06:14,340 ProgressMeter - Total runtime 62366.41 secs, 1039.44 min, 17.32 hours 
INFO  18:18:08,004 ProgressMeter - Total runtime 94304.46 secs, 1571.74 min, 26.20 hours 
```
这个耗时跟上面把lane当做是独立样本的没有什么区别，因为耗时取决于reads数据量。

接下来找变异，我不仅仅是对realign和recal，还加入了最原始的bam，全部耗时如下：
```
INFO  02:29:32,680 ProgressMeter - Total runtime 149229.64 secs, 2487.16 min, 41.45 hours 
INFO  02:15:39,234 ProgressMeter - Total runtime 148379.91 secs, 2473.00 min, 41.22 hours 
INFO  21:20:51,032 ProgressMeter - Total runtime 130663.06 secs, 2177.72 min, 36.30 hours
```
可以看到这些耗时都显著的小于把lane当做独立样本。

全部流程输出的文件大小如下:

```
 59G Jun 14 14:17 jmzeng.bam
8.4M Jun 14 14:52 jmzeng.bam.bai
1.1G Jun 21 02:29 jmzeng_merge_raw.snps.indels.vcf
 12M Jun 21 02:29 jmzeng_merge_raw.snps.indels.vcf.idx
8.4M Jun 14 22:46 jmzeng_realigned.bai
 59G Jun 14 22:46 jmzeng_realigned.bam
1.1G Jun 21 02:15 jmzeng_realigned_raw.snps.indels.vcf
 12M Jun 21 02:15 jmzeng_realigned_raw.snps.indels.vcf.idx
8.5M Jun 16 18:18 jmzeng_recal.bai
161G Jun 16 18:18 jmzeng_recal.bam
8.5M Jun 16 19:39 jmzeng_recal.bam.bai
1.1G Jun 20 21:20 jmzeng_recal_raw.snps.indels.vcf
 12M Jun 20 21:20 jmzeng_recal_raw.snps.indels.vcf.idx
 47M Jun 14 15:52 jmzeng_target.intervals
323K Jun 15 16:06 jmzeng_temp.table
```
值得注意的是，recal之后的bam文件是原始bam的3倍大小，特别耗费存储空间。

接下来我会讲解realign和recal步骤的必要性，毕竟这两个步骤也的确太耗时了，尤其是recal，不仅仅耗时还特别占硬盘存储。


#### tro家系外显子数据分析

外显子组测序（whole exome sequencing, WES）是指利用序列捕获技术将全基因组外显子区域DNA捕捉并富集后进行高通量测序的基因组分析方法。在人类基因中大约有18万个外显子,占人类基因组的1%，约30 Mb。相比于全基因组测序，外显子组测序更加经济、高效，目前已广泛应用于遗传病和癌症研究中。

在各种环境因素的作用下，机体某些体细胞染色体上发生的变异破坏或改变了某些重要的生物学过程，体细胞可能会因此异常增生而转变为肿瘤细胞。对于癌症研究，一般选取患病个体配对的癌组织和癌旁组织或患病个体组织样本及配对的外周血样本进行外显子组学研究。由于不同患病个体之间没有血缘关系，遗传背景相差较大。而在遗传病领域一般是对患病个体的父母一同测序，叫做 基于trio的全外显子组测序。

> 基于trio的全外显子组测序已经被证实是一种对于智力障碍(intellectual disability, ID)和先天性畸形(congenital malformation, CM)患者的致病基因检测非常有效的策略。


目录如下：
- 测序质量控制
- snp-calling
- snp-filter
- 不同个体的比较
- 不同软件比较
- annovar注释
- de novo变异情况


##### 第一步：测序质量控制 {-}

这一步主要看看这些外显子测序数据的测序质量如何：

![](image/C6/WES/image0011.png) 

首先用fastqc处理，会出一些图表，通常是不会有问题的，毕竟公司不想砸自己的牌子。

然后粗略统计下**平均测序深度**及**目标区域覆盖度**，这个是重点，不过一般没问题的，因为现在芯片捕获技术非常成熟了，而且实验水平大幅提升，没有以前那么多的问题了。

这个外显子项目的测序文件中mpileup文件是1371416525行，意味着总的测序长度是1.3G，以前我接触的一般是600M左右的。因为外显子目标区域并不大，就34729283bp，也就是约35M，即使加上侧翼长度。
```
54692160：外显子加上前后50bp
73066288：外显子加上前后100bp
90362533：外显子加上前后150bp
```
然后我要根据外显子记录文件对mpileup文件进行计数，统计外显子的coverage还有测序深度，这个脚本其实蛮有难度的。

我前面提到过外显子组的序列仅占全基因组序列的1%左右，而我在NCBI里面拿到 consensus coding sequence (CCDS)记录`CCDS.20150512.txt`文件，是基于hg38版本的，需要首先转换成hg19才可以来计算这次测序项目的覆盖度和平均测序深度。

参考：http://www.bio-info-trainee.com/?p=990 （ liftover基因组版本之间的coordinate转换）

```
 awk '{print "chr"$3,$4,$5,$1,0,$2,$4,$5,"255,0,0"}' CCDS.20150512.exon.txt >CCDS.20150512.exon.hg38.bed
~/bio-soft/liftover/liftOver CCDS.20150512.exon.hg38.bed ~/bio-soft/liftover/hg38ToHg19.over.chain CCDS.20150512.exon.hg19.bed unmap
```

下面这个程序就是读取转换好的外显子记录的数据，对一家三口一起统计，然后再读取每个样本的20G左右的mpileup文件进行统计，所以很耗费时间。

![](image/C6/WES/image0021.png)

外显子目标区域平均测序深度接近100X，所以很明显是非常好的捕获效率啦！而全基因组背景深度才3.3，这符合实验原理，即与探针杂交碱基多的片段比少的片段更易被捕获。对非特异杂交的基因组覆盖度非特异的背景 DNA 也进行了测序。

![](image/C6/WES/image0031.png)

接下来对测序深度进行简单统计，脚本如下，但是这个图没多大意思因为我们的外显子的35M区域平均都接近100X的测序量。

![](image/C6/WES/image0041.png)

---

##### 第二步：找变异位点 {-}

准备文件：下载必备的软件和参考基因组数据

**软件**

![](image/C6/WES/image0012.png)

ps：还有samtools，freebayes和varscan软件，我以前下载过，这次就没有再弄了，但是下面会用到

**参考基因组**

![](image/C6/WES/image0022.png)

**参考突变数据**

![](image/C6/WES/image0032.png)

1. 下载数据

![](image/C6/WES/image0042.png)

2. bwa比对

![](image/C6/WES/image0051.png)

![](image/C6/WES/image006.png)

3. sam转为bam，并sort好

![](image/C6/WES/image007.png)

![](image/C6/WES/image008.png)

4. 标记PCR重复，并去除

![](image/C6/WES/image009.png)

![](image/C6/WES/image010.png)

5. 产生需要重排的坐标记录

![](image/C6/WES/image011.png)

![](image/C6/WES/image012.png)

6. 根据重排记录文件把比对结果重新比对

![](image/C6/WES/image013.png)

![](image/C6/WES/image014.png)

7. 把最终的bam文件转为mpileup文件

![](image/C6/WES/image015.png)

![](image/C6/WES/image016.png)

8. 用bcftools 来call snp

![](image/C6/WES/image017.png)

9 用freebayes来call snp

![](image/C6/WES/image018.png)

10. 用gatk来call snp

![](image/C6/WES/image019.png)

11. 用varscan来call snp

![](image/C6/WES/image020.png)

![](image/C6/WES/image021.png)


##### 第三步：过滤变异位点 {-}

其中freebayes,bcftools,gatk都是把所有的snp细节都call出来了，可以看到下面这些软件的结果有的高达一百多万个snp，而一般文献都说外显子组测序可鉴定约8万个变异。

![](image/C6/WES/image0013.png)

这样得到突变太多了，所以需要过滤。这里过滤的统一标准都是**qual大于20，测序深度大于10**。过滤之后的snp数量如下

![](image/C6/WES/image0023.png)

```
perl -alne '{next if $F[5]<20;/DP=(\d+)/;next if $1<10;next if /INDEL/;/(DP4=.*?);/;print "$F[0]\t$F[1]\t$F[3]\t$F[4]:$1"}' Sample3.bcftools.vcf >Sample3.bcftools.vcf.filter

perl -alne '{next if $F[5]<20;/DP=(\d+)/;next if $1<10;next if /INDEL/;/(DP4=.*?);/;print "$F[0]\t$F[1]\t$F[3]\t$F[4]:$1"}' Sample4.bcftools.vcf >Sample4.bcftools.vcf.filter

perl -alne '{next if $F[5]<20;/DP=(\d+)/;next if $1<10;next if /INDEL/;/(DP4=.*?);/;print "$F[0]\t$F[1]\t$F[3]\t$F[4]:$1"}' Sample5.bcftools.vcf >Sample5.bcftools.vcf.filter

perl -alne '{next if $F[5]<20;/DP=(\d+)/;next if $1<10;next unless /TYPE=snp/;@tmp=split/:/,$F[9];print "$F[0]\t$F[1]\t$F[3]\t$F[4]:$tmp[0]:$tmp[1]"}'  Sample3.freebayes.vcf > Sample3.freebayes.vcf.filter

perl -alne '{next if $F[5]<20;/DP=(\d+)/;next if $1<10;next unless /TYPE=snp/;@tmp=split/:/,$F[9];print "$F[0]\t$F[1]\t$F[3]\t$F[4]:$tmp[0]:$tmp[1]"}'  Sample4.freebayes.vcf > Sample4.freebayes.vcf.filter

perl -alne '{next if $F[5]<20;/DP=(\d+)/;next if $1<10;next unless /TYPE=snp/;@tmp=split/:/,$F[9];print "$F[0]\t$F[1]\t$F[3]\t$F[4]:$tmp[0]:$tmp[1]"}'  Sample5.freebayes.vcf > Sample5.freebayes.vcf.filter

perl -alne '{next if $F[5]<20;/DP=(\d+)/;next if $1<10;next if length($F[3]) >1;next if length($F[4]) >1;@tmp=split/:/,$F[9];print "$F[0]\t$F[1]\t$F[3]\t$F[4]:$tmp[0]:$tmp[1]:$tmp[2]"}'  Sample3.gatk.UG.vcf  >Sample3.gatk.UG.vcf.filter

perl -alne '{next if $F[5]<20;/DP=(\d+)/;next if $1<10;next if length($F[3]) >1;next if length($F[4]) >1;@tmp=split/:/,$F[9];print "$F[0]\t$F[1]\t$F[3]\t$F[4]:$tmp[0]:$tmp[1]:$tmp[2]"}'  Sample4.gatk.UG.vcf  >Sample4.gatk.UG.vcf.filter

perl -alne '{next if $F[5]<20;/DP=(\d+)/;next if $1<10;next if length($F[3]) >1;next if length($F[4]) >1;@tmp=split/:/,$F[9];print "$F[0]\t$F[1]\t$F[3]\t$F[4]:$tmp[0]:$tmp[1]:$tmp[2]"}'  Sample5.gatk.UG.vcf  >Sample5.gatk.UG.vcf.filter

perl -alne '{@tmp=split/:/,$F[9];next if $tmp[3]<10;print "$F[0]\t$F[1]\t$F[3]\t$F[4]:$tmp[0]:$tmp[3]"}' Sample3.varscan.snp.vcf >Sample3.varscan.snp.vcf.filter

perl -alne '{@tmp=split/:/,$F[9];next if $tmp[3]<10;print "$F[0]\t$F[1]\t$F[3]\t$F[4]:$tmp[0]:$tmp[3]"}' Sample4.varscan.snp.vcf >Sample4.varscan.snp.vcf.filter

perl -alne '{@tmp=split/:/,$F[9];next if $tmp[3]<10;print "$F[0]\t$F[1]\t$F[3]\t$F[4]:$tmp[0]:$tmp[3]"}' Sample5.varscan.snp.vcf >Sample5.varscan.snp.vcf.filter

```
这样不同工具产生的snp记录数就比较整齐了，我们先比较四种不同工具的call snp的情况，然后再比较三个人的区别。

然后写了一个程序把所有的snp合并起来比较

![](image/C6/WES/image0033.png)

得到了一个很有趣的表格，我放在excel里面看了看 ，主要是要看生物学意义，但是我的生物学知识好多都忘了，得重新学习了 

![](image/C6/WES/image0043.png)


##### 第四步：不同个体的比较 {-}

3-4-5分别就是孩子、父亲、母亲

我对每个个体取他们的**四种软件的公共snp**来进行分析，并且只分析基因型，看看是否符合孟德尔遗传定律。

![](image/C6/WES/image0014.png)

结果如下：

![](image/C6/WES/image0024.png)

粗略看起来好像很少不符合孟德尔遗传定律，然后我写了程序计算。

![](image/C6/WES/image0034.png)

总共127138个可以计算的位点，共有18063个位点不符合孟德尔遗传定律。

我检查了一下不符合的原因，发现我把

`chr1 100617887 C T:DP4=0,0,36,3 T:1/1:40 T:1/1:0,40:40 miss T:DP4=0,0,49,9 T:1/1:59 T:1/1:0,58:59 miss T:DP4=0,0,43,8 T:1/1:53 T:1/1:0,53:53 T:1/1:50`

计算成了`chr1 100617887 C 0/0 0/0 1/1` 所以认为不符合，因为我认为只有四个软件都认为是snp的我才当作是snp的基因型，否则都是0/0。那么我就改写了程序，全部用gatk结果来计算。这次可以计算的snp有个176036，不符合的有20309，而且我看了不符合的snp的染色体分布，Y染色体有点异常。

![](image/C6/WES/image0044.png)

![](image/C6/WES/image0052.png)

但是很失败，没什么发现。

##### 第五步：不同软件比较 {-}

主要是画韦恩图看看，参考：http://www.bio-info-trainee.com/?p=893

对合并而且过滤的高质量snp信息来看看四种不同的snp calling软件的差异

我们用R语言来画韦恩图

![](image/C6/WES/image0015.png)

可以看出不同软件的差异还是蛮大的，所以我只选四个软件的公共snp来进行分析

首先是sample3

![](image/C6/WES/image0025.png)

然后是sample4

![](image/C6/WES/image0035.png)

然后是sample5

![](image/C6/WES/image0045.png)

可以看出，不同的软件差异还是蛮大的，所以我重新比较了一下，这次只比较，它们不同的软件在exon位点上面的snp的差异，毕竟，我们这次是外显子测序，重点应该是外显子snp

![](image/C6/WES/image0053.png)

然后我们用同样的程序，画韦恩图，这次能明显看出来了，大部分的snp位点都至少有两到三个软件支持

**所以，只有测序深度达到一定级别，用什么软件来做snp-calling其实影响并不大。**

![](image/C6/WES/image0061.png)

##### 第六步：用annovar注释 {-}

使用annovar软件参考自：http://www.bio-info-trainee.com/?p=641

```
/home/jmzeng/bio-soft/annovar/convert2annovar.pl -format vcf4  Sample3.varscan.snp.vcf > Sample3.annovar

/home/jmzeng/bio-soft/annovar/convert2annovar.pl -format vcf4  Sample4.varscan.snp.vcf > Sample4.annovar

/home/jmzeng/bio-soft/annovar/convert2annovar.pl -format vcf4  Sample5.varscan.snp.vcf > Sample5.annovar
```
然后用下面这个脚本批量注释

![](image/C6/WES/image0016.png)

Reading gene annotation from /home/jmzeng/bio-soft/annovar/humandb/hg19_refGene.txt ... Done with 50914 transcripts (including 11516 without coding sequence annotation) for 26271 unique genes

最后查看结果可知，真正在外显子上面的突变并不多

```
23515 Sample3.anno.exonic_variant_function
23913 Sample4.anno.exonic_variant_function
24009 Sample5.anno.exonic_variant_function
```
annovar软件就是把我们得到的十万多个snp分类了，看看这些snp分别是基因的哪些位置，是否引起蛋白突变，位置信息如下：

```
downstream
exonic
exonic;splicing
intergenic
intronic
ncRNA_exonic
ncRNA_intronic
ncRNA_splicing
ncRNA_UTR3
ncRNA_UTR5
splicing
upstream
upstream;downstream
UTR3
UTR5
UTR5;UTR3
```

##### 第七步：de novo变异情况 {-}

de novo变异寻找也属于snp-calling的一部分，但是有点不同的就是该软件考虑了一家三口的测序文件，找de novo突变。

功能介绍：http://varscan.sourceforge.net/trio-calling-de-novo-mutations.html

而且还专门有一篇文章讲ASD和autism与de novo变异的关系，但个人感觉文章不清不楚，没什么意思

**Trio Calling for de novo Mutations**

![](image/C6/WES/image0017.png)

```
Min coverage:   10
Min reads2:     4
Min var freq:   0.2
Min avg qual:   15
P-value thresh: 0.05
Adj. min reads2:        2
Adj. var freq:  0.05
Adj. p-value:   0.15
```
**Reading input from trio.filter.mpileup**

```
1371416525 bases in pileup file （137M的序列）
83123183 met the coverage requirement of 10 （其中有83M的测序深度大于10X）
145104 variant positions (132268 SNP, 12836 indel) （共发现15.5万的变异位点）
4403 were failed by the strand-filter
139153 variant positions reported (126762 SNP, 12391 indel)
502 de novo mutations reported (376 SNP, 126 indel) （真正属于 de novo mutations只有502个）
1734 initially DeNovo were re-called Germline
12 initially DeNovo were re-called MIE
3 initially DeNovo were re-called MultAlleles
522 initially MIE were re-called Germline
1 initially MIE were re-called MultAlleles
3851 initially Untransmitted were re-called Germline
```
然后我看了看输出的文件**trio.mpileup.output.snp.vcf**

软件是这样解释的
> The output of the trio subcommand is a single VCF in which all variants are classified as germline (transmitted or untransmitted), de novo, or MIE.

- FILTER - mendelError if MIE, otherwise PASS

- **STATUS - 1=untransmitted, 2=transmitted, 3=denovo, 4=MIE**

- DENOVO - if present, indicates a high-confidence de novo mutation call

里面的信息量还是不清楚。

我首先对拿到的trio.de_novo.mutaion.snp.vcf文件进行简化，只看基因型！

```
head status.txt   （顺序是dad,mom,child）
STATUS=2 0/0 0/1 0/1
STATUS=2 1/1 1/1 1/1
STATUS=2 0/1 0/0 0/1
STATUS=2 1/1 1/1 1/1
STATUS=1 0/1 0/0 0/0
STATUS=1 0/1 0/0 0/0
STATUS=2 1/1 1/1 1/1
STATUS=2 1/1 1/1 1/1
STATUS=2 1/1 1/1 1/1
STATUS=2 0/1 0/1 0/1
#那么总结如下：
26564 STATUS=1 无所以无 （0/0 0/1 0/0或者 0/1 0/0 0/0等等）
97764 STATUS=2 有所以有 （1/1 1/1 1/1 或者0/1 0/1 1/1等等）
385 STATUS=3 无中生有 （0/0 0/0 0/1 或者0/0 0/0 1/1）
1485 STATUS=4 有中生无 （1/1 0/1 0/0 等等）
```
我用annovar注释了一下

```
/home/jmzeng/bio-soft/annovar/convert2annovar.pl -format vcf4  trio.mpileup.output.snp.vcf > trio.snp.annovar
/home/jmzeng/bio-soft/annovar/annotate_variation.pl -buildver hg19  --geneanno --outfile  trio.snp.anno trio.snp.annovar /home/jmzeng/bio-soft/annovar/humandb
```
结果是：
```
A total of 132268 locus in VCF file passed QC threshold, representing 132809 SNPs (86633 transitions and 46176 transversions) and 3 indels/substitutions
```
可以看到最后被**注释到外显子上面的突变有两万多个**。
```
23794  284345 3123333 trio.snp.anno.exonic_variant_function
```
这个应该是非常有意义的，但是我还没学会后面的分析。只能先做到这里了……

### 转录组

转录组测序的研究对象为特定细胞在某一功能状态下所能转录出来的**所有 RNA 的总和**，包括 mRNA 和非编码 RNA 。通过转录组测序，能够全面获得物种特定组织或器官的转录本信息，从而进行转录本结构研究、变异研究、**基因表达水平研究**以及全新转录本发现等研究。

其中，基因表达水平的探究是转录组领域**最热门**的方向，利用转录组数据来识别转录本和表达定量，是转录组数据的核心作用。由于这个作用，他可以不依赖其他组学信息，单独成为一个产品项目RNA-seq测序。所以很多时候**转录组测序**会与**RNA-seq**混为一谈。

现在RNA-seq数据**使用广泛**，但是没有一套流程可以解决所有的问题。比较值得关注的RNA-seq分析中的重要的步骤包括：实验设计，质控，read比对，表达定量，可视化，差异表达，识别可变剪切，功能注释，融合基因检测，eQTL定位等。


#### RNA-seq表达量分析指引

##### (1)：[计算机资源的准备](http://www.biotrainee.com/thread-1742-1-1.html) {-}

最好是有mac或者linux系统，8G+的内存，500G的存储即可。
如果你是Windows，那么安装必须安装 git,notepad++,everything，还有虚拟机，在虚拟机里面安装linux，最好是ubuntu。
需要安装的软件包括 sratoolkit,fastqc,hisats,samtools,htseq-count,R,Rstudio 
软件安装的代码，在生信技能树公众号后台回复**老司机**即可拿到。
进阶作业，每个软件都收集一个中文教程链接，并自己阅读，发在论坛里面。 

##### (2)：[读文章拿到测序数据](http://www.biotrainee.com/thread-1743-1-1.html) {-}

本系列课程学习的文章是：AKAP95 regulates splicing through scaffolding RNAs and RNA processing factors. Nat Commun 2016 Nov 8;7:13347. PMID: 27824034 很容易在文章里面找到数据地址GSE81916 这样就可以下载sra文件
作业，看文章里的methods部分，把它用到的软件和参数摘抄下来！

然后理解GEO/SRA数据库的数据存放形式，把规律和笔记发在论坛上面！ 

##### (3)：了解fastq测序数据 {-}

需要用安装好的sratoolkit把sra文件转换为fastq格式的测序文件，并且用fastqc等软件测试测序文件的质量！

作业，理解测序reads，GC含量，质量值，接头，index，fastqc的全部报告，搜索中文教程，并发在论坛上面。 

##### (4)：了解参考基因组及基因注释 {-}

在UCSC下载hg19参考基因组，我博客有详细说明，从gencode数据库下载基因注释文件，并且用IGV去查看你感兴趣的基因的结构，比如TP53,KRAS,EGFR等等。


作业，截图几个基因的IGV可视化结构！还可以下载ENSEMBL，NCBI的gtf，也导入IGV看看，截图基因结构。了解IGV常识。


##### (5)： 序列比对 {-}

比对软件很多，首先大家去收集一下，因为我们是带大家入门，请统一用hisat2，并且搞懂它的用法。

直接去hisat2的主页下载index文件即可，然后把fastq格式的reads比对上去得到sam文件。

接着用samtools把它转为bam文件，并且排序(注意N和P两种排序区别)索引好，载入IGV，再截图几个基因看看！

顺便对bam文件进行简单QC，参考直播我的基因组系列。 

##### (6)： reads计数 {-}

实现这个功能的软件也很多，还是烦请大家先自己搜索几个教程，入门请统一用htseq-count，对每个样本都会输出一个表达量文件。

需要用脚本合并所有的样本为表达矩阵。参考：生信编程直播第四题：多个同样的行列式文件合并起来

对这个表达矩阵可以自己简单在excel或者R里面摸索，求平均值，方差。
看看一些生物学意义特殊的基因表现如何，比如GAPDH,β-ACTIN等等。
这是一个分水岭，后面的分析主要靠R了，前面的分析都最好是在linux系统下面完成，主要是安装软件，下载数据，运行。

有几个笔记不错的整合作业：

* [转录组入门（1-6）从测序数据到生成count矩阵](http://www.biotrainee.com/thread-1931-1-1.html)
* [JD加栋 的个人博客](http://www.zd200572.com/2017/07/15/2017-RNAseq-executing/)
* [PANDA姐的转录组入门（0-6）合辑](http://www.biotrainee.com/thread-1966-1-1.html)


##### (7)： 差异基因分析 {-}

这个步骤推荐在R里面做，载入表达矩阵，然后设置好分组信息，统一用DEseq2进行差异分析，当然也可以走走edgeR或者limma的voom流程。

基本任务是得到差异分析结果，进阶任务是比较多个差异分析结果的异同点。

##### (8)： 差异基因结果注释 {-}

我们统一选择p<0.05而且abs(logFC)大于一个与众的基因为显著差异表达基因集，对这个基因集用R包做KEGG/GO超几何分布检验分析。

然后把表达矩阵和分组信息分别作出cls和gct文件，导入到GSEA软件分析。
基本任务是完成这个分析。


最后，把同样的代码实践与其它几篇转录组文章，并且把代码和分析结果发在论坛上面；

* http://biotrainee.com/jmzeng/RNA-seq/RNA-seq-example-GSE81916-two-group.sh
* http://biotrainee.com/jmzeng/RNA-seq/DEG.zip 

我以前在博客写过的

* http://www.bio-info-trainee.com/2218.html

> 比如可以来一些实战：

* 生信技能树»生信技能树›互动作业›项目实战›[mRNA-seq数据分析实战](http://www.biotrainee.com/thread-115-1-1.html)
* [MeDIP-seq,ChIP-seq,RNA-seq结合起来分析](http://www.biotrainee.com/thread-883-1-1.html)

#### RNA-seq检测变异分析实战

前面我们说到RNA-seq最重要的就是对所测样品进行基因表达量的测定，但也有部分课题需要检测RNA-seq试剂里面的变异位点。
这里也进行简单的介绍，如下：

##### RNA-seq 序列比对  {-}

对 RNA-seq 产出的数据进行变异检测分析，与常规重测序的主要区别就在序列比对这一步，因为 RNA-seq 的数据是来自转录本的，比对到参考基因组需要跨越转录剪切位点，所以 RNA-seq 进行变异检测的重点就在于**跨剪切位点的精确序列比对**

文献 [systematic evaluation of spliced alignment programs for RNA-seq data](http://www.nature.com/nmeth/journal/v10/n12/full/nmeth.2722.html) 中对 RNA-seq 数据常用的 11 款比对软件进行了详细测试，包括 STAR 2-pass，而 GATK 对 RNA-seq 数据变异检测的最佳实践流程中选用了 STAR 2-pass 这一方法进行比对，STAR 发表的文章至今已被引用 1900 余次，这款软件的比对速度很快，也是 [ENCODE](https://www.encodeproject.org/) 项目的御用比对软件。

STAR 2-pass 模式需要进行两次序列比对，建立两次参考基因组索引。它的思路是第一次建参考基因组索引之后进行初步的序列比对，根据初步比对结果得到该样本所有的剪切位点信息，包括参考基因组注释 GTF 中已知的剪切位点和比对时新发现的剪切位点，然后利用第一次比对得到的剪切位点信息重新对参考基因组建立索引，然后进行第二次的序列比对，这样可以得到更精确的比对结果。

这里使用了一个测试数据演示流程，第一次对参考基因组建索引：
```shell
# star 1-pass index
STAR --runThreadN 8 --runMode genomeGenerate \
        --genomeDir ./star_index/ \
        --genomeFastaFiles ./genome/chrX.fa \
        --sjdbGTFfile ./genes/chrX.gtf
```

然后进行第一次序列比对：
```shell
#star 1-pass align
STAR --runThreadN 8 --genomeDir ./star_index/ \
        --readFilesIn ./samples/ERR188044_chrX_1.fastq.gz ./samples/ERR188044_chrX_2.fastq.gz \
        --readFilesCommand zcat \
        --outFileNamePrefix ./star_1pass/ERR188044
```

之后根据第一次比对得到的所有剪切位点，重新对参考基因组建立索引：
```shell
# star 2-pass index
STAR --runThreadN 8 --runMode genomeGenerate \
        --genomeDir ./star_index_2pass/ \
        --genomeFastaFiles ./genome/chrX.fa \
        --sjdbFileChrStartEnd ./star_1pass/ERR188044SJ.out.tab
```

再进行 STAR 二次序列比对：
```shell
# star 2-pass align
STAR --runThreadN 8 --genomeDir ./star_index_2pass/ \
        --readFilesIn ./samples/ERR188044_chrX_1.fastq.gz ./samples/ERR188044_chrX_2.fastq.gz \
        --readFilesCommand zcat \
        --outFileNamePrefix ./star_2pass/ERR188044
```

由于后面要用 GATK 进行 call 变异，还需要对比对结果 SAM 文件进行一些处理，这些都可以用 picard 来做，包括 SAM 头文件添加 \@RG 标签，SAM 文件排序并转 BAM 格式，然后标记 duplicate：
```shell
# picard Add read groups, sort, mark duplicates, and create index
java -jar picard.jar AddOrReplaceReadGroups \
        I=./star_2pass/ERR188044Aligned.out.sam \
        O=./star_2pass/ERR188044_rg_added_sorted.bam \
        SO=coordinate \
        RGID=ERR188044 \
        RGLB=rna \
        RGPL=illumina \
        RGPU=hiseq \
        RGSM=ERR188044 

java -jar picard.jar MarkDuplicates \
        I=./star_2pass/ERR188044_rg_added_sorted.bam \
        O=./star_2pass/ERR188044_dedup.bam  \
        CREATE_INDEX=true \
        VALIDATION_STRINGENCY=SILENT \
        M=./star_2pass/ERR188044_dedup.metrics
```
到此序列比对就完成了。

---

##### 使用 GATK 进行变异检测  {-}

感觉 GATK 里面的工具都很慢（相对于其他的软件特别慢！），都是单线程在跑，有的虽然可以设置为多线程但是感觉没啥速度上的提升，所以要有点耐心……

由于 STAR 软件使用的 MAPQ 标准与 GATK 不同，而且比对时会有 reads 的片段落到内含子区间，需要进行一步 MAPQ 同步和 reads 剪切，使用 GATK 专为 RNA-seq 应用开发的工具 `SplitNCigarReads` 进行操作，它会将落在内含子区间的 reads 片段直接切除，并对 MAPQ 进行调整。DNA 测序的重测序应用中也有序列比对软件的 MAPQ 与 GATK 无法直接对接的情况，需要进行调整。

```shell
# samtools faidx chrX.fa
# samtools dict chrX.fa
java -jar GenomeAnalysisTK.jar -T SplitNCigarReads \
        -R ./genome/chrX.fa \
        -I ./star_2pass/ERR188044_dedup.bam \
        -o ./star_2pass/ERR188044_dedup_split.bam \
        -rf ReassignOneMappingQuality \
        -RMQF 255 \
        -RMQT 60 \
        -U ALLOW_N_CIGAR_READS
```

之后就是可选的 Indel Realignment，对已知的 indel 区域附近的 reads 重新比对，可以稍微提高 indel 检测的真阳性率，如果时间紧张也可以不做，这一步影响很轻微

```shell
# 可选步骤 IndelRealign
java -jar GenomeAnalysisTK.jar -T RealignerTargetCreator \
        -R ./genome/chrX.fa \
        -I ./star_2pass/ERR188044_dedup_split.bam \
        -o ./star_2pass/ERR188044_realign_interval.list \
        -known Mills_and_1000G_gold_standard.indels.hg19.sites.vcf 

java -jar GenomeAnalysisTK.jar -T IndelRealigner \
        -R ./genome/chrX.fa \
        -I ./star_2pass/ERR188044_dedup_split.bam \
        -known Mills_and_1000G_gold_standard.indels.hg19.sites.vcf \
        -o ./star_2pass/ERR188044_realign.bam \
        -targetIntervals ./star_2pass/ERR188044_realign_interval.list

        
```

然后还是可选的 BQSR，这一步操作主要是针对测序质量不太好的数据，进行碱基质量再校准，如果对自己的测序数据质量足够自信可以省略，2500 之后 Hiseq 仪器的数据质量都挺不错的，可以根据 FastQC 结果来决定。这一步省了又能节省时间

```shell
# 可选步骤 BQSR
java -jar GenomeAnalysisTK.jar \
        -T BaseRecalibrator \
        -R ./genome/chrX.fa \
        -I ./star_2pass/ERR188044_realign.bam \
        -knownSites 1000G_phase1.snps.high_confidence.hg19.sites.vcf \
        -knownSites Mills_and_1000G_gold_standard.indels.hg19.sites.vcf \
        -o ./star_2pass/ERR188044_recal_data.table

java -jar GenomeAnalysisTK.jar  \
        -T PrintReads \
        -R ./genome/chrX.fa \
        -I ./star_2pass/ERR188044_realign.bam \
        -BQSR ./star_2pass/ERR188044_recal_data.table \
        -o ./star_2pass/ERR188044_BQSR.bam

```

比如下面的数据就可以放心的省略这两步了：

![R1 数据质量够好](image/C6/R1_QC.png)

![R2 数据质量也够好](image/C6/R2_QC.png)

现在终于可以进行变异检测了，GATK 官网说 HC 表现比 UC 好，所以这里用 HC 进行变异检测：
```shell
java -jar GenomeAnalysisTK.jar -T HaplotypeCaller \
        -R ./genome/chrX.fa \
        -I ./star_2pass/ERR188044_dedup_split.bam \
        -dontUseSoftClippedBases \
        -stand_call_conf 20.0 \
        -o ./star_2pass/ERR188044.vcf

```
call 完变异之后再进行过滤：
```shell
java -jar GenomeAnalysisTK.jar \
        -T VariantFiltration \
        -R ./genome/chrX.fa \
        -V ./star_2pass/ERR188044.vcf \
        -window 35 \
        -cluster 3 \
        -filterName FS -filter "FS > 30.0" \
        -filterName QD -filter "QD < 2.0" \
        -o ./star_2pass/ERR188044_filtered.vcf
```
然后就拿到变异检测结果了，可以用 ANNOVAR 或 SnpEff 或 VEP 进行注释，根据自己的需要进行筛选了。


  [1]: http://static.zybuluo.com/wangpeng905/tiitfgpjnuvyvfdnvvnud319/R1_QC.png
  [2]: http://static.zybuluo.com/wangpeng905/iqx3yy6wf72dmodirjnjvs6j/R2_QC.png
  
#### 全转录组

> 全转录组即包含mRNA，small RNA，lncRNA，circRNA的测序。

搞研究的小伙伴目前大多默认转录组指的是mRNA。而我们知道转录组中也包含着非编码RNA（ncRNA），包括具有调控功能的ncRNAs和管家RNAs（如tRNA，rRNA等）。对转录组中的非编码RNA，目前大家的研究多集中在具有调控功能的small RNA（以miRNA为代表），长链非编码RNA（lncRNA）和环状RNA（circRNA）上，而这几类ncRNA的调控对象都和mRNA有关。因此，全转录组即包含了ncRNAs和mRNA。
大家目前主要关心调控ncRNAs与mRNA的相互作用关系。全转录组的提出，是为了与大家默认的转录组（mRNA）有所区分，并不是一个新概念。但是在实验和数据分析上，两者还是有一些不同的。

##### 全转录组文库构建 {-}

全转录组测序是通过构建2个文库，1个小RNA文库（包含miRNA）和一个去除核糖体的链特异性文库（包含mRNA，lncRNA，和circRNA），然后分别上机测序，最后主要获得4类RNA——miRNA，lncRNA，mRNA，circRNA的序列信息。

由于miRNA序列较短（18~26 nt），而其他三类RNA序列较长（通常在1 kb以上），因此它们构建文库的方式会不一样，长序列需要先片段化再建库。这也造成了两者的测序读长是不一样的，小RNA文库的测序读长是SE50，长序列文库的测序读长是PE150，因而需要分别上机测序。
我们熟悉的转录组测序只需要构建一个文库，针对mRNA进行序列鉴定和分析。

##### 全转录组测序的数据分析  {-}

不同于只能获取mRNA信息的转录组测序，在全转录组测序完成后，我们可以获取4类主要RNA（miRNA，mRNA，lncRNA，circRNA）的序列信息。接着我们对这4类RNA进行标准分析和整合分析。
标准分析是针对每类RNA主要讨论，序列鉴定，序列特征分析，差异表达分析，和功能富集分析；
整合分析是全转录组测序的重点内容，基于标准分析的结果，整合分析重点讨论非编码RNA与编码RNA间的调控关系，主要包括两者间互作关系miRNA vs mRNA，miRNA vs lncRNA，miRNA vs circRNA，再进一步讨论三者间互作关系mRNA vs miRNA vs lncRNA，mRNA vs miRNA vs circRNA。而三者间的互作关系正是目前讨论得如火如荼的ceRNA（competing endogenous RNA）调控网络。

 
#### 可变剪切

##### 什么是可变剪接，研究可变剪接的意义？  {-}

某些基因的一个mRNA前体可以通过选择不同的剪接位点产生不同的可变剪接（Alternative Splicing, AS）形式，这种可变剪接通过以下两种方式参与基因表达调控：

* 1）导致一个基因形成多个剪接异构体（isoform），从而编码不同的蛋白质
* 2）通过无义介导的mRNA降解（NMD）和miRNA调节来调整mRNA的稳定性及翻译。

可变剪接是转录本和蛋白质多样性的主要来源，已有研究表明可变剪接与某些数量性状基因、性别决定通路（如果蝇）、遗传疾病（如脆性X综合征）等密切相关。对可变剪接进行精准的鉴定分析，能够更深入地研究基因表达模型和调控机制。

##### 如何对标准的mRNA-seq数据进行可变剪切分析？   {-}

这个数据分析点比较小众，理论上不属于本书的讲解范畴，我们就简单描述部分。可以用ASprofile 软件对 Cufflinks 预测的转录本的可变剪切事件进行分类统计，常见可变剪切事件如下所示:

- AE: Alternative exon ends (5' , 3' , or both) ----- 可变 5' 或3' 端剪切
- XAE: Approximate AE (5' , 3' , or both) ----- 近似可变 5' 或3' 端剪切
- IR: Intron retention ----- 单内含子保留
- XIR: Approximate IR ----- 近似单内含子保留
- MIR: Multi-IR ----- 多内含子保留
- XMIR: Approximate MIR ----- 近似多内含子保留
- TSS: Alternative 5' first exon ----- 第一个外显子可变剪切
- TTS: Alternative 3' last exon ----- 最后一个外显子可变剪切
- SKIP: Skipped exon ----- 单外显子跨跃
- XSKIP: Approximate SKIP ----- 近似单外显子跨跃
- MSKIP: Multi-exon SKIP ----- 多外显子跨跃
- XMSKIP: Approximate MSKIP ----- 近似多外显子跨跃

可以将**Length >= 200bp** 且 **exon number >=2** 作为可靠的新基因的转录本筛选条件，新基因的转录本需要做Nr, KEGG的数据库注释。

至于基因结构优化，对于人、小鼠、拟南芥等模式生物来说，基因注释相对完整，但对于研究没那么透彻的其他物种，我们可以用reads来优化基因结构，从而完善它们的基因注释信息。

Reads比对参考基因组后，我们用**Cufflink**软件对reads进行转录本重构，将重构结果与参考转录本序列进行比较，重构出来的转录本可能会延长基因注释的5’或3’端，由此实现优化基因结构的目的。

##### 可变剪切事件的可视化 {-}

Python软件SpliceGrapher可以将可变剪切模型绘制成图片

SpliceGrapher可以根据测序序列预测可变剪切模型，也可以用已知的基因注释文件生成可变剪切模型。它需要两种输入文件，注释gtf/gff3文件和测序reads与参考基因组比对的sam文件。SpliceGrapher可以预测出多种多样的可变剪切事件，已知的基因模型中存在Alt 3’（可变3’端）、Skipped Exon（跳过外显子）、Intron Retention（内含子保留）、Alt 5’（可变5’端）、既是Alt 3’又是Skipped Exon、既是Alt 5’又是Intron Retention等多种可变剪切事件。不同的事件用不同的颜色标注出来。

发表这个工具的文章是Xiaoxian Liu, et al. Detecting Alternatively Spliced Transcript Isoforms from Single-Molecule Long-Read Sequences without a Reference Genome. Mol Ecol Resour. (2017)

#### 融合基因分析

基因融合现象发生在很多致命疾病中，如果是癌症等疾病的融合基因分析可以使用defuse软件， 利用reads的基因跨越以及一对reads的相对距离进行基因融合查找，当然，也可以使用BGI的SOAP系列，生信菜鸟团博客里面有详细描述软件的安装以及示例数据的测试使用。

结题报告能给用户的就是软件找到的可能的融合基因表格,表格里面可能有下面的内容：



#### 自学miRNA-seq八讲

##### 第一讲：文献选择与解读  {-}

前阵子逛BioStar论坛的时候看到了一个关于miRNA分析的问题，提问者从NCBI的SRA中下载文献提供的原始数据，然后处理的时候出现了问题。我看到他列出的数据来自iron torrent测序仪，而且我以前也没有做过miRNA-seq的数据分析， 就自学了一下。因为我有RNA-seq的基础，所以理解学习起来比较简单。

在这里记录自己的学习过程，希望对需要的朋友有帮助。

这里选择的文章是2014年发表的，作者用ET-1刺激human iPSCs (hiPSC-CMs) 细胞前后，观察miRNA和mRNA表达量的变化，我并没有细看文章的生物学意义，仅仅从数据分析的角度解读一下这篇文章，mRNA表达量用的是Affymetrix Human Genome U133 Plus 2.0 Array，分析起来特别容易。得到表达矩阵，然后用limma这个包找差异表达基因即可。

但是miRNA分析起来就有点麻烦了，作者用的是iron torrent测序仪。不过从SRA数据中心下载的是已经去掉接头的fastq格式测序数据，所以这里其实并不需要考虑测序仪的特异性。

###### 关于该文章的几个资料 {-}

- paper : http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0108051
- Aggarwal P, Turner A, Matter A, Kattman SJ et al. RNA expression profiling of human iPSC-derived cardiomyocytes in a cardiac hypertrophy model. PLoS One 2014;9(9):e108051. PMID: 25255322
- The accession numbers are 1. SuperSeries (mRNA+miRNA) - GSE60293
- mRNA expression array - GSE60291  (Affymetrix Human Genome U133 Plus 2.0 Array)
- miRNA-Seq - GSE60292  (Ion Torrent)
- GEO   : http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE60292
- FTP   : ftp://ftp-trace.ncbi.nlm.nih.gov/sra/sra-instant/reads/ByStudy/sra/SRP/SRP045/SRP045420

接下来我们要知道文章做了哪些分析，然后才能自己模仿看是否可以得到同样的分析结果。

###### 文章数据处理流程 {-}

- Ion Torrent's Torrent Suite version 3.6 was used for basecalling
- Raw sequencing reads were aligned using the SHRiMP2 aligner and were aligned against the human reference genome (hg19) for novel miRNA prediction and then against a custom reference sequence file containing miRBase v.20 known human miRNA hairpins, tRNA, rRNA, adapter sequences and predicted novel miRNA sequences.(Genome_build: hg19, miRBase v.20 human miRNA hairpins)
- The miRDeep2 package (default parameters) was used to predict novel (as yet undescribed) miRNAs
- Alignments with less than 17 bp matches and a custom 3′ end phred q-score threshold of 17 were filtered out.
- miRNA quanitification was done using HTSeq v0.5.3p3 using the default union parameter.
  Differential miRNA expression was analyzed using the DESeq (v.1.12.1) R/Bioconductor package
- In this study, differentially expressed genes that had a false discovery rate cutoff at 10% (FDR< = 0.1), a log2 fold change greater than 1.5 and less than −1.5 were considered significant.
- Target gene prediction was performed using the TargetScan (version 6.2) database
- We also used miRTarBase (version 4.3), to identify targets that have been experimentally validated
- miR-Deep2 and miReap predict exact precursor sequence according from mature sequence 



 文章提到了fastq数据**质量控制**标准，**数据比对**工具，比对的**参考基因组**（两条比对线路），**获得miRNA表达量**，**miRNA预测**，**miRNA靶基因预测**。

这也是我们学习miRNA-seq的数据分析的标准套路， 而且作者给出了所有的分析结果，我们完全可以通过自己的学习来重现他的分析过程。



- Supplementary_files_format_and_content: tab-delimited text files containing raw read counts for known mature human miRNAs.（表达矩阵）

- We detected 836 known human mature miRNAs in the control-CMs and 769 in the ET1-CMs

- Based on our miRNA-Seq data, we predicted 506 sequences to be potentially novel, as yet undescribed miRNAs.

- In order to validate the expression profiles of the miRNAs detected, we performed RT-qPCR on a subset of five known human mature and five of our predicted novel miRNAs.

- we obtained a total of 1,922 predicted miRNA-mRNA pairs represented by 309 genes and 174 known mature human miRNAs. 

当然仅仅是套路分析还不够，所以他进行了 miRNA和mRNA 进行网络分析并做了少量湿实验来验证，最后还扯了一些生物学意义。

---

##### 第二讲：搜集学习资料 {-}

因为我是完全从零开始入门miRNA-seq分析，所以收集的资料比较齐全。

首先看了部分中文资料，**了解miRNA测序是怎么回事，该分析什么，然后主要围绕着第一讲文献里的分析步骤来搜索资料。**

###### miRNA定义 {-}

我首先找到了miRNA定义：[http://nar.oxfordjournals.org/content/34/suppl_1/D135.full ](http://nar.oxfordjournals.org/content/34/suppl_1/D135.full)

>  MicroRNAs (miRNAs) are **small RNA molecules**, which are **∼22 nt sequences**  that have an important role in the translational regulation and degradation of mRNA by the base's pairing to the 3′-untranslated regions (3′-UTR) of the mRNAs. The miRNAs are derived from the **precursor transcripts of ∼70–120 nt sequences**, which fold to **form as stem–loop structures**, which are thought to be highly conserved in the evolution of genomes. Previous analyses have suggested that **∼1% of all human genes are miRNA genes,** which regulate the production of protein for 10% or more of all human coding genes。

###### 选择参考序列 {-}

然后我比较纠结的问题是参考序列如何选择，因为miRNA序列很少，把它map到3G大小的人类基因组有点浪费计算资源，正好我的服务器又坏了，不想太麻烦，想用自己的个人电脑搞定这个学习过程。

我看到很多帖子提到的都是用bowtie比对到**参考miRNA数据库**(miRNA count: 28645 entries)  [http://www.mirbase.org/](http://www.mirbase.org/) ，从这个数据库，我明白了前体miRNA和成熟的miRNA的区别，前体miRNA长度一般是**70–120 **碱基，一般是茎环结果，也就是发夹结构，所以叫做hairpin。成熟之后，一般**22 个碱基，**在miRNA数据库很容易下载到这些数据，目前人类这个物种已知的成熟miRNA共有2588条序列，而前体miRNA共有1881条序列，我下载（下载时间2016年6月 ）的代码如下所示：

```
wget ftp://mirbase.org/pub/mirbase/CURRENT/hairpin.fa.gz   ##　28645　reads
wget ftp://mirbase.org/pub/mirbase/CURRENT/mature.fa.zip   ##   35828 reads 
wget ftp://mirbase.org/pub/mirbase/CURRENT/hairpin.fa.zip
wget ftp://mirbase.org/pub/mirbase/CURRENT/genomes/hsa.gff3 ##
wget ftp://mirbase.org/pub/mirbase/CURRENT/miFam.dat.zip
grep sapiens mature.fa |wc  　# 2588 
grep sapiens hairpin.fa |wc       # 1881 
## Homo sapiens
perl -alne '{if(/^>/){if(/Homo/){$tmp=1}else{$tmp=0}};next if $tmp!=1;s/U/T/g if !/>/;print }' hairpin.fa >hairpin.human.fa
perl -alne '{if(/^>/){if(/Homo/){$tmp=1}else{$tmp=0}};next if $tmp!=1;s/U/T/g if !/>/;print }' mature.fa >mature.human.fa
# 这里值得一提的是miRBase数据库下载的序列，居然都是用U表示的，也就是说就是miRNA序列，而不是转录成该miRNA的基因序列，而我们测序的都是基因序列。
```

通过这个代码制作的**hairpin.human.fa** 和 **mature.human.fa** 就是本次数据分析的参考基因组。

在搜集资料的过程中，我看到了一篇文献讲挖掘1000genomes的数据找到位于miRNA的snp位点

[https://genomemedicine.biomedcentral.com/articles/10.1186/gm363](https://genomemedicine.biomedcentral.com/articles/10.1186/gm363) 

看起来比较新奇，不过跟本次学习过程没有关系，我就是记录一下，有空回来学习学习。



**博客讲解如何分析miRNA数据**

- [http://genomespot.blogspot.com/2013/08/quick-alignment-of-microrna-seq-data-to.html](http://genomespot.blogspot.com/2013/08/quick-alignment-of-microrna-seq-data-to.html)

**公司数据分析流程：**

- [http://bioinfo5.ugr.es/miRanalyzer/miRanalyzer_tutorial.html](http://bioinfo5.ugr.es/miRanalyzer/miRanalyzer_tutorial.html)

- [http://www.partek.com/sites/default/files/Assets/UserGuideMicroRNAPipeline.pdf](http://www.partek.com/sites/default/files/Assets/UserGuideMicroRNAPipeline.pdf)

- [http://partek.com/Tutorials/microarray/microRNA/miRNA_tutorial.pdf](http://partek.com/Tutorials/microarray/microRNA/miRNA_tutorial.pdf)

- [http://www.arraystar.com/reviews/microrna-sequencing-data-analysis-guideline/](http://www.arraystar.com/reviews/microrna-sequencing-data-analysis-guideline/)

- [http://bioinfo5.ugr.es/sRNAbench/sRNAbench_tutorial.pdf](http://bioinfo5.ugr.es/sRNAbench/sRNAbench_tutorial.pdf)

- [http://seqcluster.readthedocs.io/mirna_annotation.html](http://seqcluster.readthedocs.io/mirna_annotation.html)

**耶鲁大学**

- [http://www.yale.edu/giraldezlab/miRNA.html](http://www.yale.edu/giraldezlab/miRNA.html)

**南方基因**

-  [http://www.southgene.com/newsshow.php?cid=55&id=73](http://www.southgene.com/newsshow.php?cid=55&id=73)

**miRNA研究整套方案** 

- [http://wenku.baidu.com/view/5f38577a31b765ce05081429.html?re=view](http://wenku.baidu.com/view/5f38577a31b765ce05081429.html?re=view)

**Biostar 讨论帖子**

- [https://www.biostars.org/p/3344/](https://www.biostars.org/p/3344/)

- [https://www.biostars.org/p/98486/](https://www.biostars.org/p/98486/)

**miRNA-seq数据处理实战指南**

- [http://bib.oxfordjournals.org/content/early/2015/04/17/bib.bbv019.full](http://bib.oxfordjournals.org/content/early/2015/04/17/bib.bbv019.full)

**直接用一个包搞定**

[- http://bioconductor.org/packages/release/bioc/html/easyRNASeq.html](http://bioconductor.org/packages/release/bioc/html/easyRNASeq.html)

**github流程**：miRNA Analysis Pipeline v0.2.7

- [https://github.com/bcgsc/mirna/tree/master/v0.2.7](https://github.com/bcgsc/mirna/tree/master/v0.2.7)

- [https://tools.thermofisher.com/content/sfs/manuals/CO25176_0512.pdf](https://tools.thermofisher.com/content/sfs/manuals/CO25176_0512.pdf)

**miRNA annotation**

- [http://seqcluster.readthedocs.io/mirna_annotation.html](http://seqcluster.readthedocs.io/mirna_annotation.html)

**网页版分析工具**

- [https://wiki.uio.no/projects/clsi/images/2/2f/HTS_2014_miRNA_analysis_Lifeportal_14_final.pdf](https://wiki.uio.no/projects/clsi/images/2/2f/HTS_2014_miRNA_analysis_Lifeportal_14_final.pdf)
- [http://www.training.prace-ri.eu/uploads/tx_pracetmo/NGSdataAnalysisWithChipster.pdf](http://www.training.prace-ri.eu/uploads/tx_pracetmo/NGSdataAnalysisWithChipster.pdf)

**可视化IGV User Guide**

- [http://www.broadinstitute.org/igv/book/export/html/6](http://www.broadinstitute.org/igv/book/export/html/6)

比较特殊的是新的miRNA预测，miRNA靶基因预测，这块软件太多并没有成型的流程和标准。

---

##### 第三讲：下载公共数据 {-}

前面已经讲到了该文章的数据已经上传到NCBI的SRA数据中心，所以直接根据索引号下载，然后用**SRAtoolkit**转出我们想要的fastq测序数据即可。

下载的数据一般要进行质量控制，可视化展现一下质量如何，然后根据大题测序质量进行简单过滤。所以需要提前安装一些软件来完成这些任务，包括：**sratoolkit /fastx_toolkit /fastqc/bowtie2/hg19/miRBase/SHRiMP**

下面是我用新服务器下载安装软件的一些代码记录，因为fastx_toolkit /fastqc我已经安装过，就不列代码了

```
## pre-step: download sratoolkit /fastx_toolkit_0.0.13/fastqc/bowtie2/hg19/miRBase/SHRiMP
## http://www.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=software
## http://www.ncbi.nlm.nih.gov/books/NBK158900/
## 我这里特意挑选的二进制版本程序下载的，这样直接解压就可以用，但是需要挑选适合自己的操作系统的程序。
cd ~/biosoft
mkdir sratoolkit &&  cd sratoolkit
wget http://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/2.6.3/sratoolkit.2.6.3-centos_linux64.tar.gz
##
##  Length: 63453761 (61M) [application/x-gzip]
##  Saving to: "sratoolkit.2.6.3-centos_linux64.tar.gz"
tar zxvf sratoolkit.2.6.3-centos_linux64.tar.gz
cd ~/biosoft
mkdir bowtie &&  cd bowtie
wget https://sourceforge.net/projects/bowtie-bio/files/bowtie2/2.2.9/bowtie2-2.2.9-linux-x86_64.zip/download
#Length: 27073243 (26M) [application/octet-stream]
#Saving to: "download"
mv download  bowtie2-2.2.9-linux-x86_64.zip
unzip bowtie2-2.2.9-linux-x86_64.zip
## http://compbio.cs.toronto.edu/shrimp/
mkdir SHRiMP &&  cd SHRiMP
wget http://compbio.cs.toronto.edu/shrimp/releases/SHRiMP_2_2_3.lx26.x86_64.tar.gz
tar zxvf SHRiMP_2_2_3.lx26.x86_64.tar.gz 
cd SHRiMP_2_2_3
export SHRIMP_FOLDER=$PWD  ## 这个软件使用的时候比较奇葩，需要设置到环境变量，不能简单的调用全路径
```

**SHRiMP**这个软件比较小众，我也是第一次听说过。

本来我计划是能用bowtie搞定，但是第一次比对出了一个bug，就是下载的miRNA序列里面的U没有转换成T，所以导致比对率非常之低。于是我不得不根据文章里面记录的软件SHRiMP 来做比对，最后发现比对率完全没有改善，搞得我都在怀疑是不是作者乱来了。

下面是下载数据，质量控制的代码，希望大家可以照着运行一下。

```
## step1 : download raw data
mkdir miRNA_test && cd miRNA_test
echo {14..19} |sed 's/ /\n/g' |while read id; \
do  wget "ftp://ftp-trace.ncbi.nlm.nih.gov/sra/sra-instant/reads/ByStudy/sra/SRP/SRP045/SRP045420/SRR15427$id/SRR15427$id.sra"  ;\
done
## step2 :  change sra data to fastq files.
## 主要是用shell脚本来批量下载
ls *sra |while read id; do ~/biosoft/sratoolkit/sratoolkit.2.6.3-centos_linux64/bin/fastq-dump $id;done
rm *sra
##  33M --> 247M
#Read 1866654 spots for SRR1542714.sra
#Written 1866654 spots for SRR1542714.sra
## step3 : download the results from paper
## http://www.bio-info-trainee.com/1571.html
## ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE1nnn/GSE1009/suppl/GSE1009_RAW.tar
mkdir paper_results && cd paper_results
wget ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE60nnn/GSE60292/suppl/GSE60292_RAW.tar
## tar xvf GSE60292_RAW.tar
ls *gz |while read id ; do (echo $id;zcat $id | cut -f 2 |perl -alne '{$t+=$_;}END{print $t}');done
ls *gz |xargs gunzip
## step4 : quality assessment
ls *fastq | while read id ; do ~/biosoft/fastqc/FastQC/fastqc $id;done
## Sequence length 8-109
## %GC 52
## Adapter Content passed
## write a script : :: cat >filter.sh
ls *fastq |while read id
do
echo $id
~/biosoft/fastx_toolkit_0.0.13/bin/fastq_quality_filter -v -q 20 -p 80 -Q33  -i $id -o tmp ;
~/biosoft/fastx_toolkit_0.0.13/bin/fastx_trimmer -v -f 1 -l 27 -i tmp  -Q33 -z -o ${id%%.*}_clean.fq.gz ;
done
rm tmp
## discarded 12%~~49%%
ls *_clean.fq.gz | while read id ; do ~/biosoft/fastqc/FastQC/fastqc $id;done
mkdir QC_results
mv *zip *html QC_results
```

这个代码是我自己根据文章的理解写出的，因为我本身不擅长miRNA数据分析，所以在进行QC的时候参数选择可能并不是那么友好.

```
~/biosoft/fastx_toolkit_0.0.13/bin/fastq_quality_filter -v -q 20 -p 80 -Q33  -i $id -o tmp ;
~/biosoft/fastx_toolkit_0.0.13/bin/fastx_trimmer -v -f 1 -l 27 -i tmp  -Q33 -z -o ${id%%.*}_clean.fq.gz ;
```

最后得到的**clean.fq.gz**系列文件，就是我需要进行比对的序列。

---

##### 第四讲：测序数据比对 {-}

序列比对是大多数类型数据分析的核心，如果要利用好测序数据，比对细节非常重要，我这里只是研读一篇文章也就没有对比对细节过多考虑，只是列出自己的代码和自己的几点思考，力求重现文章作者的分析结果。

对miRNA-seq数据有两条比对策略：

1. 下载miRBase数据库里面的已知miRNA序列来进行比对

2. 直接比对到参考基因组(比如人类的是hg19/hg38)

前面的比对非常简单，而且很容易就可以数出已经的所以miRNA序列的表达量；后面的比对有点耗时，而且算表达量的时候也不是很方便，但是它的优点是可以来预测新的miRNA，所以大多数文章都会把这两条路给走一下。

本文选择的是SHRiMP这个小众软件，起初我并没有在意，就用的bowtie2而已，参考基因组就用了miRBase数据库下载的人类的参考序列。

```
## step5 : alignment to miRBase v21 (hairpin.human.fa/mature.human.fa )
#### step5.1 using bowtie2 to do alignment
mkdir  bowtie2_index &&  cd bowtie2_index
~/biosoft/bowtie/bowtie2-2.2.9/bowtie2-build ../hairpin.human.fa hairpin_human
~/biosoft/bowtie/bowtie2-2.2.9/bowtie2-build ../mature.human.fa  mature_human
ls *_clean.fq.gz | while read id ; do  ~/biosoft/bowtie/bowtie2-2.2.9/bowtie2 -x miRBase/bowtie2_index/hairpin_human -U $id   -S ${id%%.*}.hairpin.sam ; done
## overall alignment rate:  10.20% / 5.71%/ 10.18%/ 4.36% / 10.02% / 4.95%  (before convert U to T )
## overall alignment rate:  51.77% / 70.38%/51.45% /61.14%/ 52.20% / 65.85% (after convert U to T )
ls *_clean.fq.gz | while read id ; do  ~/biosoft/bowtie/bowtie2-2.2.9/bowtie2 -x miRBase/bowtie2_index/mature_human  -U $id   -S ${id%%.*}.mature.sam ; done
## overall alignment rate:  6.67% / 3.78% / 6.70% / 2.80%/ 6.55% / 3.23%    (before convert U to T )
## overall alignment rate:  34.94% / 46.16%/ 35.00%/ 38.50% / 35.46% /42.41%(after convert U to T )
#### step5.2 using SHRiMP to do alignment
##    http://compbio.cs.toronto.edu/shrimp/README
##    3.5 Mapping cDNA reads against a miRNA database
cd ~/biosoft/SHRiMP/SHRiMP_2_2_3
export SHRIMP_FOLDER=$PWD
cd -
##　　We project the database with:
$SHRIMP_FOLDER/utils/project-db.py --seed 00111111001111111100,00111111110011111100,00111111111100111100,00111111111111001100,00111111111111110000 \
 --h-flag --shrimp-mode ls miRBase/hairpin.human.fa
##
$SHRIMP_FOLDER/bin/gmapper-ls -L  hairpin.human-ls SRR1542716.fastq  --qv-offset 33   \
-o 1 -H -E -a -1 -q -30 -g -30 --qv-offset 33 --strata -N 8  >map.out 2>map.log
```

大家可以看到我们把测序reads比对到前体miRNA和成熟的miRNA结果是有略微区别的，**因为一个前体miRNA可以形成多个成熟的miRNA，而并不是所有的成熟的miRNA形式都被记录在数据库，所以一般推荐比对到前体miRNA数据库，这样还可以预测新的成熟miRNA，也是非常有意义的。**

另外非常重要的一点是，把U变成T前后比对率差异非常大，这其实是一个非常蠢的错误，我就不多说了。但是做到这一步，其实可以跟文章来做验证，文章有提到比对率，比对的序列。

我也是在博客里面看到这个信息的：

> Thank you so  much!. Yes I contacted the lab-guy and he just said that trimmed the first 4 bp and last 4bp. ( as you found)
> So  I firstly trimmed the adapter sequences(TGGAATTCTCGGGTGCCAAGGAACTCCAGTCAC)
> And then, trimmed the first 4bp and last 4bp from reads, which leads to the 22bp peak of read-length distribution(instead of 24bp)
> Anyhow, I tried to map with bowtie2 again.
```
> bowtie2 --local -N 1 -L 16
> -x ../miRNA_reference/hairpin_UtoT.fa
> -U first4bptrimmed_A1-SmallRNA_S1_L001_R1_001_Illuminaadpatertrim.fastq
> -S f4_trimmed.sam
```

> I also changed hairpin.fa file (U to T) 
> Oh.. thank you David,
> Finallly, I got

```
>  2565353 reads; of these:
>  2565353 (100.00%) were unpaired; of these:
>  479292 (18.68%) aligned 0 times
>  11959 (0.47%) aligned exactly 1 time
>  2074102 (80.85%) aligned >1 times
>  81.32% overall alignment rate
```

---

##### 第五讲：获取miRNA表达量 {-}

得到比对后的sam/bam文件只能算是level2的数据，一般我们给他人share的结果也是直接给表达矩阵的， miRNA分析跟mRNA分析类似，但是它的表达矩阵更好获取一点。

如果是mRNA，我们一般会跟基因组来比较，而基因组就是24条参考染色体，想知道具体比对到了哪个基因，需要根据基因组注释文件来写程序提取表达量信息，现在比较流行的是htseq这个软件，我前面也写过教程如何安装和使用，这里就不啰嗦了。但是对于miRNA，因为我比对的就是那1881条前体miRNA序列，所以直接分析比对的sam/bam文件就可以知道每条参考miRNA序列的表达量了。 

```
## step6: counts the reads which mapping to each miRNA reference.
## we need to exclude unmapped as well as multiple-mapped  reads
## XS:i:<n> Alignment score for second-best alignment. Can be negative. Can be greater than 0 in --local mode
## NM:i:1   ## NM i Edit distance to the reference, including ambiguous bases but excluding clipping
#The following command exclude unmapped (-F 4) as well as multiple-mapped (grep -v “XS:”) reads
#samtools view -F 4 input.bam | grep -v "XS:" | wc -l
## 180466//1520320
##cat >count.hairpin.sh
ls *hairpin.sam  | while read id
do
samtools view  -SF 4 $id |perl -alne '{$h{$F[2]}++}END{print "$_\t$h{$_}" foreach sort keys %h }'  > ${id%%_*}.hairpin.counts
done
## bash count.hairpin.sh
##cat >count.mature.sh
ls *mature.sam  | while read id
do
samtools view  -SF 4 $id |perl -alne '{$h{$F[2]}++}END{print "$_\t$h{$_}" foreach sort keys %h }'  > ${id%%_*}.mature.counts
done
## bash count.mature.sh
```

上面的代码，是我自己写的脚本来算表达量，非常简单，因为我没有考虑细节，直接想得到各个样本测序数据的表达量而已。如果是比对到了参考基因组，就要根据miRNA的gff注释文件用htseq等软件来计算表达量。

得到了表达量，就可以跟文献来做比较

```
### step7: compare the results with paper's
GSM1470353: control-CM, experiment1; Homo sapiens; miRNA-Seq   SRR1542714
GSM1470354: ET1-CM, experiment1; Homo sapiens; miRNA-Seq  SRR1542715
GSM1470355: control-CM, experiment2; Homo sapiens; miRNA-SeqSRR1542716
GSM1470356: ET1-CM, experiment2; Homo sapiens; miRNA-Seq SRR1542717
GSM1470357: control-CM, experiment3; Homo sapiens; miRNA-Seq SRR1542718
GSM1470358: ET1-CM, experiment3; Homo sapiens; miRNA-Seq SRR1542719
### 下面我用R语言来检验一下，我得到的分析结果跟文章发表的结果的区别。
a=read.table("bowtie_bam/SRR1542714.mature.counts")
b=read.table("paper_results/GSM1470353_iPS_010313_Unstim_known_miRNA_counts.txt")
plot(log(tmp[,2]),log(tmp[,3]))
cor(tmp[,2],tmp[,3])
##[1] 0.8413439
```

相关性还不错，总算没有分析错。

这个代码是我自己根据文章的理解写出的，因为我本身不擅长miRNA数据分析，所以在进行alignment的时候参数选择可能并不是那么友好。

---

##### 第六讲：miRNA表达量差异分析 {-}

这一讲是miRNA-seq数据分析的分水岭，前面的5讲说的是读文献下载数据比对，然后计算表达量，属于常规的流程分析，一般在公司测序之后都可以拿到分析结果，或者文献也会给出下载结果。

但是单纯的分析一个样本意义不大，一般来说，我们做研究都是针对于不同状态下的miRNA表达量差异分析，然后做注释，功能分析，网络分析，这才是重点和难点。

我这里就直接拿文献处理好的miRNA表达量来展示如何做下游分析，首先就是差异分析。

根据文献，我们可以知道样本的分类情况是

> GSM1470353: control-CM, experiment1; Homo sapiens; miRNA-Seq   SRR1542714
>
> GSM1470354: ET1-CM, experiment1; Homo sapiens; miRNA-Seq  SRR1542715
>
> GSM1470355: control-CM, experiment2; Homo sapiens; miRNA-SeqSRR1542716
>
> GSM1470356: ET1-CM, experiment2; Homo sapiens; miRNA-Seq SRR1542717
>
> GSM1470357: control-CM, experiment3; Homo sapiens; miRNA-Seq SRR1542718
>
> GSM1470358: ET1-CM, experiment3; Homo sapiens; miRNA-Seq SRR1542719
>
> 可以看到是6个样本的测序数据，分成两组，就是ET1刺激了CM细胞系前后对比而已！

同时，我们也拿到了这6个样本的表达矩阵，计量单位是counts的reads数，所以我们一般会选用DESeq2，edgeR这样的常用包来做差异分析，当然，做差异分析的工具还有十几个，我这里只是拿一根最顺手的举例子，就是**DESeq2**。

下面的代码有点长，因为我在bioconductor系列教程里面多次提到了DESeq2使用方法，这里就只贴出代码，反正我要说的重点是，我们通过差异分析得到了差异miRNA列表

```
### step8: differential expression analysis by R package for miRNA expression patterns:
## 文章里面提到的结果是：
MicroRNA sequencing revealed over 250 known and 34 predicted novel miRNAs to be differentially expressed between ET-1 stimulated and unstimulated control hiPSC-CMs.
## (FDR < 0.1 and 1.5 fold change)
rm(list=ls())
setwd('J:\\miRNA_test\\paper_results')  ##把从GEO里面下载的文献结果放在这里
sampleIDs=c()
groupList=c()
allFiles=list.files(pattern = '.txt')
i=allFiles[1]
sampleID=strsplit(i,"_")[[1]][1]
treat=strsplit(i,"_")[[1]][4]
dat=read.table(i,stringsAsFactors = F)
colnames(dat)=c('miRNA',sampleID)
groupList=c(groupList,treat)
for (i in allFiles[-1]){
sampleID=strsplit(i,"_")[[1]][1]
treat=strsplit(i,"_")[[1]][4]
a=read.table(i,stringsAsFactors = F)
colnames(a)=c('miRNA',sampleID)
dat=merge(dat,a,by='miRNA')
groupList=c(groupList,treat)
}
### 上面的代码只是为了把6个独立的表达文件给合并成一个表达矩阵
## we need to filter the low expression level miRNA
exprSet=dat[,-1]
rownames(exprSet)=dat[,1]
suppressMessages(library(DESeq2))
exprSet=ceiling(exprSet)
(colData <- data.frame(row.names=colnames(exprSet), groupList=groupList))
## DESeq2就是这么简单的用
dds <- DESeqDataSetFromMatrix(countData = exprSet,
colData = colData,
design = ~ groupList)
dds <- DESeq(dds)
png("qc_dispersions.png", 1000, 1000, pointsize=20)
plotDispEsts(dds, main="Dispersion plot")
dev.off()
res <- results(dds)
## 画一些图，相当于做QC吧
png("RAWvsNORM.png")
rld <- rlogTransformation(dds)
exprSet_new=assay(rld)
par(cex = 0.7)
n.sample=ncol(exprSet)
if(n.sample>40) par(cex = 0.5)
cols <- rainbow(n.sample*1.2)
par(mfrow=c(2,2))
boxplot(exprSet,  col = cols,main="expression value",las=2)
boxplot(exprSet_new, col = cols,main="expression value",las=2)
hist(exprSet[,1])
hist(exprSet_new[,1])
dev.off()library(RColorBrewer)
(mycols <- brewer.pal(8, "Dark2")[1:length(unique(groupList))])
# Sample distance heatmap
sampleDists <- as.matrix(dist(t(exprSet_new)))
#install.packages("gplots",repos = "http://cran.us.r-project.org")
library(gplots)
png("qc-heatmap-samples.png", w=1000, h=1000, pointsize=20)
heatmap.2(as.matrix(sampleDists), key=F, trace="none",
col=colorpanel(100, "black", "white"),
ColSideColors=mycols[groupList], RowSideColors=mycols[groupList],
margin=c(10, 10), main="Sample Distance Matrix")
dev.off()

png("MA.png")
DESeq2::plotMA(res, main="DESeq2", ylim=c(-2,2))
dev.off()
## 重点就是这里啦，得到了差异分析的结果
resOrdered <- res[order(res$padj),]
resOrdered=as.data.frame(resOrdered)
write.csv(resOrdered,"deseq2.results.csv",quote = F)

##下面也是一些图，主要是看看样本之间的差异情况
library(limma)
plotMDS(log(counts(dds, normalized=TRUE) + 1))
plotMDS(log(counts(dds, normalized=TRUE) + 1) - log(t( t(assays(dds)[["mu"]]) / sizeFactors(dds) ) + 1))
plotMDS( assays(dds)[["counts"]] )  ## raw count
plotMDS( assays(dds)[["mu"]] ) ##- fitted values.
```

最后我们得到的差异分析结果：**deseq2.results.csv**，就可以跟进FDR和fold change来挑选符合要求的差异miRNA。

---

##### 第七讲：miRNA样本配对mRNA表达量获取 {-}

这一讲其实算不上自学miRNA-seq分析，本质是affymetrix的mRNA表达芯片数据分析，而且还是最常用的那种GPL570   HG-U133_Plus_2，但因为是跟miRNA样本配对检测而且后面会利用到这两个数据分析结果来做共表达网络分析等等，所以就贴出对该芯片数据的分析结果。

文章里面也提到了 Messenger RNA expression analysis identified 731 probe sets with significant differential expression，作者挑选的差异分析结果的显著基因列表如下 http://journals.plos.org/plosone/article/asset?unique&id=info:doi/10.1371/journal.pone.0108051.s002
mRNA expression array - GSE60291  (Affymetrix Human Genome U133 Plus 2.0 Array)

hgu133plus2 芯片数据很常见，可以从GEO里面下载该study的原始测序数据，然后用**affy,limma**包来分析，也可以直接用**GEOquery**包来下载作者分析好的表达矩阵，然后直接做差异分析。我这里选择的是后者，而且我跟作者分析方法有一点区别是，我先把探针都注释好了基因，然后只挑最大表达量的基因。而作者是直接对探针为单位的的表达矩阵进行差异分析，对分析结果里面的探针进行基因注释。我这里无法给出哪种方法好的绝对评价。代码如下

```
m(list=ls())
library(GEOquery)
library(limma)
GSE60291 <- getGEO('GSE60291', destdir=".",getGPL = F)

#下面是表达矩阵
exprSet=exprs(GSE60291[[1]])
library("annotate")
GSE60291[[1]]
## 下面是分组信息
pdata=pData(GSE60291[[1]])
treatment=factor(unlist(lapply(pdata$title,function(x) strsplit(as.character(x),"-")[[1]][1])))
#treatment=relevel(treatment,'control')
## 下面做基因注释
platformDB='hgu133plus2.db'
library(platformDB, character.only=TRUE)
probeset <- featureNames(GSE60291[[1]])
#EGID <- as.numeric(lookUp(probeset, platformDB, "ENTREZID"))
SYMBOL <-  lookUp(probeset, platformDB, "SYMBOL")
## 下面对每个基因挑选最大表达量探针
a=cbind(SYMBOL,exprSet)
## remove the duplicated probeset
rmDupID <-function(a=matrix(c(1,1:5,2,2:6,2,3:7),ncol=6)){
exprSet=a[,-1]
rowMeans=apply(exprSet,1,function(x) mean(as.numeric(x),na.rm=T))
a=a[order(rowMeans,decreasing=T),]
exprSet=a[!duplicated(a[,1]),]
#
exprSet=exprSet[!is.na(exprSet[,1]),]
rownames(exprSet)=exprSet[,1]
exprSet=exprSet[,-1]
return(exprSet)
}
exprSet=rmDupID(a)
rn=rownames(exprSet)
exprSet=apply(exprSet,2,as.numeric)
rownames(exprSet)=rn
exprSet[1:4,1:4]
#exprSet=log(exprSet) ## based on e
boxplot(exprSet,las=2)
## 下面用limma包来进行芯片数据差异分析
design=model.matrix(~ treatment)
fit=lmFit(exprSet,design)
fit=eBayes(fit)
#vennDiagram(decideTests(fit))
DEG=topTable(fit,coef=2,n=Inf,adjust='BH')
dim(DEG[abs(DEG[,1])>1.2 & DEG[,5]<0.05,])  ## 806 genes
write.csv(DEG,"ET1-normal.DEG.csv")
```

得到的**ET1-normal.DEG.csv** 文件就是我们的差异分析结果，可以跟文章提供的差异结果做比较，几乎一模一样。

如果根据logFC:1.2 和pValue:0.05来挑选，可以拿到806个基因。

---

##### 第八讲：miRNA-mRNA表达相关下游分析 {-}

通过前面的分析，我们已经量化了ET1刺激前后的细胞的miRNA和mRNA表达水平，也通过成熟的统计学分析分别得到了差异miRNA和mRNA，这时候我们就需要换一个参考文献了，因为前面提到的那篇文章分析的不够细致，我这里选择了浙江大学的一篇TCGA数据挖掘分析文章：

[Identifying miRNA/mRNA negative regulation pairs in colorectal cancer](http://www.nature.com/articles/srep12995%20)

里面首先查找miRNA-mRNA基因对，因为miRNA主要还是负向调控mRNA表达，所以根据我们得到的两个表达矩阵做相关性分析，很容易得到符合统计学意义的miRNA-mRNA基因对，具体分析内容如下

- 把得到的差异miRNA的表达量画一个热图，看看它是否能显著的分类
- 用miRWalk2.0等数据库或者根据来获取这些差异miRNA的validated target genes
- 然后看看这些**pairs of miRNA- target genes的表达量相关系数**，选取显著正相关或者负相关的pairs
- 这些被选取的pairs of miRNA- target genes拿去做**富集分析**
- 最后这些pairs of miRNA- target genes做**PPI网络分析**

首先我们看第一个热图的实现

```
resOrdered=na.omit(resOrdered)
DEmiRNA=resOrdered[abs(resOrdered$log2FoldChange)>log2(1.5) & resOrdered$padj <0.01 ,]
write.csv(resOrdered,"deseq2.results.csv",quote = F)
DEmiRNAexprSet=exprSet[rownames(DEmiRNA),]
write.csv(DEmiRNAexprSet,'DEmiRNAexprSet.csv')

DEmiRNAexprSet=read.csv('DEmiRNAexprSet.csv',stringsAsFactors = F)
exprSet=as.matrix(DEmiRNAexprSet[,2:7])
rownames(exprSet)=rownames(DEmiRNAexprSet)
heatmap(exprSet)
gplots::heatmap.2(exprSet)
library(pheatmap)
## http://biit.cs.ut.ee/clustvis/
```

因为我前面保存的表达量就基于counts的，所以画热图还需要进行normalization，我这里懒得弄了，就用了一个网页版工具可以自动生成热图：http://biit.cs.ut.ee/clustvis/

![miRNA-heatmap](image/C6/miRNA-heatmap.png)



感觉还不错，可以很清楚的看到ET1刺激前后细胞中miRNA表达量变化

然后就是检验我们感兴趣的有显著差异的miRNA的target genes，这时候有两种方法：一个是先由数据库得到已经被检验的miRNA的target genes；另一种是根据miRNA和mRNA表达量的相关性来预测。

用数据库来查找MiRNA的作用基因，非常多的工具，比较常用的有**TargetScan/miRTarBase** 

- http://nar.oxfordjournals.org/content/early/2015/11/19/nar.gkv1258.full
- http://mirtarbase.mbc.nctu.edu.tw/
- http://mirtarbase.mbc.nctu.edu.tw/cache/download/6.1/hsa_MTI.xlsx
- http://www.targetscan.org/vert_71/ (version 7.1 (June 2016))

我还看到过一个整合工具： **miRecords ** (DIANA-microT, MicroInspector, miRanda, MirTarget2, miTarget, NBmiRTar, PicTar, PITA, RNA22, RNAhybrid and TargetScan/TargertScanS)里面提到了查找miRNA的作用基因这一过程，高假阳性，至少被5种工具支持，才算是真的。

还有很多类似的工具，**miRWalk2，psRNATarget**网页版工具。

最后值得一提的是中山大学的：[ starBase  ](http://starbase.sysu.edu.cn/panCancer.php)

>  Pan-Cancer Analysis Platform is designed for deciphering Pan-Cancer Networks of lncRNAs, miRNAs, ceRNAs and RNA-binding proteins (RBPs) by mining clinical and expression profiles of 14 cancer types (>6000 samples) from The Cancer Genome Atlas (TCGA) Data Portal (all data available without limitations).

虽然我没有仔细的用，但是看介绍好牛的样子。

还有一个R包：**miRLAB**，它是先通过算所有配对的**miRNA- genes的表达量相关系数**，选取显著正相关或者负相关的pairs，然后反过来通过已知数据库来验证。

后面我就不讲了，主要看你得到miRNA的时候其它生物学数据是否充分，如果是癌症病人，有生存相关数据，可以做生存分析，如果你同时测了甲基化数据，可以做甲基化相关分析。

如果只是单纯的miRNA测序数据，可以回过头去研究一下de novo的miRNA预测的步骤，也是研究重点。


### 表观组

几十年来，DNA一直被认为是决定生命遗传信息的核心物质，但是近些年新的研究表明，生命遗传信息从来就不是基因所能完全决定的，比如科学家们发现，可以在不影响DNA序列的情况下改变基因组的修饰，这种改变不仅可以影响个体的发育，而且还可以遗传下去。这种在基因组的水平上研究表观遗传修饰的领域被称为“表观基因组学(epigenomics）”。表观基因组学使人们对基因组的认识又增加了一个新视点：对基因组而言，不仅仅是序列包含遗传信息，而且其修饰也可以记载遗传信息。

#### 自学ChIP-seq分析九讲

##### 第一讲：文献选择与解读 {-}


**文献；CARM1 Methylates Chromatin Remodeling Factor BAF155 to Enhance Tumor Progression and Metastasis** 

我很早以前想自学CHIP-seq的时候关注过这篇文章，那时候懂得还不多，甚至都没有仔细看这篇文章就随便下载了数据进行分析，也只是跑一些软件而已。这次仔细阅读这篇文章才发现里面门道很多，尤其是ChIP-seq的实验基础和表观遗传学的生物学基础知识。

作者首先实验证明了用small haripin RNA来knockout CARM1 只能达到90%的敲除效果，有趣的是，对CARM1的功能影响非常小，说明只需要极少量的CARM1就可以发挥很好的作用，因此作者通过zinc finger nuclease这种基因组编辑技术设计了100%敲除CARM1的实验材料，

这样就能比较CARM1有无时各种蛋白被催化状态了，其中SWI/SNF(BAF) chromatin remodeling complex  染色质重构复合物的一个亚基 BAF155，非常明显的只有在CARM1这个基因完好无损的细胞系里面才能被正常的甲基化。作者证明了BAF155是CARM1这个基因非常好的一个底物， 而且通过巧妙的实验设计，证明了BAF155这个蛋白的第1064位氨基酸(R) 是 CARM1的作用位点。

因为早就有各种文献说明了SWI/SNF(BAF) chromatin remodeling complex  染色质重构复合物在癌症的重要作用， 所以作者也很自然想探究BAF155在癌症的功能详情，这里作者选择的是**ChIP-seq**技术。BAF155是一种转录因子(transcription factor)（能与基因5端上游特定序列专一性结合，从而保证目的基因以特定的强度在特定的时间与空间表达的蛋白质分子）。ChIP-seq技术最适合来探究BAF155这样转录因子的功能，所以作者构造了一种细胞系（MCF7），它的BAF155蛋白的第1064位氨基酸(R) 突变而无法被CARM1这个基因催化而甲基化，然后比较突变的细胞系和野生型细胞系的BAF155的ChIP-seq结果，这样就可以研究BAF155这个转录因子，是否必须要被CARM1这个基因催化而甲基化后才能行使生物学功能。

作者用me-BAF155特异性抗体+western bloting 证明了正常的野生型MCF7细胞系里面有~74%的BAF155被甲基化。

有一个细胞系SKOV3，可以正常表达除了BAF155之外的其余14种SWI/SNF(BAF) chromatin remodeling complex  染色质重构复合物，而不管是把突变的细胞系和野生型细胞系的BAF155混在里面都可以促进染色质重构复合物的组装，所以甲基化与否并不影响这个染色质重构复合物的组装，重点应该研究的是甲基化会影响BAF155在基因组其它地方结合。

结果显示，突变的细胞系和野生型细胞系种BAF155在基因组结合位置(peaks)还是有较大的overlap的，重点是看它们的peaks在各种基因组区域(基因上下游，5,3端UTR，启动子，内含子，外显子，基因间区域，microRNA区域)分布情况的差别，还有它们距离转录起始位点的距离的分布区别，还有它们注释到的基因区别，已经基因富集到什么通路等等。

虽然作者在人的细胞系(MCF7)上面做ChIP-seq，但是在老鼠细胞系(MDA-MB-231)做了mRNA芯片数据分析，BAF155这个蛋白的第1064位氨基酸(R) 突变细胞系和野生型细胞系，用的是Affymetrix HG U133 Plus 2.0这个常用平台。

which was hybridized to Affymetrix HG U133 Plus 2.0 microarrays containing 54,675 probesets for >47,000 transcripts and variants, including 38,500 human genes.

To identify genes differentially expressed between MDA-MB-231-BAF155WT and MDA-MB-231-BAF155R1064K

表达矩阵下载地址：[http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4004525/bin/NIHMS556863-supplement-03.xlsx](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4004525/bin/NIHMS556863-supplement-03.xlsx)

我简单摘抄作者ChIP-seq数据的生物信息学分析结果

- All samples were mapped from fastq files using BOWTIE [-m 1 -- best] to mm9 [UCSCmouse genome build 9]

- Sequences were mapped to the human genome (hg19) using BOWTIE (--best –m 1) to yield unique alignments

- Peaks were called by using HOMER [[http://biowhat.ucsd.edu/homer/](http://biowhat.ucsd.edu/homer/)] and QuEST [[http://mendel.stanford.edu/sidowlab/downloads/quest/](http://mendel.stanford.edu/sidowlab/downloads/quest/)].

**用到的软件有**

- **QuEST 2.4 **(Valouev et al., 2008) was run using the recommend settings for transcription factor (TF) like binding with the following exceptions:

  kde_bandwith=30, region_size=600, ChIP threshold=35, enrichment fold=3, rescue fold=3.

- **HOMER **(Heinz et al., 2010) analysis was run using the default settings for peak finding.

  False Discovery Rate (FDR) cut off was **0.001 (0.1%) for all peaks. **

  The tag density for each factor was **normalized to 1x107 tags** and displayed using the UCSC genome browser.

- **Motif analysis** (de novo and known), was performed using the** HOMER software and Genomatix. **

- **Peak overlaps **were processed with HOMER and Galaxy (Giardine et al., 2005).

- **Peak comparisons **between replicates were processed with EdgeR statistical package in R

以上就是我们接下来需要学习的流程化分析步骤，下面我给一个主要流程的截图，但主要是实验是如何设计

这里有一个文章发表了关于CHIP-seq的流程的：[http://biow.sb-roscoff.fr/ecole_bioinfo/protected/jacques.van-helden/ThomasChollier_NatProtoc_2012_peak-motifs.pdf](http://biow.sb-roscoff.fr/ecole_bioinfo/protected/jacques.van-helden/ThomasChollier_NatProtoc_2012_peak-motifs.pdf)

 

**同时我还推荐大家看几篇相关文献**

- Genome-wide maps of chromatin state in pluripotent and lineage-committed cells. [http://www.nature.com/nature/journal/v448/n7153/pdf/nature06008.pdf](http://www.nature.com/nature/journal/v448/n7153/pdf/nature06008.pdf)
- Mapping and analysis of chromatin state dynamics in nine human cell types(GSE26386): [http://www.nature.com/nature/journal/v473/n7345/full/nature09906.html](http://www.nature.com/nature/journal/v473/n7345/full/nature09906.html)
- Promiscuous RNA binding by Polycomb Repressive Complex 2 [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3823624/pdf/nihms517229.pdf](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3823624/pdf/nihms517229.pdf)

---

##### 第二讲：资料收集 {-}



CHIP-seq的确是非常完善的NGS流程，各种资料层出不穷。

大家首先可以看下面几个完整流程的PPT来对CHIP-seq流程有个大致的印象，我对前面提到的文献数据处理的几个要点，就跟下面这个图片类似。



![chip-seq-workflow-all-5-steps](image/C6/chip-seq-workflow-all-5-steps.jpg)



- QuEST is a statistical software for analysis of ChIP-Seq data with data and analysis results visualization through UCSC Genome Browser.  [http://www-hsc.usc.edu/](http://www-hsc.usc.edu/)~valouev/QuEST/QuEST.html

- peak calling 阈值的选择： [http://www.nature.com/nprot/journal/v7/n1/fig_tab/nprot.2011.420_F2.html](http://www.nature.com/nprot/journal/v7/n1/fig_tab/nprot.2011.420_F2.html)

- MeDIP-seq and histone modification ChIP-seq analysis  [http://crazyhottommy.blogspot.com/2014/01/medip-seq-and-histone-modification-chip.html](http://crazyhottommy.blogspot.com/2014/01/medip-seq-and-histone-modification-chip.html)

- 2011-review-CHIP-seq-high-quaility-data: [http://www.nature.com/ni/journal/v12/n10/full/ni.2117.html?message-global=remove](http://www.nature.com/ni/journal/v12/n10/full/ni.2117.html?message-global=remove)

- 不同处理条件的CHIP-seq的差异peaks分析： [http://www.slideshare.net/thefacultyl/diffreps-automated-chipseq-differential-analysis-package](http://www.slideshare.net/thefacultyl/diffreps-automated-chipseq-differential-analysis-package)

- 一个实际的CHIP-seq数据分析例子： [http://www.biologie.ens.fr/](http://www.biologie.ens.fr/)~mthomas/other/chip-seq-training/

- [http://biow.sb-roscoff.fr/ecole_bioinfo/training_material/chip-seq/documents/presentation_chipseq.pdf](http://biow.sb-roscoff.fr/ecole_bioinfo/training_material/chip-seq/documents/presentation_chipseq.pdf)

- [http://ecole-bioinfo-aviesan.sb-roscoff.fr/sites/ecole-bioinfo-aviesan.sb-roscoff.fr/files/files/chipseq_CarlHerrmann_Roscoff2015.pdf](http://ecole-bioinfo-aviesan.sb-roscoff.fr/sites/ecole-bioinfo-aviesan.sb-roscoff.fr/files/files/chipseq_CarlHerrmann_Roscoff2015.pdf)

- [http://ecole-bioinfo-aviesan.sb-roscoff.fr/sites/ecole-bioinfo-aviesan.sb-roscoff.fr/files/files/defrance-ChIP-seq_annotation.pdf](http://ecole-bioinfo-aviesan.sb-roscoff.fr/sites/ecole-bioinfo-aviesan.sb-roscoff.fr/files/files/defrance-ChIP-seq_annotation.pdf)

然后下面的各种资料，是针对CHIP-seq流程的各个环境的，还有一些是针对于表观遗传学知识

- ppt : [http://159.149.160.51/epigen_milano/epigen_barozzi.pdf](http://159.149.160.51/epigen_milano/epigen_barozzi.pdf)

- best practise: [http://bioinformatics-core-shared-training.github.io/cruk-bioinf-sschool/](http://bioinformatics-core-shared-training.github.io/cruk-bioinf-sschool/)

- pipeline : [https://github.com/shenlab-sinai/chip-seq_preprocess](https://github.com/shenlab-sinai/chip-seq_preprocess)

- [https://sites.google.com/site/anshul...e/projects/idr](https://sites.google.com/site/anshul...e/projects/idr)  ## samtools view -b -F 1548 -q 30 chipSampleRep1.bam

- pipeline : [http://daudin.icmb.utexas.edu/wiki/index.php/ChIPseq_prep_and_map](http://daudin.icmb.utexas.edu/wiki/index.php/ChIPseq_prep_and_map)

- pipeline : [https://github.com/BradyLab/ChipSeq/blob/master/chipseq.sh](https://github.com/BradyLab/ChipSeq/blob/master/chipseq.sh)

- [https://github.com/crukci-bioinformatics/chipseq-pipeline](https://github.com/crukci-bioinformatics/chipseq-pipeline)

- [https://github.com/ENCODE-DCC/chip-seq-pipeline](https://github.com/ENCODE-DCC/chip-seq-pipeline)

- Hands-on introduction to ChIP-seq analysis - VIB Training   [http://www.biologie.ens.fr/](http://www.biologie.ens.fr/)~mthomas/other/chip-seq-training/

- video(A Step-by-Step Guide to ChIP-Seq Data Analysis Webinar) : [http://www.abcam.com/webinars/a-step-by-step-guide-to-chip-seq-data-analysis-webinar](http://www.abcam.com/webinars/a-step-by-step-guide-to-chip-seq-data-analysis-webinar)

- Using ChIP-Seq to identify and/or quantify bound regions (peaks)  [http://barcwiki.wi.mit.edu/wiki/SOPs/chip_seq_peaks](http://barcwiki.wi.mit.edu/wiki/SOPs/chip_seq_peaks)

- [http://jura.wi.mit.edu/bio/education/hot_topics/ChIPseq/ChIPSeq_HotTopics.pdf](http://jura.wi.mit.edu/bio/education/hot_topics/ChIPseq/ChIPSeq_HotTopics.pdf)

- [http://pedagogix-tagc.univ-mrs.fr/courses/ASG1/practicals/chip-seq/mapping_tutorial.html](http://pedagogix-tagc.univ-mrs.fr/courses/ASG1/practicals/chip-seq/mapping_tutorial.html)

- 公开课： [https://www.coursera.org/learn/galaxy-project/lecture/FUzcg/chip-sequence-analysis-with-macs](https://www.coursera.org/learn/galaxy-project/lecture/FUzcg/chip-sequence-analysis-with-macs)

- EBI的教程：[https://www.ebi.ac.uk/training/online/course/ebi-next-generation-sequencing-practical-course/chip-seq-analysis/chip-seq-practical](https://www.ebi.ac.uk/training/online/course/ebi-next-generation-sequencing-practical-course/chip-seq-analysis/chip-seq-practical)


- 台湾教程：[http://lsl.sinica.edu.tw/Services/Class/files/20151118475_2.pdf](http://lsl.sinica.edu.tw/Services/Class/files/20151118475_2.pdf) 徐唯哲 Paul Wei-Che HSU

- peak finder软件大全：
[http://wodaklab.org/nextgen/data/peakfinders.html](http://wodaklab.org/nextgen/data/peakfinders.html)
 
- [https://bioshare.bioinformatics.ucdavis.edu/bioshare/download/47aq5pp5mzza5vb/PDFs/Tuesday_MB_ChIP-Seq_Intro.pdf](https://bioshare.bioinformatics.ucdavis.edu/bioshare/download/47aq5pp5mzza5vb/PDFs/Tuesday_MB_ChIP-Seq_Intro.pdf)

- paper： Large-Scale Quality Analysis of Published ChIP-seq Data [http://www.g3journal.org/content/4/2/209.full](http://www.g3journal.org/content/4/2/209.full)

- paper： Chip-seq data analysis: from quality check to motif discovery and more [http://ccg.vital-it.ch/var/sib_april15/cases/landt12/strand_correlation.html](http://ccg.vital-it.ch/var/sib_april15/cases/landt12/strand_correlation.html)

- Workshop hands on session(RNA-Seq / ChIP-Seq  ) :  [https://hpc.oit.uci.edu/biolinux/handson.docx](https://hpc.oit.uci.edu/biolinux/handson.docx)

- [http://www.gqinnovationcenter.com/documents/bioinformatics/ChIPseq.pptx](http://www.gqinnovationcenter.com/documents/bioinformatics/ChIPseq.pptx)

- paper supplement : [http://genome.cshlp.org/content/suppl/2015/10/02/gr.192005.115.DC1/Supplemental_Information.docx](http://genome.cshlp.org/content/suppl/2015/10/02/gr.192005.115.DC1/Supplemental_Information.docx)

- [http://www.illumina.com/documents/products/datasheets/datasheet_chip_sequence.pdf](http://www.illumina.com/documents/products/datasheets/datasheet_chip_sequence.pdf)

- [http://www.ncbi.nlm.nih.gov/pubmed/22130887](http://www.ncbi.nlm.nih.gov/pubmed/22130887) "Analyzing ChIP-seq data: preprocessing, normalization, differential identification, and binding pattern characterization."

- [http://www.ncbi.nlm.nih.gov/pubmed/22499706](http://www.ncbi.nlm.nih.gov/pubmed/22499706) "Normalization, bias correction, and peak calling for ChIP-seq." (stat heavy)

- [http://www.ncbi.nlm.nih.gov/pubmed/24244136](http://www.ncbi.nlm.nih.gov/pubmed/24244136) "Practical guidelines for the comprehensive analysis of ChIP-seq data."

- [http://www.ncbi.nlm.nih.gov/pubmed/25223782](http://www.ncbi.nlm.nih.gov/pubmed/25223782) "Identifying and mitigating bias in next-generation sequencing methods for chromatin biology."

- [http://www.ncbi.nlm.nih.gov/pubmed/24598259](http://www.ncbi.nlm.nih.gov/pubmed/24598259) "Impact of sequencing depth in ChIP-seq experiments."

- figures: [https://github.com/shenlab-sinai/ngsplot](https://github.com/shenlab-sinai/ngsplot)

可视化工具

- [https://github.com/daler/metaseq](https://github.com/daler/metaseq)

- [http://liulab.dfci.harvard.edu/CEAS/usermanual.html](http://liulab.dfci.harvard.edu/CEAS/usermanual.html)

bioconductor系列工具和教程 :

- [http://faculty.ucr.edu/](http://faculty.ucr.edu/)~tgirke/HTML_Presentations/Manuals/Workshop_Dec_6_10_2012/Rchipseq/Rchipseq.pdf

- [http://bioinformatics-core-shared-training.github.io/cruk-bioinf-sschool/Day4/chipqc_sweave.pdf](http://bioinformatics-core-shared-training.github.io/cruk-bioinf-sschool/Day4/chipqc_sweave.pdf)

- [http://bioconductor.org/packages/release/bioc/html/chipseq.html](http://bioconductor.org/packages/release/bioc/html/chipseq.html)

- [http://bioconductor.org/help/workflows/chipseqDB/](http://bioconductor.org/help/workflows/chipseqDB/)

- [http://bioconductor.org/help/workflows/generegulation/](http://bioconductor.org/help/workflows/generegulation/)

- [http://bioconductor.org/help/course-materials/2009/EMBLJune09/Practicals/chipseq/BasicChipSeq.pdf](http://bioconductor.org/help/course-materials/2009/EMBLJune09/Practicals/chipseq/BasicChipSeq.pdf)

公司教程

- [http://www.partek.com/Tutorials/microarray/Tiling/ChipSeqTutorial.pdf](http://www.partek.com/Tutorials/microarray/Tiling/ChipSeqTutorial.pdf)

---

##### 第三讲：公共数据下载 {-}

这一步跟自学其它高通量测序数据处理一样，就是仔细研读paper，在里面找到作者把原始测序数据放在了哪个公共数据库里面，一般是NCBI的GEO，SRA，本文也不例外，然后解析样本数，找到下载链接规律。

```
## step1 : download raw data
> cd ~
> mkdir CHIPseq_test && cd CHIPseq_test
> mkdir rawData && cd rawData
> ## batch download the raw data by shell script :
> for ((i=593;i<601;i++)) ;do wget [ftp://ftp-trace.ncbi.nlm.nih.gov/sra/sra-instant/reads/ByStudy/sra/SRP/SRP033/SRP033492/SRR1042](ftp://ftp-trace.ncbi.nlm.nih.gov/sra/sra-instant/reads/ByStudy/sra/SRP/SRP033/SRP033492/SRR1042)$i/SRR1042$i.sra;done
```

很容易就下载了8个测序文件，每个样本的数据大小，测序量如下

```
> 621M Jun 27 14:03 SRR1042593.sra (16.9M reads)
> 2.2G Jun 27 15:58 SRR1042594.sra (60.6M reads)
> 541M Jun 27 16:26 SRR1042595.sra (14.6M reads)
> 2.4G Jun 27 18:24 SRR1042596.sra (65.9M reads)
> 814M Jun 27 18:59 SRR1042597.sra (22.2M reads)
> 2.1G Jun 27 20:30 SRR1042598.sra (58.1M reads)
> 883M Jun 27 21:08 SRR1042599.sra (24.0M reads)
> 2.8G Jun 28 11:53 SRR1042600.sra (76.4M reads)
```

虽然下载的SRA格式数据也是一个很流行的标准，但它只是数据压缩的标准，几乎没有软件能直接跟SRA的格式的测序数据来进行分析，我们需要转成fastq格式，代码如下：

```
> ## step2 :  change sra data to fastq files.
> ## cell line: MCF7 //  Illumina HiSeq 2000 //  50bp // Single ends // phred+33
> ## [http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE52964](http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE52964)
> ## [ftp://ftp-trace.ncbi.nlm.nih.gov/sra/sra-instant/reads/ByStudy/sra/SRP/SRP033/SRP033492](ftp://ftp-trace.ncbi.nlm.nih.gov/sra/sra-instant/reads/ByStudy/sra/SRP/SRP033/SRP033492)
> ls *sra |while read id; do ~/biosoft/sratoolkit/sratoolkit.2.6.3-centos_linux64/bin/fastq-dump $id;done
> rm *sra
```
解压的详情如下，可以看到SRA格式有6~9倍的压缩了，比zip格式压缩的2~3倍高多了

```
##  621M --> 3.9G
##  2.2G --> 14G
##  541M --> 3.3G
##  2.4G --> 15G
```
---

##### 第四讲：必要软件安装及结果下载 {-}

博文的顺序有点乱，因为怕读到前面的公共测序数据下载这篇文章的朋友搞不清楚，我如何调用各种软件的，所以我这里强势插入一篇博客来描述这件事，当然也只是略过，我所有的软件理论上都是安装在我的home目录下的biosoft文件夹，所以你看到我一般安装程序都是:

```
cd ~/biosoft
mkdir macs2 && cd macs2 ##指定的软件安装在指定文件夹里面
```


这只是我个人的安装习惯，因为我不是root，所以不能在linux系统下做太多事，我这里贴出我所有的软件安装代码：

```
## pre-step: download sratoolkit /fastx_toolkit_0.0.13/fastqc/bowtie2/bwa/MACS2/HOMER/QuEST/mm9/hg19/bedtools
## http://www.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=software
## http://www.ncbi.nlm.nih.gov/books/NBK158900/

## Download and install sratoolkit
cd ~/biosoft
mkdir sratoolkit && cd sratoolkit
wget http://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/2.6.3/sratoolkit.2.6.3-centos_linux64.tar.gz
##
## Length: 63453761 (61M) [application/x-gzip]
## Saving to: "sratoolkit.2.6.3-centos_linux64.tar.gz"
tar zxvf sratoolkit.2.6.3-centos_linux64.tar.gz

## Download and install bedtools
cd ~/biosoft
mkdir bedtools && cd bedtools
wget https://github.com/arq5x/bedtools2/releases/download/v2.25.0/bedtools-2.25.0.tar.gz
## Length: 19581105 (19M) [application/octet-stream]
tar -zxvf bedtools-2.25.0.tar.gz
cd bedtools2
make

## Download and install PeakRanger
cd ~/biosoft
mkdir PeakRanger && cd PeakRanger
wget https://sourceforge.net/projects/ranger/files/PeakRanger-1.18-Linux-x86_64.zip/
## Length: 1517587 (1.4M) [application/octet-stream]
unzip PeakRanger-1.18-Linux-x86_64.zip
~/biosoft/PeakRanger/bin/peakranger -h

## Download and install bowtie
cd ~/biosoft
mkdir bowtie && cd bowtie
wget https://sourceforge.net/projects/bowtie-bio/files/bowtie2/2.2.9/bowtie2-2.2.9-linux-x86_64.zip/download
#Length: 27073243 (26M) [application/octet-stream]
#Saving to: "download" ## I made a mistake here for downloading the bowtie2
mv download bowtie2-2.2.9-linux-x86_64.zip
unzip bowtie2-2.2.9-linux-x86_64.zip

mkdir -p ~/biosoft/bowtie/hg19_index
cd ~/biosoft/bowtie/hg19_index

# download hg19 chromosome fasta files
wget http://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/chromFa.tar.gz
# unzip and concatenate chromosome and contig fasta files
tar zvfx chromFa.tar.gz
cat *.fa > hg19.fa
rm chr*.fa
## ~/biosoft/bowtie/bowtie2-2.2.9/bowtie2-build ~/biosoft/bowtie/hg19_index/hg19.fa ~/biosoft/bowtie/hg19_index/hg19
## Download and install BWA
cd ~/biosoft
mkdir bwa && cd bwa

http://sourceforge.net/projects/bio-bwa/files/

tar xvfj bwa-0.7.12.tar.bz2 # x extracts, v is verbose (details of what it is doing), f skips prompting for each individual file, and j tells it to unzip .bz2 files
cd bwa-0.7.12
make
export PATH=$PATH:/path/to/bwa-0.7.12 # Add bwa to your PATH by editing ~/.bashrc file (or .bash_profile or .profile file)
# /path/to/ is an placeholder. Replace with real path to BWA on your machine
source ~/.bashrc
# bwa index [-a bwtsw|is] index_prefix reference.fasta
bwa index -p hg19bwaidx -a bwtsw ~/biosoft/bowtie/hg19_index/hg19.fa
# -p index name (change this to whatever you want)
# -a index algorithm (bwtsw for long genomes and is for short genomes)
## Download and install macs2
## // https://pypi.python.org/pypi/MACS2/
cd ~/biosoft
mkdir macs2 && cd macs2
wget ~~~~~~~~~~~~~~~~~~~~~~MACS2-2.1.1.20160309.tar.gz
tar zxvf MACS2-2.1.1.20160309.tar.gz
cd MACS2-2.1.1.20160309
python setup.py install --user

#################### The log for installing MACS2:
Creating ~/.local/lib/python2.7/site-packages/site.py
Processing MACS2-2.1.1.20160309-py2.7-linux-x86_64.egg
Copying MACS2-2.1.1.20160309-py2.7-linux-x86_64.egg to ~/.local/lib/python2.7/site-packages
Adding MACS2 2.1.1.20160309 to easy-install.pth file
Installing macs2 script to ~/.local/bin
Finished processing dependencies for MACS2==2.1.1.20160309
############################################################
~/.local/bin/macs2 --help

Example for regular peak calling:
macs2 callpeak -t ChIP.bam -c Control.bam -f BAM -g hs -n test -B -q 0.01
Example for broad peak calling:
macs2 callpeak -t ChIP.bam -c Control.bam --broad -g hs --broad-cutoff 0.1

## Download and install homer (Hypergeometric Optimization of Motif EnRichment)
## // http://homer.salk.edu/homer/
## // http://blog.qiubio.com:8080/archives/3024
## pre-install: Ghostscript，seqlogo,blat
cd ~/biosoft
mkdir homer && cd homer
wget http://homer.salk.edu/homer/configureHomer.pl
perl configureHomer.pl -install
perl configureHomer.pl -install hg19
```
一般来说，对我这样水平的人来说，软件安装就跟家常便饭一样，没有什么问题了，但如果你是初学者呢，肯定没那么轻松，所以请加强学习，我无法在这里讲解太具体的知识了。

所有软件安装完毕后就可以下载文章对这些ChIP-seq的处理结果了，这个很重要，检验我们是否重复了人家的数据分析过程。

```
## step3 : download the results from paper
## http://www.bio-info-trainee.com/1571.html
mkdir paper_results && cd paper_results
wget ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE52nnn/GSE52964/suppl/GSE52964_RAW.tar
tar xvf GSE52964_RAW.tar

ls *gz |xargs gunzip

## step4 : run FastQC to check the sequencing quality.

##这里可以看到我们下载的原始数据已经被作者处理好了，去了接头，去了低质量序列

ls *.fastq | while read id ; do ~/biosoft/fastqc/FastQC/fastqc $id;done
## Sequence length 51
## %GC 39
## Adapter Content passed

The quality of the reads is pretty good, we don't need to do any filter or trim

mkdir QC_results
mv *zip *html QC_results/
```

---

##### 第五讲：测序数据比对 {-}

比对本质是是很简单的了，各种mapping工具层出不穷，我们一般常用的就是BWA和bowtie了，我这里就挑选bowtie2吧，反正别人已经做好了各种工具效果差异的比较，我们直接用就好了，代码如下：

```
## step5 : alignment to hg19/ using bowtie2 to do alignment
## ~/biosoft/bowtie/bowtie2-2.2.9/bowtie2-build ~/biosoft/bowtie/hg19_index /hg19.fa ~/biosoft/bowtie/hg19_index/hg19
## cat >run_bowtie2.sh
ls *.fastq | while read id ;
do
echo $id
#~/biosoft/bowtie/bowtie2-2.2.9/bowtie2 -p 8 -x ~/biosoft/bowtie/hg19_index/hg19 -U $id -S ${id%%.*}.sam 2>${id%%.*}.align.log;
#samtools view -bhS -q 30 ${id%%.*}.sam > ${id%%.*}.bam ## -F 1548 https://broadinstitute.github.io/picard/explain-flags.html
# -F 0x4 remove the reads that didn't match
samtools sort ${id%%.*}.bam ${id%%.*}.sort ## prefix for the output
# samtools view -bhS a.sam | samtools sort -o - ./ > a.bam
samtools index ${id%%.*}.sorted.bam
done
```
这个索引~/biosoft/bowtie/hg19_index/hg19需要自己提取建立好，见前文

初步比对的sam文件到底该如何过滤，我查了很多文章都没有给出个子丑寅卯，各执一词，我也没办法给大家一个标准，反正我测试了好几种，看起来call peaks的差异不大，就是得不到文章给出的那些结果！！

一般来说，初步比对的sam文件只能选取unique mapping的结果，所以我用了#samtools view -bhS -q 30，但是结果并没什么改变，有人说是peak caller这些工具本身就会做这件事，所以取决于你下游分析所选择的工具。

给大家看比对的日志吧：

```
SRR1042593.fastq
16902907 reads; of these:
16902907 (100.00%) were unpaired; of these:
667998 (3.95%) aligned 0 times
12467095 (73.76%) aligned exactly 1 time
3767814 (22.29%) aligned >1 times
96.05% overall alignment rate
......
SRR1042598.fastq
58068816 reads; of these:
58068816 (100.00%) were unpaired; of these:
8433671 (14.52%) aligned 0 times
37527468 (64.63%) aligned exactly 1 time
12107677 (20.85%) aligned >1 times
85.48% overall alignment rate
[samopen] SAM header is present: 93 sequences.
SRR1042599.fastq
24019489 reads; of these:
24019489 (100.00%) were unpaired; of these:
1411095 (5.87%) aligned 0 times
17528479 (72.98%) aligned exactly 1 time
5079915 (21.15%) aligned >1 times
94.13% overall alignment rate
[samopen] SAM header is present: 93 sequences.
SRR1042600.fastq
76361026 reads; of these:
76361026 (100.00%) were unpaired; of these:
8442054 (11.06%) aligned 0 times
50918615 (66.68%) aligned exactly 1 time
17000357 (22.26%) aligned >1 times
88.94% overall alignment rate
[samopen] SAM header is present: 93 sequences.
```

可以看到比对非常成功。

我这里就不用表格的形式来展现了，毕竟我又不是给客户写报告，大家就将就着看吧。



---



##### 第六讲 寻找peaks {-}



ChIP-seq测序的本质还是目标片段捕获测序，但跟WES不同的是，你选择的IP不同，细胞或者机体状态不同，捕获到的序列差异很大。而我们研究的重点就是捕获到的差异。我们对ChIP-seq测序数据寻找peaks的本质就是得到所有测序数据比对在全基因组之后在基因组上测序深度里面寻找比较突出的部分。比如对WES数据来说，各个外显子，或者外显子的5端到3端，理论上测序深度应该是一致的，都是50X~200X，画一个测序深度曲线，应该是近似于一条直线。但对我们的ChIP-seq测序数据来说，在所捕获的区域上面，理论上测序深度是绝对不一样的，应该是近似于一个山峰。而那些覆盖度高的地方，山顶，就是我们的IP所结合的热点，也就是我们想要找的peaks，在IGV里面看到大致是下面这样：

![chipseq-0-peakdetection-large-IGV](image/C6/chipseq-0-peakdetection-large-IGV.png)

可以看到测序的reads是分布不均匀的，我们通常说的ChIP-seq测序的IP，可以是各个组蛋白上各修饰位点对应的抗体，或者是各种转录因子的抗体等等。

如何定义热点呢？通俗地讲，热点是这样一些位置，这些位置多次被测得的read所覆盖（我们测的是一个细胞群体，read出现次数多，说明该位置被TF结合的几率大）。那么，read数达到多少才叫多？这就要用到统计检验来分析。假设TF在基因组上的分布是没有任何规律的，那么，测序得到的read在基因组上的分布也必然是随机的，某个碱基上覆盖的read的数目应该服从二项分布。

具体统计学原理可以看这篇博客文章：[http://www.plob.org/2014/05/08/7227.html](http://www.plob.org/2014/05/08/7227.html)

为了达到作者文献里面的结果，我换了**8**个软件：**MACS2/HOMER/SICERpy/PePr/SWEMBL/SISSRs/BayesPeak/PeakRanger**

这里就不一一介绍peaks caller软件的安装以及使用了，因为MACS2是最常用的，所以简单贴一下我关于MACS2的学习代码：

```
## step6 : peak calling
### step6.1: with MACS2
## 我先看了看说明书：

macs2 callpeak -t TF_1.bam -c Input.bam -n mypeaks
We used the following options:
-t: This is the only required parameter for MACS, refers to the name of the file with the ChIP-seq data
-c: The control or mock data file
-n: The name string of the experiment
MAC2 creates 4 files (mypeaks peaks.narrowPeak, mypeaks summits.bed, mypeaks peaks.xls and mypeaks model.r)

# MACS首先的工作是要确定一个模型，这个模型最关键的参数就是峰宽d。这个d就是bw(band width)，而它的一半就是shiftsize。
### 然后根据文章确定了下载的测序数据的分类

GSM1278641 Xu_MUT_rep1_BAF155_MUT        SRR1042593
GSM1278642 Xu_MUT_rep1_Input        SRR1042594
GSM1278643 Xu_MUT_rep2_BAF155_MUT        SRR1042595
GSM1278644 Xu_MUT_rep2_Input        SRR1042596
GSM1278645 Xu_WT_rep1_BAF155        SRR1042597
GSM1278646 Xu_WT_rep1_Input        SRR1042598
GSM1278647 Xu_WT_rep2_BAF155        SRR1042599
GSM1278648 Xu_WT_rep2_Input         SRR1042600

## 这里有个很奇怪的问题，input的测序数据居然比IP的测序数据多？？？

848M Jun 28 14:31 SRR1042593.bam
2.7G Jun 28 14:52 SRR1042594.bam
716M Jun 28 14:58 SRR1042595.bam
2.9G Jun 28 15:20 SRR1042596.bam
1.1G Jun 28 15:28 SRR1042597.bam
2.6G Jun 28 15:48 SRR1042598.bam
1.2G Jun 28 15:58 SRR1042599.bam
3.5G Jun 28 16:26 SRR1042600.bam

## 我没有想明白为什么

## http://www2.uef.fi/documents/1698400/2466431/Macs2/f4d12870-34f9-43ef-bf0d-f5d087267602
## http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3120977/我首先用的是下面这些代码
nohup time ~/.local/bin/macs2 callpeak -c SRR1042594.bam -t SRR1042593.bam -f BAM -B -g hs -n Xu_MUT_rep1 2>Xu_MUT_rep1.masc2.log &
nohup time ~/.local/bin/macs2 callpeak -c SRR1042596.bam -t SRR1042595.bam -f BAM -B -g hs -n Xu_MUT_rep2 2>Xu_MUT_rep2.masc2.log &
nohup time ~/.local/bin/macs2 callpeak -c SRR1042598.bam -t SRR1042597.bam -f BAM -B -g hs -n Xu_WT_rep1 2>Xu_WT_rep1.masc2.log &
nohup time ~/.local/bin/macs2 callpeak -c SRR1042600.bam -t SRR1042599.bam -f BAM -B -g hs -n Xu_WT_rep2 2>Xu_WT_rep2.masc2.log &

得到的peaks少的可怜，我第一次检查，以为是因为自己没有sort 比对的bam文件导致

## forget to sort the bam files:
## 首先把bam文件sort好，构建了inde，然后继续运行！

nohup time ~/.local/bin/macs2 callpeak -c SRR1042594.sorted.bam -t SRR1042593.sorted.bam -f BAM -B -g hs -n Xu_MUT_rep1 2>Xu_MUT_rep1.masc2.log &
nohup time ~/.local/bin/macs2 callpeak -c SRR1042596.sorted.bam -t SRR1042595.sorted.bam -f BAM -B -g hs -n Xu_MUT_rep2 2>Xu_MUT_rep2.masc2.log &
nohup time ~/.local/bin/macs2 callpeak -c SRR1042598.sorted.bam -t SRR1042597.sorted.bam -f BAM -B -g hs -n Xu_WT_rep1 2>Xu_WT_rep1.masc2.log &
nohup time ~/.local/bin/macs2 callpeak -c SRR1042600.sorted.bam -t SRR1042599.sorted.bam -f BAM -B -g hs -n Xu_WT_rep2 2>Xu_WT_rep2.masc2.log &

##此时得到peaks跟上面为sort的bam文件得到的peaks一模一样，看来不是这个原因
##然后我怀疑是不是作者上传数据的时候把input和IP标记反了，所以我认为的调整过来

## Then change the control and treatment
nohup time ~/.local/bin/macs2 callpeak -t SRR1042594.sorted.bam -c SRR1042593.sorted.bam -f BAM -B -g hs -n Xu_MUT_rep1 2>Xu_MUT_rep1.masc2.log &
nohup time ~/.local/bin/macs2 callpeak -t SRR1042596.sorted.bam -c SRR1042595.sorted.bam -f BAM -B -g hs -n Xu_MUT_rep2 2>Xu_MUT_rep2.masc2.log &
nohup time ~/.local/bin/macs2 callpeak -t SRR1042598.sorted.bam -c SRR1042597.sorted.bam -f BAM -B -g hs -n Xu_WT_rep1 2>Xu_WT_rep1.masc2.log &
nohup time ~/.local/bin/macs2 callpeak -t SRR1042600.sorted.bam -c SRR1042599.sorted.bam -f BAM -B -g hs -n Xu_WT_rep2 2>Xu_WT_rep2.masc2.log &

##结果，压根就没有peaks了！！！！看了作者并没有搞错
##接下来我怀疑是自己用samtools view -bhS -q 30   处理了sam文件，这个标准太严格了！！
## 
## then just use the sam files.

nohup time ~/.local/bin/macs2 callpeak -c SRR1042594.sam -t SRR1042593.sam -f SAM -B -g hs -n Xu_MUT_rep1 2>Xu_MUT_rep1.masc2.log &
nohup time ~/.local/bin/macs2 callpeak -c SRR1042596.sam -t SRR1042595.sam -f SAM -B -g hs -n Xu_MUT_rep2 2>Xu_MUT_rep2.masc2.log &
nohup time ~/.local/bin/macs2 callpeak -c SRR1042598.sam -t SRR1042597.sam -f SAM -B -g hs -n Xu_WT_rep1 2>Xu_WT_rep1.masc2.log &
nohup time ~/.local/bin/macs2 callpeak -c SRR1042600.sam -t SRR1042599.sam -f SAM -B -g hs -n Xu_WT_rep2 2>Xu_WT_rep2.masc2.log &

## 也没有多几个peaks，最后我只能想到是我的p值太严格了
## then chang the criteria for p values :

https://github.com/taoliu/MACS/

nohup time ~/.local/bin/macs2 callpeak -c SRR1042594.sam -t SRR1042593.sam -f SAM -p 0.01 -g hs -n Xu_MUT_rep1 2>Xu_MUT_rep1.masc2.log &
nohup time ~/.local/bin/macs2 callpeak -c SRR1042596.sam -t SRR1042595.sam -f SAM -p 0.01 -g hs -n Xu_MUT_rep2 2>Xu_MUT_rep2.masc2.log &
nohup time ~/.local/bin/macs2 callpeak -c SRR1042598.sam -t SRR1042597.sam -f SAM -p 0.01 -g hs -n Xu_WT_rep1 2>Xu_WT_rep1.masc2.log &
nohup time ~/.local/bin/macs2 callpeak -c SRR1042600.sam -t SRR1042599.sam -f SAM -p 0.01 -g hs -n Xu_WT_rep2 2>Xu_WT_rep2.masc2.log &

##我大大减小了P值的标准，结果是输出一大堆的peaks

18919 Xu_MUT_rep1_peaks.xls
36277 Xu_MUT_rep2_peaks.xls
32494 Xu_WT_rep1_peaks.xls
56080 Xu_WT_rep2_peaks.xls

问题是这些peaks根本就都是假阳性！！！
我手动的check了几个之前严格过滤条件下的peaks，的确可以看到测序深度是两个山峰形状的曲线

## check some peaks 手动的 ## chr1 121484235 121485608
## masc results :
samtools depth -r chr10:42385331-42385599 SRR1042593.sorted.bam
samtools depth -r chr10:42385331-42385599 SRR1042594.sorted.bam
samtools depth -r chr20:45810382-45810662 SRR1042593.sorted.bam
samtools depth -r chr20:45810382-45810662 SRR1042594.sorted.bam

##我也check了paper里面得到的peak，但是在我的比对文件里面，肉眼看起来根本不像，所以我很纠结

paper results:
chr20 45796362 46384917
chr1 121482722 121485861
samtools depth -r chr1:121482722-121485861 SRR1042593.sorted.bam
samtools depth -r chr1:121482722-121485861 SRR1042594.sorted.bam
samtools depth -r chr20:45796362-46384917 SRR1042593.sorted.bam
samtools depth -r chr20:45796362-46384917 SRR1042594.sorted.bam 
```
很不幸，最后还是没能达到作者的结果，我没搞清楚是为什么，我还用了**BayesPeak/PeakRanger这两个软件，结果也不理想。**

- peak finder软件大全： [http://wodaklab.org/nextgen/data/peakfinders.html](http://wodaklab.org/nextgen/data/peakfinders.html)
- Peak Calling for ChIP-Seq :　[http://epigenie.com/guide-peak-calling-for-chip-seq/](http://epigenie.com/guide-peak-calling-for-chip-seq/)


---

##### 第七讲 peaks注释 {-}

经过前面的ChIP-seq测序数据处理的常规分析，我们已经成功的把测序仪下机数据变成了BED格式的peaks记录文件，我选取的这篇文章里面做了4次ChIP-seq实验，分别是两个重复的野生型MCF7细胞系的 BAF155 immun =oprecipitates和两个重复的突变型MCF7细胞系的 BAF155 immunoprecipitates，这样通过比较野生型和突变型MCF7细胞系的 BAF155 immunoprecipitates的不同结果就知道该细胞系的BAF155 突变，对它在全基因组的结合功能的影响。

```
#我这里直接从GEO里面下载了peaks结果，它们详情如下：wc -l *bed
6768 GSM1278641_Xu_MUT_rep1_BAF155_MUT.peaks.bed
3660 GSM1278643_Xu_MUT_rep2_BAF155_MUT.peaks.bed
11022 GSM1278645_Xu_WT_rep1_BAF155.peaks.bed
5260 GSM1278647_Xu_WT_rep2_BAF155.peaks.bed
49458 GSM601398_Ini1HeLa-peaks.bed
24477 GSM601398_Ini1HeLa-peaks-stringent.bed
12725 GSM601399_Brg1HeLa-peaks.bed
12316 GSM601399_Brg1HeLa-peaks-stringent.bed
46412 GSM601400_BAF155HeLa-peaks.bed
37920 GSM601400_BAF155HeLa-peaks-stringent.bed
30136 GSM601401_BAF170HeLa-peaks.bed
25432 GSM601401_BAF170HeLa-peaks-stringent.bed
```

每个BED的peaks记录，本质是就3列是需要我们注意的，就是染色体，以及在该染色体上面的起始和终止坐标，如下

```
#PeakID chr start end strand Normalized Tag Count region size findPeaks Score Clonal Fold Change
chr20 52221388 52856380 chr20-8088 41141 +
chr20 45796362 46384917 chr20-5152 31612 +
chr17 59287502 59741943 chr17-2332 29994 +
chr17 59755459 59989069 chr17-667 19943 +
chr20 52993293 53369574 chr20-7059 12642 +
chr1 121482722 121485861 chr1-995 9070 +
chr20 55675229 55855175 chr20-6524 7592 +
chr3 64531319 64762040 chr3-4022 7213 +
chr20 49286444 49384563 chr20-4482 6165 +
```

我们所谓的peaks注释，就是想看看该peaks在基因组的哪一个区段，看看它们在**各种基因组区域**(基因上下游，5,3端UTR，启动子，内含子，外显子，基因间区域，microRNA区域)**分布情况**，但是一般的peaks都有近万个，所以需要批量注释，如果脚本学的好，自己下载参考基因组的GFF注释文件，完全可以自己写一个，我这里会介绍一个R的bioconductor包ChIPpeakAnno来做CHIP-seq的peaks注释，下面的包自带的示例

```
library(ChIPpeakAnno)
bed <- system.file("extdata", "MACS_output.bed", package="ChIPpeakAnno")
gr1 <- toGRanges(bed, format="BED", header=FALSE)
## one can also try import from rtracklayer
library(rtracklayer)
gr1.import <- import(bed, format="BED")
identical(start(gr1), start(gr1.import))
gr1[1:2]
gr1.import[1:2] #note the name slot is different from gr1
gff <- system.file("extdata", "GFF_peaks.gff", package="ChIPpeakAnno")
gr2 <- toGRanges(gff, format="GFF", header=FALSE, skip=3)
ol <- findOverlapsOfPeaks(gr1, gr2)
makeVennDiagram(ol)

##还可以用binOverFeature来根据特定的GRanges对象(通常是TSS)来画分布图
## Distribution of aggregated peak scores or peak numbers around transcript start sites.
```

可以看到这个包使用起来非常简单，只需要把我们做好的peaks文件(GSM1278641_Xu_MUT_rep1_BAF155_MUT.peaks.bed等等)用`toGRanges`或者`import`读进去，成一个GRanges对象即可，上面的代码是比较两个peaks文件的overlap。然后还可以根据R很多包都自带的数据来注释基因组特征

```
data(TSS.human.GRCh37) ## 主要是借助于这个GRanges对象来做注释，也可以用getAnnotation来获取其它GRanges对象来做注释
## featureType ： TSS, miRNA, Exon, 5'UTR, 3'UTR, transcript or Exon plus UTR
peaks=MUT_rep1_peaks
macs.anno <- annotatePeakInBatch(peaks, AnnotationData=TSS.human.GRCh37,
output="overlapping", maxgap=5000L)

## 得到的macs.anno对象就是已经注释好了的，每个peaks是否在基因上，或者距离基因多远，都是写的清清楚楚

if(require(TxDb.Hsapiens.UCSC.hg19.knownGene)){
aCR<-assignChromosomeRegion(peaks, nucleotideLevel=FALSE,
precedence=c("Promoters", "immediateDownstream",
"fiveUTRs", "threeUTRs",
"Exons", "Introns"),
TxDb=TxDb.Hsapiens.UCSC.hg19.knownGene)
barplot(aCR$percentage)
}
```

得到的条形图如下，虽然很丑，但这就是peaks注释的精髓，搞清楚每个peaks在基因组的位置特征：

![ChIPpeakAnno-genomic-region-distribution](image/C6/ChIPpeakAnno-genomic-region-distribution.png)

 

同理，对每个peaks文件，都可以做类似的分析！

但是对多个peaks文件，比如本文中的，想比较野生型和突变型MCF7细胞系的 BAF155 immunoprecipitates的结果的不同，就需要做peaks之间的差异分析，已经后续的差异基因注释啦

当然，值得一提的是peaks注释我更喜欢网页版工具，反正peaks文件非常小，直接上传到别人做好的web tools，就可立即出一大堆可视化图表分析结果啦，大家可以去试试看：

[http://chipseek.cgu.edu.tw/](http://chipseek.cgu.edu.tw/)

[http://bejerano.stanford.edu/great/public/html/](http://bejerano.stanford.edu/great/public/html/)

[http://liulab.dfci.harvard.edu/CEAS/](http://liulab.dfci.harvard.edu/CEAS/)

虽然我花费了大部分篇幅来描述ChIPpeakAnno这个包的用法，**但是真正的重点是你得明白peaks记录了什么，要注释什么，以及把这3个网页工具的可视化图表分析结果全部看懂，这网页版工具才是重点！**



---

##### 第八讲：寻找motif {-}

motif是比较有特征的短序列，会多次出现的，一般认为它的生物学意义重大，做完CHIP-seq分析之后，一般都会寻找motif 。

查找有两种，一种是de novo的，要求的输入文件的fasta序列，一般是根据peak的区域的坐标提取好序列；另一种是依赖于数据库的搜寻匹配，很多课题组会将现有的ChIP-seq数据进行整合，提供更全面，更准确的motif数据库。

**motif的定义如下：**

> motif: recurring pattern. eg, sequence motif, structure motif or network motif
>
> DNA sequence motif: short, recurring patterns in DNA that are presumed to have a biological function.

> 从上边的定义可以看出，其实motif这个**单词**就是形容一种反复出现的模式，而**序列motif**往往是DNA上的反复出现的模式，并被假设拥有生物学功能。而且，经常是一些具有序列特异性的蛋白的结合位点（如，转录因子）或者是涉及到重要生物过程的（如，RNA 起始，RNA 终止， RNA 剪切等等）。

摘抄自：[http://blog.163.com/zju_whw/blog/static/225753129201532104815301/](http://blog.163.com/zju_whw/blog/static/225753129201532104815301/)

motif最先是通过实验的方法发现的，换句话说，不是说有了ChIP-seq才有了motif分析，起始很早人们就开始研究motif了！例如，**'TATAAT’ box**在1975年就被pribnow发现了，它与上游的**‘TTGACA’motif**是RNA聚合酶结合位点的特异性序列。而且，当时的人们就知道，不是所有的结合位点都一定完美地与motif匹配，大部分都只匹配了12个碱基中的7-9个。结合位点与motif的匹配程度往往也与蛋白质与DNA的结合强弱有关。

目前被人们识别出来的motif也越来越多，如TRANSFAC和JASPAR数据库都有着大量转录因子的motif。而随着ChIP-seq数据的大量产出，motif的研究会进一步深入，有一些课题组会将现有的ChIP-seq数据进行整合，提供更全面，更准确的motif数据库。

从算法上来讲，这是很复杂的，我就不多说了，我这里主要讲best practice：

一篇文献列出了2014年以前的近乎所有知名的A survey of motif finding Web tools for detecting binding site motifs in ChIP-Seq data 链接见：[https://biologydirect.biomedcentral.com/articles/10.1186/1745-6150-9-4](https://biologydirect.biomedcentral.com/articles/10.1186/1745-6150-9-4)

 **最常用的是 MEME工具套件 ：**

 [http://meme-suite.org/](http://meme-suite.org/)  输入文件是fasta序列，需要对peaks进行转换，根据bed的基因坐标从基因组里面提取对应的序列咯： [http://bedtools.readthedocs.io/en/latest/content/tools/getfasta.html](http://bedtools.readthedocs.io/en/latest/content/tools/getfasta.html)

它里面集成了4个寻找motif 的工具，每个工具都是一篇文章，里面有详细描述具体原理，但是整个网页给人的感觉是too busy，让初学者无从下手。

![meme-suit-motif-finding](image/C6/meme-suit-motif-finding.png)



把自己的fasta序列上传上去即可，还是选取我们本次系列教程的数据

```
$ ls -lh  *fasta
-rw-r--r-- 1 Jimmy 197121  18M Jul  7 19:40 GSM1278641_Xu_MUT_rep1_BAF155_MUT_sequence.fasta
-rw-r--r-- 1 Jimmy 197121 9.9M Jul  7 19:38 GSM1278643_Xu_MUT_rep2_BAF155_MUT_sequence.fasta
-rw-r--r-- 1 Jimmy 197121  26M Jul  7 19:41 GSM1278645_Xu_WT_rep1_BAF155_sequence.fasta
-rw-r--r-- 1 Jimmy 197121  14M Jul  7 19:41 GSM1278647_Xu_WT_rep2_BAF155_sequence.fasta
```

然后就可以看到所有结果啦，大家可以试试看。

最后值得一提的是现在流行的R的bioconductor系列包，也可以寻找motif：

一般的R包都可以直接从BED文件里面记录的基因坐标来找motif，有点需要输入fasta序列，就需要自己根据bed的基因坐标从基因组里面提取对应的序列咯：

rGADEM (motif discovery): [http://bioconductor.org/packages/devel/bioc/html/rGADEM.html](http://bioconductor.org/packages/devel/bioc/html/rGADEM.html)

MotIV (motif validation): [http://bioconductor.org/packages/devel/bioc/html/MotIV.html](http://bioconductor.org/packages/devel/bioc/html/MotIV.html)

[http://lgsun.grc.nia.nih.gov/CisFinder/](http://lgsun.grc.nia.nih.gov/CisFinder/)

[http://bioinfo.cs.technion.ac.il/drim/](http://bioinfo.cs.technion.ac.il/drim/)

[http://www.ncbi.nlm.nih.gov/pubmed/20736340](http://www.ncbi.nlm.nih.gov/pubmed/20736340)

还有一个PICS (ChIP-seq): 虽然不是bioconductor的包 [http://www.rglab.org/pics-probabilistic-inference-for-chip-seq/](http://www.rglab.org/pics-probabilistic-inference-for-chip-seq/) 貌似国内被墙了，无法打开。



---

##### 第九讲：ChIP-seq可视化大全 {-}

讲到这里，我们的自学ChIP-seq分析系列教程就告一段落了，当然，我会随时查漏补缺，根据读者的反馈来更新着系列教程。

其实可视化这已经是一个比较复杂的方向了，不仅仅是针对于ChIP-seq数据。可视化本身是发文章的先决条件，而让人一目了然图片也说明了数据分析人员对数据本身的理解。我这里就列出一些目录和一些工具和ppt。这个主要靠大家自学，而且我博客空间有限，就不上传一大堆图片了，大家随便找一些经典的paper里面都会有很多可视化分析。

首先强烈推荐两个网页版工具，针对找到的peaks可视化:

[http://chipseek.cgu.edu.tw/](http://chipseek.cgu.edu.tw/)

[http://bejerano.stanford.edu/great/public/html/](http://bejerano.stanford.edu/great/public/html/)

然后再推荐一个哈佛刘小乐实验室出品的软件，也是专门为了作图 [http://liulab.dfci.harvard.edu/CEAS/usermanual.html](http://liulab.dfci.harvard.edu/CEAS/usermanual.html)

还有一个java工具：也可以可视化CHIP-seq的peaks结果EXPANDER (EXpression Analyzer and DisplayER) is a java-based tool for analysis of gene expression data.[http://acgt.cs.tau.ac.il/expander/help/ver7.0Help/html/Input_Data_.htm](http://acgt.cs.tau.ac.il/expander/help/ver7.0Help/html/Input_Data_.htm)

如图所示

![CHIP-seq的peaks](image/C6/multiple-IP-chip-seq-data-visualization.jpg)

然后我所了解的图片大概有下面这些，都是有专门的软件，甚至自己写脚本也可以做的

- peaks长度分布柱状图
- 每个peak的测序情况可视化(IGV,sushi)
- 测序reads在全基因组各个染色体的分布(Chromosome ideograms)
- reads相对基因位置分布统计
- peaks相对基因位置分布统计
- reads在基因组位置分布统计（染色体分开作图）
- peaks在基因组位置分布统计（染色体分开作图）
- 统计peaks在各种基因组区域(基因上下游，5,3端UTR，启动子，内含子，外显子，基因间区域，- microRNA区域)分布情况，条形图和饼图均可
- peak与转录起始位点距离的分析（曲线图和热图）


**最后总结一下**

CHIP-seq pipeline :　[http://www.slideshare.net/COST-events/chipseq-data-analysis](http://www.slideshare.net/COST-events/chipseq-data-analysis)

大家一定要看这个ChIP-seq guidelines and practices of the ENCODE and modENCODE consortia.  [http://www.ncbi.nlm.nih.gov/pubmed/22955991](http://www.ncbi.nlm.nih.gov/pubmed/22955991)

#### ChIP-seq数据分析指引

##### 写在前面 {-}

参加了[RNA-seq基础入门 ](http://www.biotrainee.com/thread-1750-1-1.html )的朋友可以先看看 我以前分享过有参组学(全基因组，全外显子组学，转录组学，表观)的几个NGS测序数据分析的表现形式的异同点，主要是各个地方的测序深度，各个地方的覆盖情况的区别！

分析流程都是fastq-->bam-->vcf/expression/peaks，中间选择不同软件，不同参数而已。 视频在链接： http://pan.baidu.com/s/1jIQFGSA 密码：48uj

本次其实已经有不少人已经完成了，优秀作业如下：

* [jimmy的ChIP-seq实战分析](http://mp.weixin.qq.com/s/_A0rHldzEgVk7bgwt457qQ)
* [张生的ChIP-seq实战分析](https://mp.weixin.qq.com/s/mEwl7-f2WTNfmK-FkNkP0A)
* [赵小凡的ChIP-seq实战分析](http://mp.weixin.qq.com/s/LcAEzGEmjj37XYogeU3v6A)



##### step1:计算机资源的准备 {-}

这个跟转录组对计算资源的要求是大同小异的，最好是有mac或者linux系统，8G+的内存，500G的存储即可。

* 如果你是Windows，那么安装必须安装 git,notepad++,everything，还有虚拟机，在虚拟机里面安装linux，最好是ubuntu。
* 如果本身就是mac或者linux，那么很简单了，安装好wget吧

需要安装的各种ChIP-seq软件包括 sratoolkit,fastqc,bowtie2,samtools,htseq-count,bedtools,macs2,HOMER,R,Rstudio 

软件安装的代码，在生信技能树公众号后台回复老司机即可拿到。

如果呀详细了解计算机配置清单，软件安装等，请查看我们[公众号推文](http://mp.weixin.qq.com/s/e-m8edpa4G-uqZxY4affPA)

###### 作业1 {-}

安装好软件，下载软件的说明书，整理它们的官网链接。


##### step2:读文章拿到测序数据 {-}

本次讲解选取的文章是为了探索PRC1，PCR2这样的蛋白复合物，不是转录因子或者组蛋白的CHIP-seq，请注意区别。

文章题目

RYBP and Cbx7 define specific biological functions of polycomb complexes in mouse embryonic stem cells

https://www.ncbi.nlm.nih.gov/pubmed/23273917

从文章里面找到数据存放地址如下：

数据下载：

* https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE42466
* ftp://ftp-trace.ncbi.nlm.nih.gov/sra/sra-instant/reads/ByStudy/sra/SRP/SRP017/SRP017311

###### 作业2 {-}

看文章里的methods部分，把它用到的软件和参数摘抄下来，然后理解GEO/SRA数据库的数据存放形式，把规律和笔记发在论坛上面

##### step3:了解fastq测序数据 {-}

需要用安装好的sratoolkit把sra文件转换为fastq格式的测序文件，并且用fastqc软件及MultiQC汇总查看测试测序文件的质量！

###### 作业3 {-}

理解测序reads，GC含量，质量值，接头，index，fastqc的全部报告，搜索中文教程，并发在论坛上面。


##### step4:了解参考基因组及基因注释 {-}

在UCSC下载hg19参考基因组，我博客有详细说明，从gencode数据库下载基因注释文件，并且用IGV去查看你感兴趣的基因的结构，比如TP53,KRAS,EGFR等等。

###### 作业4 {-}

截图几个基因的IGV可视化结构！还可以下载ENSEMBL，NCBI的gtf，也导入IGV看看，截图基因结构。了解IGV常识。

##### step5:序列比对 {-}

比对软件很多，首先大家去收集一下，因为我们是带大家入门ChIP-seq基础，请统一用bowtie2，并且搞懂它的用法。 再思考一下为什么不同的组学数据有着不同的最佳比对软件。

直接去bowtie2的主页下载index文件即可，然后把fastq格式的reads比对上去得到sam文件。

接着用samtools把它转为bam文件，并且排序索引好，考虑一下是否需要去重PCR重复，载入IGV，再截图几个基因看看！

顺便对bam文件进行简单QC，参考直播我的基因组系列。

###### 作业5 {-}

把ChIP-seq比对得到的bam跟转录组的bam统一载入IGV，看看各种genomic features上面的reads分布的区别，想一想为什么是这样。


##### step6:寻找peaks {-}

peaks-calling的软件也不少，如果需要了解原理，请看我[前面的推文](http://mp.weixin.qq.com/s/tnbhwZ16QiAnWulCvBNFsQ)  这里统一用MACS2软件

###### 作业6 {-}

把通过macs2得到的bed格式peaks文件，也载入IGV，跟bam文件进行比较。

##### step7:peaks注释 {-}

得到的bed格式peaks文件只是记录每个peaks的染色体以及起始终止坐标，一般情况下需要看看该peaks在基因组的哪一个区段。
看看它们在各种基因组区域(基因上下游，5,3端UTR，启动子，内含子，外显子，基因间区域，microRNA区域)分布情况，但是一般的peaks都有近万个，所以需要批量注释！

能做CHIP-seq的peaks注释，有R的bioconductor包ChIPpeakAnno，以及chipseeker包，还有HOMER软件，大家都可以用一下。 注释完毕，顺便可视化一下。

###### 作业7 {-}

找到R包，并读文档，整理文档和链接，以及文档里面的例子，如何学习一个R包。 比较多种注释的结果的差异。

##### step8：信号的可视化 {-}

因为peaks在基因组的分布是有规律的，如果是集中在TSS附近，就可以画TSS附近的信号强度图，一些人为处理可以改变peaks的分布，同理信号强度也会改变，这个是大家的注意分析结果以及生物学一样。

可以选择NGSPLOT这个R包，或者deeptools这个python软件，个人比较喜欢deeptools

这里可以选择

###### 作业8 {-}

得到一些genomic features的信号强度可视化图。

##### 后记 {-}

因为本文选择的是PRC1，PCR2这样的蛋白复合物，不是转录因子或者组蛋白的CHIP-seq，所以一般不需要做motif等等。

而且我们文章并没有设计处理前后的IP实验，没有peaks的变化，也不需要找差异结合位点。

#### 450K甲基化芯片数据处理传送门

##### 写在前面 {-}

Illumina甲基化芯片目前仍是很多实验室做甲基化项目的首选，尤其是对于大样本研究而言，其性价比相当高。这种芯片的发展主要经历了27K、450K以及850K，目前积累的数据主要是450K芯片的，未来850K可能会成为主流。之前我写过一篇[450K芯片预处理的帖子](http://www.biotrainee.com/thread-237-1-1.html)，其中也介绍了这种芯片的基础知识以及流程图和代码，大家可以先看看。芯片的处理流程一般就是：**数据读入——数据过滤——数据校正——下游分析。**

##### step1:计算机资源的准备 {-}

与测序相比，芯片的处理可能对计算资源的要求是不算高，主要使用的工具就是R，R的使用比较耗内存，尤其是处理大批量数据的时候。

R本身是支持各种系统的，所以不管是mac、windows还是linux理论上都是可以的，只要下载对应版本即可。当然，如果你会linux最好在linux操作。其实数据分析很多都是相通的，所以之前群主推荐的配置和工具都是可以拿来用的。

需要安装的R packages包括 ChAMP，minfi和wateRmelon等.

###### 作业1 {-}

1. 安装好R软件及相应的包，下载R包的说明书，整理它们的官网链接。
2. 了解illumina 450K甲基化芯片的探针设计，下载manifest文件。

##### step2:读文章拿到测序数据 {-}

本次讲解用到的数据来自文章[The relationship between DNA methylation, genetic and expression inter-individual variation in untransformed human fibroblast](https://www.ncbi.nlm.nih.gov/pubmed/24555846)

从文章里面找到数据存放地址如下：https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE52025

###### 作业2 {-}

看文章里的methods部分，把它的分析步骤摘抄下来，然后理解GEO数据库的数据存放形式，把规律和笔记发在论坛上面，类似于[RNA-seq数据处理入门](http://www.biotrainee.com/thread-1750-1-1.html)和[CHIP-seq数据处理入门](http://www.biotrainee.com/thread-2013-1-1.html)

##### step3:了解芯片数据 {-}

需要阅读相关的资料，比如illumina的官网介绍及相关的文献，对甲基化及该芯片的技术核心有一定了解，对于存在的问题也要知道，这对于后面分析时理解校正的意义非常重要！

###### 作业3 {-}

理解芯片的probe，Bead，p值，I型探针和II型探针等。

##### step4:了解probe注释 {-}

在GEO或者illumina下载450K注释文件，理解每一列的意义及探针的分类。或者下载官网的manifest文件并且理解。

###### 作业4 {-}

下载注释文件，理解甲基化探针的分类及注释。

##### step5:数据读入 {-}

处理甲基化芯片的R包其实很多，我之前用的是minfi，现在用ChAMP应该更加方便，它整合了很多分析处理数据的方法，例如过滤和校正等，所以大家可以以ChAMP为主。

###### 作业5 {-}

查看甲基化芯片文件的命名规则，整理文件读入所需的表格，使用ChAMP包读入文件。

##### step6:数据过滤及数据校正 {-}

数据过滤主要是根据p值和bead数，probe还需要注意过滤snp和multiple-hit，样本过滤可以考虑PCA或MDS，很多时候R包会直接帮我们做了，但是需要对过滤的标准做到心中有数。

数据校正主要是I型探针和II型探针校正，批次校正和混杂因素校正等。

###### 作业6 {-}

根据p值和bead数过滤探针和样本，过滤SNP和multiple-hit的探针，使用BMIQ校正探针类型，使用combat校正批次效应，使用lm校正混杂因素。

##### step7:下游分析 {-}

下游分析一般根据需求来定，比如差异甲基化分析、甲基化与表达的整合分析等。

##### 作业7 {-}

学习T-test和线性回归的差异甲基化分析。

##### step8：探针注释、绘图等 {-}

甲基化探针可以根据官方给的注释文件进行基因和CGI的注释。

也可以使用webgestalt对感兴趣的探针做GO和Pathway的分析。

可以使用ggplot等对探针的分布进行绘图。

###### 作业8 {-}

理解甲基化探针的CGI及基因位置注释并且简单可视化。


### 微生物组

“微生物组”（microbiome）是指存在于特定环境（生态位，biotype）中所有微生物种类及其遗传信息和功能的集合，其不仅包括该环境中微生物间的相互作用，还包括有微生物与该环境中其它物种及环境的相互作用。

微生物组本身就包含了上面的基因组，转录组的数据分析，同时也涉及到蛋白和代谢的组学研究。

微生物作为地球上进化历史最长、生物量最大、生物多样性最丰富的生命形式，推动地球化学物质循环，影响人类健康乃至地球生态系统，蕴藏着极为丰富的物种资源和基因资源。在自然条件下，各类微生物与其所处的环境及环境中存在的宿主构成了复杂的生态系统，微生物以其社会行为，在维护人类健康及地球生态系统物质循环中发挥着不可替代的作用。

正因为认识到了微生物对人和地球的重要性，美国一批处在学科前沿的科学家就曾在《科学》周刊上，提出开展“联合微生物组研究计划（Unified Microbiome Initiative，简称 UMI）”的建议。

UMI是经由白宫科学与技术政策办公室、加利福尼亚Oxnard的Kavli基金会组织这些科学家经过一系列研讨后提出来。UMI将把美国国立卫生研究院、美国自然基金委、美国农业部、美国能源部、美国环境总署等政府部门和私立基金会以及企业界的力量动员和整合起来，开展对人体、植物、动物、土壤和海洋等几乎所有环境中微生物组的深入研究。这是美国在推出脑科学计划和精准医学计划以后，又一个举世瞩目的大科学计划。

#### 细菌序列的功能预测与注释 {-}

目前在NCBI等数据库中拥有大部分细菌的全基因组序列，这些全基因序列涵盖了这些细菌的功能以及代谢途径等信息。研究者在细菌16S扩增子测序中，通常会使用[PICRUST](http://huttenhower.sph.harvard.edu/galaxy/root?tool_id=PICRUSt_normalize)及[Tax4Fun](http://tax4fun.gobics.de/)进行细菌功能预测与注释。

#### 真菌序列的功能预测与注释 {-}

由于目前真菌的全基因信息相对较少，而且真菌基因组远比细菌复杂，所以对真菌群落的研究一直没有很好地办法直接把其分类信息注释到功能基因以及代谢途径的信息上去。在2016年Nguyen开发了一个使用在线的和本地的python语言写的脚本，[FUNGuild](http://www.stbates.org/guilds/app.php)来对真菌的扩增子测序拿到的OUT table 进行基本的功能注释。目前它还只能对真菌的物种信息注释到其是病原、共生、腐生这三大类，并进一步细分为12个小类：

- 动物病原菌（animalpathogens）
- 丛枝菌根真菌（arbuscularmycorrhizal fungi）
- 外生菌根真菌（ectomycorrhizal fungi）
- 杜鹃花类菌根真菌（ericoidmycorrhizal fungi）
- 叶内生真菌（foliar endophytes）
- 地衣寄生真菌（lichenicolousfungi）
- 地衣共生真菌（lichenizedfungi）
- 菌寄生真菌（mycoparasites）
- 植物病原菌（plantpathogens）
- 未定义根内生真菌（undefinedroot endophytes）
- 未定义腐生真菌（undefinedsaprotrophs）
- 木质腐生真菌（woodsaprotrophs）

还包括三类形态特殊的真菌：yeast、facultativeyeast和thallus。 

网页工具使用也非常简单，就是把格式为.txt或.csv的OUT table上传到网页即可。最后可以统计出不同guild的比例做柱状图、可以分析TOP丰度OTU的guild情况，也可以结合理化因子等对研究对象做出合理的解释与假设等等。 

这样功能分类就搞定了，再通过生物信息学脚本将物种分类与功能guild分类联系起来就大功告成啦并对其所有的注释信息给出文献的出处，但这也是真菌宏基因研究从单纯群落结构分析转向功能结构分析迈出的重要一步。



### 蛋白质组

蛋白质组学（Proteomics）是指在**大规模水平上研究蛋白质的特征**，包括蛋白质的表达水平，翻译后修饰，蛋白与蛋白相互作用等，由此获得蛋白质水平关于疾病发生、细胞代谢等过程的整体而全面的认识。从蛋白角度分析疾病发生与发展的分子机理，挖掘疾病标志物（biomarker），并为免疫治疗和药物开发提供强力支撑，迈进精准医疗。蛋白质组的研究不仅为生命活动规律提供物质基础，也为多种疾病机制的阐明及攻克提供理论。

蛋白质组学的研究是生命科学进入后基因组时代的特征，不仅是探索生命奥秘的必须工作，也能为人类健康事业带来巨大的利益。蛋白质不仅能为生命活动规律提供物质基础，而且蛋白质丰度的改变能够阐明多种疾病的机理。

#### 定量蛋白质组学

**定量蛋白质组学**是蛋白质组学中非常重要的研究方向。目前有很多定量方法例如SILAC, TMT, LFQ, PRM等多种方法，每种方法都有各自的优缺点，如何根据实验需求选择合适的定量方法是实验中的难点和热点。

##### 蛋白质芯片（Protein Array）{-}

> 蛋白质芯片检测技术，是将大量不同的蛋白质有序地排列、固定于固相载体表面，形成微阵列，利用蛋白质分子间特异性结合的原理，实现对生物蛋白质分子精准、快速、高通量的检测。

主要类型：

* 夹心法芯片(Sandwich-based Array)
* 标记法芯片(Label-based Array)
* 定量芯片(Quantitative Array)
* 半定量芯片(Semi-Quantitative Array)

##### 质谱（Mass Spectrometry）{-}

> 质谱法是利用电场和磁场，将运动的离子按它们的质荷比分离后，测出离子准确质量并确定离子的化合物组成，即是通过分析样品的离子质荷比而实现对样品的定性和定量的一种方法。

主要类型：

* 二维电泳+质谱(2D/Mass Spectrometry, MS) 
* 表面增强激光解吸电离飞行时间质谱(Surface-enhanced laser desorption/ionization-time of flight, SELDI)
* 同位素标记相对和绝对定量(Isobaric tags for relative and absolute quantitation, iTRAQ)


#### 修饰蛋白质组学

**翻译后修饰**是指对翻译后的蛋白质进行共价加工的过程，通过在一个或多个氨基酸残基加上修饰基团，可以改变蛋白质的理化性质。许多至关重要的生命进程不仅由蛋白质的相对丰度控制，更重要的是受到时空特异性和翻译后修饰的调控，揭示翻译后修饰的发生规律是解析蛋白质复杂多样的生物功能的一个重要前提。

常见的翻译后修饰包括**磷酸化**、**糖基化**、**乙酰化**、**泛素化**等等，具体如下表:

![蛋白质常见的翻译后修](image/C6/protein_modification.png)


质谱是鉴定蛋白质翻译后修饰的重要方法，其原理是利用蛋白质发生修饰后的质量偏移来实现翻译后修饰位点的鉴定；同时，由于翻译后修饰的蛋白质在样本中含量低且动态范围广，检测前需要对发生修饰的蛋白质或肽段进行富集，然后再进行质谱鉴定。


#### 定量修饰蛋白质组学

**修饰蛋白质组学定量分析**主要是将修饰蛋白质或肽段富集方法与蛋白质组学相对定量的技术结合起来，在鉴定翻译后修饰位点的同时，对不同样本中翻译后修饰的程度进行相对定量，从而实现大规模的修饰蛋白质组学定性和定量分析；通常用于修饰定量分析的蛋白质组学方法包括非标记定量Label free，标记定量iTRAQ及SILAC。 根据技术原理可以分为两种

- 同重元素标记法和非标记法（lable free）。其中标记法依据使用的平台不一样可分为iTRAQ（ABI公司，分4标和8标）和TMT（Thermo公司，分2-10标）
- 非标记法根据数据采集的模式可以分为两种：lable free（DDA，Data Dependent Acquisition）和DIA（DIA，Data-Independent Acquisition）。

定量之后就可以走转录组数据的分析流程了，包括差异表达，GO/KEGG等功能数据库的注释，共表达热图，相关分析。 具体分析流程如下：


![定量修饰蛋白质组学分析数据分析流程](image/C6/protein_modification_expression.png)

### 代谢组

**代谢组学**是继基因组学和蛋白质组学之后新发展起来的一门学科，它通过对**人体内小分子代谢物（50~1,500 Da）**进行精准定性定量，分析代谢物与人体生理病理变化的关系，研究疾病发生发展、寻找疾病生物标记物、预测疾病预后等。代谢组学在临床诊断上将有广阔的发展前景，主要应用方向有四个方面：

* 临床诊断（Biomarker）
* 病因与病理机制
* 临床用药指导中
* 临床前动物模型筛选上

机体在环境因素作用下，复杂信号通路中基因、酶、蛋白调节**最终结果的整体体现为代谢表型。**精准医疗相比传统医疗最大的优势体现在它是根据个体的差异有针对性地实施治疗策略，提高个体疾病治疗的成功率和有效率，可减少治疗后的副作用。代谢组学通过高分辨的**质谱、核磁等分析技术**，检测体液当中的代谢产物，通过模式识别模型筛选和疾病相关的代谢标志物。这些代谢标志物的变化体现了机体与饮食、生活方式、肠道菌群、地理位置及遗传背景等因素相互作用的结果。

最终体现在代谢表型的差异上。因此，代谢表型的不同反映了由遗传背景及环境因素所引起的个体差异。代谢组学技术的特点体现为代谢表型的特点。首先人体代谢表型由遗传背景和环境双重因素决定。

#### 代谢组学相关数据库

常用的代谢组学相关数据库有人类代谢组数据库[Human Metabolome Database， HMDB](http://www.hmdb.ca/)、[KEGG数据]库(http://www.genome.jp/kegg/pathway.html)、[Reactome数据库](http://www.reactome.org)等，一一介绍如下：

* 人类代谢组数据库（HMDB）是代谢组学热门数据库之一，包含人体内发现的小分子代谢物的详细信息，包含不少于79,650种代谢物条目。
* SMPDB数据库与HMDB关联，包含约700种人类代谢和疾病途径的途径图。KEGG数据库是代谢组热门数据库之一，包含代谢通路和互作网络信息。
* Reactome数据库主要收集了人体主要代谢通路信息以及重要反应。
* MassBank数据库主要收集许多高分辨率低代谢组分的谱图。

**BioCyc数据库**包含通路和基因组数据。METLIN数据库，是商业化的代谢组及串联质谱数据库，包含有约43000种代谢物和22000个MS/MS谱图。FiehnLib数据库是商业化的代谢组数据库，包含约1000个保守的代谢分子的EI光谱。

**NIST/EPA/NIH Mass Spectral Library数据库**也是商业化的代谢组数据库，包含超过190,000 个EI谱图。 BioCyc数据库收集了通路和基因组数据，可以免费使用。MetaCyc数据库广泛收集了许多来自不同生物体的代谢通路以及酶的信息，囊括了超过51000篇文献。MMCD数据库收集有超过10000种代谢物的信息以及它们的质谱和核磁共振谱数据，大多数是拟南芥的代谢物。

如何更好地整合各种组学数据目前仍是生物学界面临的一个重大挑战，并且有时还要面对不够完善的实验设计、不同实验平台的数据的整合。常用的方法是代谢通路水平的分析、生物网络分析、经验关联分析等。

#### 代谢组学相关网页工具

> 有一些软件或网站可以提供现成的整合多种组学数据的分析。

* 如可以进行代谢通路富集分析的有：IMPaLA网站，使用了来自11个数据库等3000多个代谢通路的信息，可以用于整合多种组学的分析；此外还有iPEAP软件，MetaboAnalyst网站等也可以提供代谢通路富集分析。


* 提供生物网络分析的包括：SAMNetWeb网站，可以提供转录组和蛋白组的通路腹肌分析和网络分析；pwOmics包，是R软件包，能够由随着时间变化的数转录组和蛋白组信息构建网络；相似的软件还有MetaMapR（R软件包，拥有用户界面）、MetScape（Cytoscape插件） 、Grinn（R软件包）等。


* 可以进行经验关联分析的有：WGCNA（R软件包），可以基于相关性和网络拓扑结构对多种组学数据进行整合分析；其他R软件包还有MixOmic、DiffCorr、qpgraph、huge。



#### 代谢组学数据特点

代谢组学数据具有以下特点：

- 高噪声：生物体内含有大量维持自身正常功能的内源性小分子，具有特定研究意义的生物标志物只是其中很少一部分，绝大部分代谢物和研究目的无关。
- 高维、小样本：代谢物的数目远大于样品个数，不适合使用传统的统计学方法进行分析，多变量分析容易出现过拟合和维数灾难问题。
- 高变异性：一是不同代谢物质的理化性质差异巨大，其浓度含量动态范围宽达7～9个数量级，二是生物个体间存在各种来源的变异，如年龄、性别都可能影响代谢产物的变化，三是仪器测量受各种因素影响，容易出现随机测量误差和系统误差，这使得识别有重要作用的生物标志物可能极其困难。
- 相互作用关系复杂：各种代谢物质可能不仅具有简单的相加效应，而且可能具有交互作用，从而增加了识别这些具有复杂关系的生物标志物的难度。
- 相关性和冗余性：各种代谢物并非独立存在，而是相互之间具有不同程度的相关性，同时由于碎片、加合物和同位素的存在使得数据结构存在很大的冗余性，这就需要采用合理的统计分析策略来揭示隐藏其中的复杂数据关系。
- 分布的不规则和稀疏性： 代谢组学数据分布不规则，而且数据具有稀疏性（即有很多值为零) ，因此，传统的一些线性和参数分析方法此时可能失效。

#### 代谢组数据分析

说起代谢组数据分析软件，可能大家第一个想到的就是SIMCA-P。[SIMCA-P](http://umetrics.com/products/simca)软件是由Umetrics公司在1987研究开发，目前是一款公认的多元变量统计分析软件，被绝大多数代谢组服务提供商所采用。

SIMCA-P虽然是一款强大的多元变量统计分析软件，但也有不足之处：

1. SIMCA-P是一款商业软件，需要收费（但有30天免费试用），windows版本的。对于想使用免费软件的老师来说，不那么友好；
2. 代谢组数据的分析除了多元变量统计分析，还有原始数据前处理（pre-processing）、数据处理（processing）、单变量统计分析等，很显然SIMCA-P在这些方面力不从心。

在《Metabolomics-Fundamentals and Applications》这本书的第四章节《Processing and Visualization of Metabolomics Data Using R》中，作者列出了一些代谢组数据处理、统计分析与可视化的开源免费软件工具！

数据分析的流程

1）数据预处理；2）PCA分析；3）PLS-DA分析；4）OPLS-DA分析；5）差异化合物筛选；6）差异化合物鉴定；7）多组分关联分析（同一个体同时取多组分）；8）代谢通路分析；9）多组学数据关联分析（具备多组学数据）

##### 数据的预处理{-}

代谢组学数据分析的目的是希望从中挖掘出生物相关信息，然而，代谢组学数据的变异来源很多，不仅包括生物变异，还包括环境影响和操作性误差等方面。

处理手段主要包括归一化（standardization) 、标准化（normalization) ，即中心化（centering) 和尺度化（scaling)，以及数据转换（transformation)。

##### 归一化是针对样品的操作 {-}

由于生物个体间较大的代谢物浓度差异或样品采集过程中的差异（如取不同时间的尿样) ，为了消除或减轻这种不均一性，一般使用代谢物的相对浓度，即每个代谢物除以样品的总浓度，以此来校正个体差异或其他因素对代谢物绝对浓度的影响。

##### 标准化是对不同样品代谢物的操作 {-}

即统计学意义上的变量标准化。标准化的目的是消除不同代谢物浓度数量级的差别，但同时也可能会过分夸大低浓度组分的重要性，即低浓度代谢物的变异系数可能更大。

数据转换是指对数据进行非线性变换，如log转换和power转换等。数据转换的目的是将一些偏态分布的数据转换成对称分布的数据，并消除异方差性的影响，以满足一些线性分析技术的要求。不同的预处理方法会对统计分析结果产生不同的影响（见表1) ，在实际应用中，我们应该根据具体的研究目的﹑数据类型以及要选用的统计分析方法综合考虑，选择适当的预处理方式。例如，Robert A. van den Berg等（2006) 通过实际代谢组学数据的分析发现，选用不同预处理方法在很大程度上影响着主成分分析（PCA) 的结果，自动尺度化（auto scaling)和全距尺度化（range scaling) 在对代谢组学数据进行探索性分析时表现更优，其PCA 分析后的结果在生物学上能够得到更合理的解释。

##### 单变量分析方法 {-}

单变量分析方法简便﹑直观和容易理解，在代谢组学研究中通常用来快速考察各个代谢物在不同类别之间的差异。

代谢组学数据在一般情况下难以满足参数检验的条件，使用较多的是非参数检验的方法，如Wilcoxon 秩和检验或Kruskal-Wallis 检验，t’检验也是一种比较好的统计检验方法。

由于代谢组学数据具有高维的特点，所以在进行单变量分析时，会面临多重假设检验的问题。如果我们不对每次假设检验的检验水准α进行校正，则总体犯一类错误的概率会明显增加。

一种解决方法是采用Bonferion校正，即用原检验水准除以假设检验的次数m作为每次假设检验新的检验水准（α/m) 。由于Bonferion校正的方法过于保守，会明显降低检验效能，所以在实际中更为流行的一种做法是使用阳性发现错误率（false discovery rate，FDR) 。

这种方法可用于估计多重假设检验的阳性结果中，可能包含多少假阳性结果。FDR 方法不仅能够将假阳性的比例控制在规定的范围内，而且较之传统的方法在检验效能上也得到显著的提高。

实际中也可以使用局部FDR（用fdr表示) ，其定义为某一次检验差异显著时，其结果为假阳性的概率。局部FDR 的使用，使得我们能够估计出任意变量为假阳性的概率，通常情况下有FDR≤fdr。

除了进行传统的单变量假设检验分析，代谢组学分析中通常也计算代谢物浓度在两组间的改变倍数值（fold change) ，如计算某个代谢物浓度在两组中的均值之比，判断该代谢物在两组之间的高低表达。计算ROC 曲线下面积（AUC) 也是一种经常使用的方法。

##### 多变量分析 {-}

代谢组学产生的是高维的数据，单变量分析不能揭示变量间复杂的相互作用关系，因此多变量统计分析在代谢组学数据分析中具有重要的作用。

总体来说，代谢组学数据多变量统计分析方法大致可以分为两类：

- 一类为非监督的学习方法，即在不给定样本标签的情况下对训练样本进行学习，如PCA、非线性映射（NLM) 等；
- 另一类为有监督的学习方法，即在给定样本标签的情况下对训练样本进行学习，如偏最小二乘判别分析（PLS-DA) 、基于正交信号校正的偏最小二乘判别分析（OPLS-DA) 、人工神经网络（ANN) 、支持向量机（SVM) 等。其中，PCA、PLS-DA和OPLS-DA是目前代谢组学领域中使用最为普遍的多变量统计分析方法。

##### PCA {-}

PCA是从原始变量之间的相互关系入手，根据变异最大化的原则将其线性变换到几个独立的综合指标上（即主成分) ，取2～3个主成分作图，直观地描述不同组别之间的代谢模式差别和聚类结果，并通过载荷图寻找对组间分类有贡献的原始变量作为生物标志物。通常情况下，由于代谢组学数据具有高维、小样本的特性，同时有噪声变量的干扰，PCA的分类结果往往不够理想。

尽管如此，PCA作为代谢组学数据的预分析和质量控制步骤，通常用于观察是否具有组间分类趋势和数据离群点。在组间分类趋势明显时，说明其中一定有能够分类的标志物。

PCA还可以用于分析质控样品是否聚集在一起，如果很分散或具有一定的变化趋势，则说明检测质量存在一定的问题。Zhang Zhiyu 等（2010) 通过PCA 成功区分了骨肉瘤患者和正常人，并发现良性骨肿瘤患者中有两例是异常值。Kishore K. Pasikanti 等（2009) 利用PCA 对尿液膀胱癌代谢组学数据进行分析后观察到质控样品在PCA得分图上紧密聚集，从而验证了仪器检测的稳定性和代谢组学数据的可靠性。

 

##### PLS-DA {-}

PLS-DA 是目前代谢组学数据分析中最常使用的一种分类方法，它在降维的同时结合了回归模型，并利用一定的判别阈值对回归结果进行判别分析。ZhangTao 等（2013) 运用PLS-DA技术分析尿液卵巢癌代谢组学数据，成功将卵巢癌患者和良性卵巢肿瘤患者以及子宫肌瘤患者相互鉴别，并鉴定出组氨酸、色氨酸、核苷酸等多种具有判别能力的卵巢癌生物标志物。

PLS的思想是，通过最大化自变量数据和应变量数据集之间的协方差来构建正交得分向量（潜变量或主成分) ，从而拟合自变量数据和应变量数据之间的线性关系。

**PLS**的降维方法与**PCA **的不同之处在于**PLS **既分解自变量**X **矩阵也分解应变量**Y **矩阵，并在分解时利用其协方差信息，从而使降维效果较**PCA **能够更高效地提取组间变异信息。

当因变量Y为二分类情况下，通常一类编码为1，另一类编码为0或-1；当因变量Y为多分类时，则需将其化为哑变量。通常，评价PLS-DA 模型拟合效果使用R2X、R2Y和Q2Y这三个指标，这些指标越接近1 表示PLS-DA 模型拟合数据效果越好。其中，R2X 和R2Y 分别表示PLSDA分类模型所能够解释X 和Y 矩阵信息的百分比，Q2Y 则为通过交叉验证计算得出，用以评价PLS-DA模型的预测能力，Q2Y 越大代表模型预测效果较好。

实际中，PLS-DA 得分图常用来直观地展示模型的分类效果，图中两组样品分离程度越大，说明分类效果越显著。代谢组学数据分析中另一种常用的方法是OPLS-DA，它是PLS-DA 的扩展，即首先使用正交信号校正技术，将X 矩阵信息分解成与Y 相关和不相关的两类信息，然后过滤掉与分类无关的信息，相关的信息主要集中在第一个预测成分。Johan Trygg 等认为该方法可以在不降低模型预测能力的前提下，有效减少模型的复杂性和增强模型的解释能力。

与PLSDA模型相同，可以用R2X、R2Y、Q2Y 和OPLS-DA 得分图来评价模型的分类效果。Carolyn M. Slupsky 等（2010) 使用OPLS-DA 发现卵巢癌患者、乳腺癌患者、正常人这三者之间的尿液代谢轮廓显著不同，从而推断尿液代谢组学可能为癌症的特异性诊断提供重要依据。

由于代谢组学数据具有高维、小样本的特性，使用有监督学习方法进行分析时很容易产生过拟合的现象。

为此，需要使用置换检验考察PLS-DA 在无差异情况下的建模效果。该方法在固定X 矩阵的前提下，随机置换Y分类标签n次，每次随机置换后建立新的PLS-DA 模型，并计算相应的R2Y 和Q2Y； 然后，与真实标签模型得到的结果进行比较，用图形直观表达是否有过拟合现象。

由于样本量的不足，通常采用上述的交叉验证和置换检验方法作为模型验证方法。而实际中，在样本量允许的情况下，最为有效的模型验证方法即将整个数据集严格按照时间顺序划分为内部训练数据和外部测试数据两部分，利用内部训练数据建立模型，再对外部测试数据进行预测，客观地评价模型的有效性和适用性

##### 生物标志物的筛选 {-}

代谢组学分析的最终目标是希望从中筛选出潜在的生物相关标志物，从而探索其中的生物代谢机制，因此需要借助一定的特征筛选方法进行变量筛选。

对于高维代谢组学数据的特征筛选，研究的目的是从中找出对样本分类能力最强或较强的一个或若干个变量。**特征筛选方法主要分为三类： 过滤法、封装法和嵌入法**。


- **过滤法**主要是采用单变量筛选方法对变量进行筛选，优点是简单而快捷，能够快速的降维，如t’检验、Wilcoxon秩和检验、SAM等方法。
- **封装法**是一种多变量特征筛选策略，通常是以判别模型分类准确性作为优化函数的前向选择、后向选择和浮动搜索特征变量的算法，它通常是按照“节省原则”进行特征筛选，最终模型可能仅保留其中很少部分的重要变量，如遗传算法等。
- **嵌入法**的基本思想是将变量选择与分类模型的建立融合在一起，变量的重要性评价依靠特定分类模型的算法实现，在建立模型的同时，可以给出各变量重要性的得分值，如PLS-DA方法的VIP统计量等。

为了更加客观、全面地评价每个变量的重要性，代谢组学研究中一般采取将上述方法结合起来的方式进行变量筛选。比较常见的一种策略是先进行单变量分析，再结合多变量模型中变量重要性评分作为筛选标准，如挑选fdr≤0.05 和VIP＞1.5的变量作为潜在生物标志物。

用筛选的潜在生物标志物对外部测试数据集进行预测，评价其预测效果。最后，可以通过研究生物标志物的生物学功能和代谢通路，分析不同生物标志物之间的相互作用和关系，从而为探索生物代谢机制提供重要线索和信息。

Yang Jinglei 等（2013) 即在代谢组学分析中使用fdr≤0.2和VIP＞1.5的双重标准来筛选精神分裂症的特异生物标志物，所筛选出的差异代谢物其AUC 在训练数据中达94. 5%，外部测试数据中达0. 895。



<!--chapter:end:06-biology.Rmd-->

\cleardoublepage 

# (APPENDIX) 附录 {-}

# 编程实战题 {#action}

列出我们板块的人气最旺的20个题目，作为第五章节学习后的复习题目。

#### [1.生信编程很简单](http://www.biotrainee.com/thread-834-1-1.html) {-}

##### 编程语言系统入门 {-}

- [生信分析人员如何系统入门python?](https://mp.weixin.qq.com/s/073y_TDA2gcJVGrw0XBxMA)
- [生信分析人员如何系统入门perl？](http://mp.weixin.qq.com/s/lq6pSGtbZxbc0A6-ZLqaqQ)
- [生信分析人员如何系统入门R？](http://mp.weixin.qq.com/s/ouejcgJ0rLy7eBxNPRO50g)
- [生信分析人员如何系统入门Linux？](http://mp.weixin.qq.com/s/xWtUFG37WMTzA3c0PnCNtg)

##### 题目 {-}

###### 对FASTQ的操作 {-}

- 5,3段截掉几个碱基
- 序列长度分布统计
- FASTQ 转换成 FASTA
- 统计碱基个数及GC%

###### 对FASTA的操作 {-}

- 取互补序列
- 取反向序列
- DNA to RNA
- 大小写字母形式输出
- 每行指定长度输出序列
- 按照序列长度/名字排序
- 提取指定ID的序列
- 随机抽取序列

###### 高级难度 {-}

- 根据坐标取序列
- 多文件合并
- 根据ID列表取序列
- GTF文件探索
- 简并碱基的引物序列还原成多条序列
- snp进行注释并格式化输出

##### 下载安装bowtie2（内含测试数据） {-}

```
cd ~/biosoft
mkdir bowtie &&  cd bowtie
wget https://sourceforge.net/projects/bowtie-bio/files/bowtie2/2.2.9/bowtie2-2.2.9-linux-x86_64.zip  
unzip bowtie2-2.2.9-linux-x86_64.zip
```

#### [2.人类基因组的外显子区域的长度](http://www.biotrainee.com/thread-3-1-1.html) {-}
 
##### 题目 {-}

下载人类外显子的坐标文件，编写代码统计外显子区域的长度。

##### 测试数据 {-}

- Rbioconductor的TxDb.Hsapiens.UCSC.hg19.knownGene包
- NCBI数据库：ftp://ftp.ncbi.nlm.nih.gov/pub/CCDS/current_human/

##### R实现代码示例 {-}

```
rm(list=ls())
a=read.table(choose.files(),sep = '\t',stringsAsFactors = F,header = T) # 选择你下的CCDs文件
tmp <- apply(a[1:100,], 1, function(gene){ # 取前100行数据分析调试
# gene=a[3,]
  x=gene[10] # Column10 外显子坐标位置列
  if(grepl('\\]',x)){ # 判断x中是否存在有]这样的符号，如果有就利用正则替换掉。
    x=sub('\\[','',x)
    x=sub('\\]','',x)
    # 这个时候得到的对象还是像这样的“880073-880179, 880436-880525……”
    tmp <- strsplit(as.character(x),',')[[1]]# 我们先从逗号开始分割成小块
    start <- as.numeric(unlist(lapply(tmp,function(y){# 取开始位点
      strsplit(as.character(y),'-')[[1]][1]
    })))
    end <- as.numeric(unlist(lapply(tmp,function(y){ # 取结束位点
      strsplit(as.character(y),'-')[[1]][2]
    })))
    gene_d <- data.frame(gene=gene[3], # 将基因名，染色体，开始、结束位点绑定为数据框
                      chr=gene[1],
                      start=start,
                      end=end
    )
    return (gene_d)#返回数据框
  }
}) 
tmp_pos=c() # 构造一个空的向量
lapply(tmp[1:10], function(x){ # 取前10个list文件计算调试
# print(x)
  if ( !is.null(x)){
    apply(x, 1,function(y){
      #print(y)
      for ( i in as.numeric(y[3]):as.numeric(y[4]) ) # y[3]为坐标起点，y[4]为终止坐标，历编
        tmp_pos <<- c(tmp_pos,paste0(y[2],"-",i))
    })
    
  }
})
length(tmp_pos) # 计算exon的长度
length(unique(tmp_pos)) # 计算去重后的exon的长度
```

#### [3.hg19基因组序列的一些探究](http://www.biotrainee.com/thread-625-1-1.html) {-}

#####  题目 {-}

求：hg19 每条染色体长度，每条染色体N的含量，GC含量。
（高级作业：蛋白编码区域的GC含量会比基因组其它区域的高吗？ ）

#####  测试数据 {-}

- hg19基因组序列下载

```
wget http://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/chromFa.tar.gz # 也可以在浏览器上下载
tar xvzf chromFa.tar.gz
cat *.fa > hg19.fa
rm chr*.fa # 先把着急删，我待会可能要那他测试运行速度
```

- 简单测试数据

```
>chr_1
ATCGTCGaaAATGAANccNNttGTA
AGGTCTNAAccAAttGggG
>chr_2
ATCGAATGATCGANNNGccTA
AGGTCTNAAAAGG
>chr_3
ATCGTCGANNNGTAATggGA
AGGTCTNAAAAGG
>chr_4
ATCGTCaaaGANNAATGANGgggTA
```

##### Perl代码示例 {-}

###### 单行命令 {-}

```
perl -alne '{if(/^>/){$chr=$_}else{ $A_count{$chr}+=($_=~tr/Aa//); $T_count{$chr}+=($_=~tr/Tt//);$C_count{$chr}+=($_=~tr/Cc//); $G_count{$chr}+=($_=~tr/Gg//); $N_count{$chr}+=($_=~tr/Nn//); }}END{print "$_\t$A_count{$_}\t$T_count{$_}\t$C_count{$_}\t$G_count{$_}\t$N_count{$_}" foreach sort keys  %N_count}' test.fa 
```

###### 完整代码 {-}

```
while (<>){
chomp;
if(/^>/){
$chr=$_
}
else{ 
$A_count{$chr}+=($_=~tr/Aa//);
$T_count{$chr}+=($_=~tr/Tt//);
$C_count{$chr}+=($_=~tr/Cc//); 
$G_count{$chr}+=($_=~tr/Gg//);
$N_count{$chr}+=($_=~tr/Nn//); 
}
} 
foreach (sort keys  %N_count){
$length = $A_count{$_}+$T_count{$_}+$C_count{$_}+$G_count{$_}+$N_count{$_};
$gc = ($G_count{$_}+$C_count{$_})/($A_count{$_}+$T_count{$_}+$C_count{$_}+$G_count{$_});
print "$_\t$A_count{$_}\t$T_count{$_}\t$C_count{$_}\t$G_count{$_}\t$N_count{$_}\t$length\t$gc\n" 
}
```

##### 参考结果{-}

结果如下；
```
>chr_1        13        10        7        10        4
>chr_2        11        6        5        8        4
>chr_3        10        6        3        10        4
>chr_4        9        4        2        7        3
```

#### [4.hg38每条染色体的基因、转录本分布](http://www.biotrainee.com/thread-626-1-1.html) {-}

#####  题目 {-}

对GTF注释文件进行探究，统计每条染色体基因数、转录本数、内含子数、外显子数。

高级作业：下载[human/rat/mouse/dog/cat/chicken等物种的gtf注释文件](http://asia.ensembl.org/info/data/ftp/index.html) 编写函数实现对多个GTF文件进行批量统计染色体基因、转录本的分布及转录本外显子个数；

继续探索回答以下问题：

- 所有基因平均有多少个转录本？
- 所有转录本平均有多个exon和intron？
- 如果要比较多个数据库呢（gencode/UCSC/NCBI？）？
- 如果把基因分成多个类型呢？protein coding gene，pseudogene，lncRNA还有miRNA的基因？它们的特征又有哪些变化呢？

##### 测试数据 {-}

```
wget -c ftp://ftp.ensembl.org/pub/release-87/gtf/homo_sapiens/Homo_sapiens.GRCh38.87.chr.gtf.gz
gzip -d Homo_sapiens.GRCh38.87.chr.gtf.gz
```

##### 代码示例 {-}

```
# 每条染色体的基因个数
zcat Homo_sapiens.GRCh38.87.chr.gtf.gz |perl -alne '{print if $F[2] eq "gene" }' |cut -f 1 |sort |uniq -c
# 基因分类
zcat Homo_sapiens.GRCh38.87.chr.gtf.gz |perl -alne '{next unless $F[2] eq "gene" ;/gene_biotype "(.*?)";/;print $1}'  |sort |uniq -c
```

#### [5.多个同样行列式文件的合并](http://www.biotrainee.com/thread-603-1-1.html) {-}

#####  题目 {-}

将htseq-count生成的所有独立样本文件进行合并（每个样品对应一个文件，包括了所有基因表达量）。
希望通过编程处理每个文件得到输出的表达矩阵（行名是基因名，列名是样品名），如下所示：

##### 模拟数据 {-}

用perl脚本模仿htseq-count计算每个样本所有的基因表达量的输出独立样本文件：

```
## 首先新建文件tmp.sh 输入这个代码：
perl -le '{print "gene_$_\t".int(rand(1000)) foreach 1..99}'
## 然后用perl脚本调用这个tmp.sh文件：
perl -e 'system(" bash tmp.sh >$_.txt") foreach a..z'
## 这样就生成了a~z这26个样本的counts文件
```

第一列是基因，第二列是该基因的counts值，共有a~z这26个样本的counts文件，需要合并成一个大的行列式，这样才能导入到R里面做差异分析，如果手工用excel表格做，当然是可以的，但是太麻烦，如果有500个样本，正常人都不会去手工做了，需要编程。

**每个样本的基因顺序并不一致，这时候你应该怎么做呢？**

##### 真实数据 {-}

实际需求如下：[GSE48213](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE48213)里面有56个文件，需要合并成一个表达矩阵，来根据cell-line的不同，分组做差异分析。可以查看[paper](https://www.ncbi.nlm.nih.gov/pubmed/24176112 )

```
wget -c ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE48nnn/GSE48213/suppl/GSE48213_RAW.tar
tar -xf GSE48213_RAW.tar
gzip -d *.gz
```

##### 代码示例 {-}

```
## 首先在GSE48213_RAW目录里面生成tmp.txt文件，使用shell脚本：
awk '{print FILENAME"\t"$0}' * |grep -v EnsEMBL_Gene_ID >tmp.txt
## 然后把tmp.txt导入R语言里面用reshape2处理即可！
setwd('tmp/GSE48213_RAW/')
a=read.table('tmp.txt',sep = '\t',stringsAsFactors = F)
library(reshape2)
fpkm <- dcast(a,formula = V2~V1)
```

#### [6.根据GTF画基因的多个转录本结构](http://www.biotrainee.com/thread-624-1-1.html) {-}

#####  题目 {-}

从NCBI,ENSEMBL,UCSC,GENCODE数据库下载各种GTF注释文件，编写代码得到所有基因的转录本个数，以及每个转录本的外显子的坐标，绘制如下转录本结构图：

> 比如对这个ANXA1基因来说，非常多的转录本，但是基因的起始终止坐标，是所有转录本起始终止坐标的极大值和极小值。同时，它是一个闭合基因，因为它存在一个转录本的起始终止坐标等于该基因的起始终止坐标。可以看到它的外显子并不是非常整齐的，虽然多个转录本会共享某些外显子，但是也存在某些外显子比同区域其它外显子长的现象。

##### 测试数据 {-}

```
wget -c http://www.broadinstitute.org/cancer/cga/sites/default/files/data/tools/rnaseqc/gencode.v7.annotation_goodContig.gtf.gz
gzip -d gencode.v7.annotation_goodContig.gtf.gz
```

##### R实现代码示例 {-}

```
rm(list=ls())

## http://www.broadinstitute.org/cancer/cga/sites/default/files/data/tools/rnaseqc/gencode.v7.annotation_goodContig.gtf.gz

setwd('tmp')
gtf <- read.table('gencode.v7.annotation_goodContig.gtf.gz',stringsAsFactors = F,
                  header = F,comment.char = "#",sep = '\t'
                  )
table(gtf[,2])
gtf <- gtf[gtf[,2] =='HAVANA',]
gtf <- gtf[grepl('protein_coding',gtf[,9]),]

lapply(gtf[1:10,9], function(x){
  y=strsplit(x,';') 
})

gtf$gene <- lapply(gtf[,9], function(x){
  y <- strsplit(x,';')[[1]][5]
  strsplit(y,'\\s')[[1]][3]
  }
)
draw_gene = 'TP53'
structure = gtf[gtf$gene==draw_gene,]
  
colnames(structure) =c(
  'chr','db','record','start','end','tmp1','tmp2','tmp3','tmp4','gene'
)
gene_start <- min(c(structure$start,structure$end))
gene_end <- max(c(structure$start,structure$end))
tmp_min=min(c(structure$start,structure$end))
structure$new_start=structure$start-tmp_min
structure$new_end=structure$end-tmp_min
tmp_max=max(c(structure$new_start,structure$new_end))
num_transcripts=nrow(structure[structure$record=='transcript',])
tmp_color=rainbow(num_transcripts)

x=1:tmp_max;y=rep(num_transcripts,length(x))
#x=10000:17000;y=rep(num_transcripts,length(x))
plot(x,y,type = 'n',xlab='',ylab = '',ylim = c(0,num_transcripts+1))
title(main = draw_gene,sub = paste("chr",structure$chr,":",gene_start,"-",gene_end,sep=""))
j=0;
tmp_legend=c()
for (i in 1:nrow(structure)){
  tmp=structure[i,]
  if(tmp$record == 'transcript'){
    j=j+1
    tmp_legend=c(tmp_legend,paste("chr",tmp$chr,":",tmp$start,"-",tmp$end,sep=""))
  }
  if(tmp$record == 'exon') lines(c(tmp$new_start,tmp$new_end),c(j,j),col=tmp_color[j],lwd=4)
}
```

##### 参考结果 {-}


#### [7.下载最新版的KEGG信息，并且解析好](http://www.biotrainee.com/thread-672-1-1.html) {-}

#####  题目 {-}

下载最新版的KEGG注释文本文件，编写脚本整理成kegg的pathway的ID与基因ID的对应格式。

##### 测试数据 {-}

- 1 首先打开[KEGG官方网站](http://www.genome.jp/kegg/catalog/org_list.html)，网页中展示出了各个物种的分类、拉丁名称、英文名称等信息。


- 2 直接网页中搜索（Ctrl + F）需要下载的物种英文名称或拉丁名。如果不确定物种名称，网站中提供了详细的分类系统，也可根据前面的物种分类信息进行查找。
  本文以拟南芥为例，搜索“Arabidopsis thaliana”即可找到。找到后点击物种名称前的3个字母缩写链接（下图红色框中的位置）。


- 3 进入后的网页中包含了物种的一些基因组信息，点击上方的“Brite hierarchy”，进入后再点击“KEGG Orthology (KO)”；


- 4 在跳转出的网页中点击“Download htext”，弹出下载窗口进行下载，国外网站有时会出现无法下载的情况，多试几次即可；

- 5 当然，下载好之后还没有结束。下载得到文本文件，可以看到里面的结构层次非常清楚，C开头的就是kegg的pathway的ID所在行，D开头的就是属于它的kegg的所有的基因。A,B是kegg的分类，总共是6个大类，42个小类。


##### Perl代码示例 {-}

```
perl -alne '{if(/^C/){/PATH:hsa(\d+)/;$kegg=$1}else{print "$kegg\t$F[1]" if /^D/ and $kegg;}}' hsa00001.keg >kegg2gene.txt
```

##### 参考结果 {-}


#### [8.写超几何分布检验](http://www.biotrainee.com/thread-749-1-1.html) {-}

#####  题目 {-}

学习GO/KEGG的富集分析的原理，编写代码实现超几何分布检验，将得到的结果与测试数据中的kegg.enrichment.html进行比较。

(P值的计算：C(k,M)*C(n-k,N-M)/C(n,N) )

##### 测试数据 {-}

- kegg2gene（第六讲kegg数据解析结果）

>暂时不用最新版的kegg注释数据，为了能够统一答案

- 差异基因list和背景基因list(R代码)

```
source("http://bioconductor.org/biocLite.R")
biocLite("org.Hs.eg.db")
biocLite("KEGG.db")
biocLite("GOstats")
biocLite("hgu95av2.db")

library(org.Hs.eg.db)
library(KEGG.db)
library(GOstats)
library("hgu95av2.db")

##得到kegg2gene.list(KEGG注释信息)

tmp=toTable(org.Hs.egPATH)
write.table(tmp,'kegg2gene.list.txt',quote = F,row.names = F)
## 得到universeGeneIds.txt(背景基因list)
ls('package:hgu95av2.db')
universeGeneIds <- unique(mappedRkeys(hgu95av2ENTREZID))
write.table(universeGeneIds,'universeGeneIds.txt',quote = F,row.names = F)

## 得到diff_gene.txt(差异基因list)

set.seed('123456.789')
diff_gene = sample(universeGeneIds,300)
write.table(diff_gene,'diff_gene.txt',quote = F,row.names = F)

## 得到kegg.enrichment.html(富集分析结果)

annotationPKG='org.Hs.eg.db'
hyperG.params = new("KEGGHyperGParams", geneIds=diff_gene, universeGeneIds=universeGeneIds, annotation=annotationPKG,
categoryName="KEGG", pvalueCutoff=1, testDirection = "over")
KEGG.hyperG.results = hyperGTest(hyperG.params);
htmlReport(KEGG.hyperG.results, file="kegg.enrichment.html", summary.args=list("htmlLinks"=TRUE))

```

##### R代码示例 {-}

下载链接： https://github.com/jmzeng1314/humanid/blob/master/R/hyperGtest_jimmy.R

```
library("hgu95av2.db")[/align][align=left]ls('package:hgu95av2.db')
universeGeneIds <- unique(mappedRkeys(hgu95av2ENTREZID))
set.seed('123456.789')
diff_gene = sample(universeGeneIds,300)
library(org.Hs.eg.db)
library(KEGG.db)
tmp=toTable(org.Hs.egPATH)
GeneID2kegg<<- tapply(tmp[,2],as.factor(tmp[,1]),function(x) x)
kegg2GeneID<<- tapply(tmp[,1],as.factor(tmp[,2]),function(x) x)
#results <- hyperGtest_jimmy(GeneID2kegg,kegg2GeneID,diff_gene,universeGeneIds)

GeneID2Path=GeneID2kegg
Path2GeneID=kegg2GeneID

diff_gene_has_path=intersect(diff_gene,names(GeneID2Path))
n=length(diff_gene) #306
N=length(universeGeneIds) #5870
results=c()

for (i in names(Path2GeneID)){
  #i="04672"
  M=length(intersect(Path2GeneID[[i]],universeGeneIds))
  #print(M)
  
  if(M<5)
    next
  exp_count=n*M/N
  #print(paste(n,N,M,sep="\t"))
  k=0
  for (j in diff_gene_has_path){
    if (i %in% GeneID2Path[[j]]) k=k+1
  }
  OddsRatio=k/exp_count
  if (k==0) next
  p=phyper(k-1,M, N-M, n, lower.tail=F)
  #print(paste(i,p,OddsRatio,exp_count,k,M,sep="    "))
  results=rbind(results,c(i,p,OddsRatio,exp_count,k,M))
}
colnames(results)=c("PathwayID","Pvalue","OddsRatio","ExpCount","Count","Size")
results=as.data.frame(results,stringsAsFactors = F)
results$p.adjust = p.adjust(results$Pvalue,method = 'BH')
results=results[order(results$Pvalue),]
rownames(results)=1:nrow(results)
```

#### [9.ID转换](http://www.biotrainee.com/thread-941-1-1.html)  {-}

#####  题目 {-}

probe_id，gene_id，gene_name, symbol之间的转换。

##### 测试数据 {-}

- 需要转换的探针ID(变量my_probe)

```
rm(list=ls())
library("hgu95av2.db")
ls('package:hgu95av2.db')
probe2entrezID=toTable(hgu95av2ENTREZID)
probe2symbol=toTable(hgu95av2SYMBOL)
probe2genename=toTable(hgu95av2GENENAME)

my_probe = sample(unique(mappedLkeys(hgu95av2ENTREZID)),30)

tmp1 = probe2symbol[match(my_probe,probe2symbol$probe_id),]
tmp2 = probe2entrezID[match(my_probe,probe2entrezID$probe_id),]
tmp3 = probe2genename[match(my_probe,probe2genename$probe_id),]

write.table(my_probe,'my_probe.txt',quote = F,col.names = F,row.names =F)
write.table(tmp1$symbol,'my_symbol.txt',quote = F,col.names = F,row.names =F)
write.table(tmp2$gene_id,'my_geneID.txt',quote = F,col.names = F,row.names =F)
```

- 下载对应关系
  ftp://ftp.ncbi.nlm.nih.gov/gene/DATA/ 
- illumina芯片的探针

> 所有bioconductor支持的芯片平台对应关系：通过bioconductor包来获取所有的芯片探针与gene的对应关系；可以从NCBI的GPL平台下载：http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GPL6947；也可以直接加载对应的包；或者直接去公司的主页下载manifest文件。


```
library("illuminaHumanv4.db")
ls('package:illuminaHumanv4.db')

probe2entrezID=toTable(illuminaHumanv4ENTREZID)
probe2symbol=toTable(illuminaHumanv4SYMBOL)
probe2genename=toTable(illuminaHumanv4GENENAME)

my_probe = sample(unique(mappedLkeys(illuminaHumanv4ENTREZID)),30)

probe2symbol[match(my_probe,probe2symbol$probe_id),]
probe2entrezID[match(my_probe,probe2entrezID$probe_id),]
probe2genename[match(my_probe,probe2genename$probe_id),]

```

##### R代码示例

基因的转换：运行下面的R代码，得到的my_symbol_gene和my_entrez_gene就是需要转换的ID。

```
library("illuminaHumanv4.db")
ls('package:illuminaHumanv4.db')
my_entrez_gene = sample(unique(mappedRkeys(illuminaHumanv4ENTREZID)),30)
my_symbol_gene = sample(unique(mappedRkeys(illuminaHumanv4SYMBOL)),30)

library("org.Hs.eg.db")
ls('package:org.Hs.eg.db')

entrezID2symbol <- toTable(org.Hs.egSYMBOL)

entrezID2symbol[match(my_entrez_gene,entrezID2symbol$gene_id),]
entrezID2symbol[match(my_symbol_gene,entrezID2symbol$symbol),]
```


#### [10.根据指定染色体及坐标得到序列](http://www.biotrainee.com/thread-666-1-1.html)

#####  题目 {-}

参考基因组hg19，指定染色体及坐标"chr5","8397384"，编写程序得到这个坐标以及它上下一个碱基。（机器无法计算hg19，则使用测试数据，指定坐标是 3号染色体的第6个碱基。）

##### 测试数据 {-}

```
>chr_1
ATCGTCGaaAATGAANccNNttGTA
AGGTCTNAAccAAttGggG
>chr_2
ATCGAATGATCGANNNGccTA
AGGTCTNAAAAGG
>chr_3
ATCGTCGANNNGTAATggGA
AGGTCTNAAAAGG
>chr_4
ATCGTCaaaGANNAATGANGgggTA
```

#### [11.根据指定染色体及坐标得到位置信息](http://www.biotrainee.com/thread-863-1-2.html) {-}

#####  题目 {-}

基因的chr,start,end都是已知的（坐标是hg38系统），任意给定基因组的chr:pos(chr1:2075000-2930999), 判断该区间在哪个基因上面？（可用现成软件bedtools）

##### 测试数据 {-}

```
chr7        148697841        148698941
chr7        148698942        148699029
chr7        148699911        148701053
chr7        148701109        148701307
chr7        148701354        148702694
chr7        148703100        148703520
chr7        148703831        148704175
chr7        148704484        148704734
chr7        148704857        148705937
chr7        148706271        148706671
```


#### [12.把文件内容按照染色体分开写出](http://www.biotrainee.com/thread-1329-1-1.html) {-}

#####  题目 {-}

根据染色体把文件拆分成1~22和其它染色体的两个文件呢？

##### 测试数据 {-}

```
wget ftp://ftp.ncbi.nlm.nih.gov/pub/CCDS/current_human/CCDS.current.txt
```
把文件改成bed格式，如下：
```
chr2    43995310    43995986
chr17  49788603    49789067
chr17  59565573    59566163
chr19  8390308 8390745
chr12  49188033    49189033
chr7    974903  975570
chr7    98878532    98879500
chr7    44044672    44045322
chr1    153634052  153634772
chr11  60905850    60906575
```

##### Perl代码示例 {-}

```
use FileHandle;
foreach( 1..22 ){
    $normal_chr{"chr".$_}=1 ;
    open $fh{"chr".$_},">chr$_.txt" or die;
}
open other,">chr_other.txt" or die;
open FH,'a.bed';
while(<FH>){
    chomp;
    @F=split;
    if(exists $normal_chr{$F[0]}){
        $fh{$F[0]}->print( "$_\n" );
    }else{ 
        print other $_;
    }
}
foreach( 1..22 ){ 
    close $fh{$_};
}
close other;
```

#### [13.JSON格式数据的格式化](http://www.biotrainee.com/thread-1355-1-1.html) {-}

#####  题目 {-}

学习json格式，下载测试数据，从该json文件里面提取：technique factor target principal_investigator submission label category type Developmental-Stage organism key这几列信息。

##### 测试数据 {-}

http://biotrainee.com/jbrowse/JBrowse-1.12.1/sample_data/json/modencode/modencodeMetaData.json

##### Perl代码示例 {-}

```
#!/usr/bin/env perl
use strict;
use warnings;
use autodie ':all';
use 5.10.0;

use JSON 2;

my $data = from_json( do { local $/; open my $f, '<', $ARGV[0]; scalar <$f> } );

my @fields = qw( technique factor target principal_investigator submission label category type Developmental-Stage organism key );

say join ',', map "\"$_\"", @fields;

for my $item ( @{$data->{items}} ) {
    $item->{key} = $item->{label};
    no warnings 'uninitialized';
    for my $track ( @{$item->{Tracks}} ) {
        $item->{label} = $track;
        say join ',', map "\"$_\"", @{$item}{@fields};
    }
}
```

##### 参考结果 {-}

完成之后的结果应该是：http://biotrainee.com/jbrowse/JBrowse-1.12.1/sample_data/json/modencode/modencodeMetaData.csv


#### [14.多个探针对应一个基因，取平均值、最大值](http://www.biotrainee.com/thread-2077-1-1.html ) {-}


#####  题目 {-}

编写脚本对多个探针对应一个基因，取平均值、最大值。

##### 测试数据 {-}

R里面的包自带很多芯片表达数据，所以测试数据就在下面的R代码里面

##### R代码示例 {-}

```
# 平均值
BiocInstaller::biocLite('CLL')
BiocInstaller::biocLite('hgu95av2.db')

library('hgu95av2.db')
library(CLL)
data(sCLLex)
sCLLex=sCLLex[,1:8] ## 样本太多，我就取前面8个
group_list=sCLLex$Disease
exprSet=exprs(sCLLex)
exprSet=as.data.frame(exprSet)
exprSet$probe_id=rownames(exprSet)
head(exprSet)
probe2symbol=toTable(hgu95av2SYMBOL)
dat=merge(exprSet,probe2symbol,by='probe_id')
results=t(sapply(split(dat,dat$symbol),function(x) colMeans(x[,1:(ncol(x)-1)])))

```

#### [15.把counts矩阵转换成RPKM矩阵](http://www.biotrainee.com/thread-1791-1-1.html ) {-}

#####  题目 {-}

编写脚本将hisat2+htseq-counts之后得到reads的counts矩阵转换成RPKM矩阵。

##### 测试数据 {-}

```
wget ftp://ftp.ncbi.nlm.nih.gov/pub/CCDS/current_mouse/CCDS.current.txt
grep -v '^#' CCDS.current.txt | perl -alne '{/\[(.*?)\]/;$len=0;foreach(split/,/,$1){@tmp=split/-/;$len+=($tmp[1]-$tmp[0])};$h{$F[2]}=$len if $len >$h{$F[2]}} END{print "$_\t$h{$_}" foreach sort keys %h}' >mm10_ccds_length.txt
```

##### R代码示例 {-}

```
genes_len=read.table("mm10_ccds_length.txt",stringsAsFactors=F)
head(genes_len)
              V1    V2
1      -343C11.2  1139
2 00R_AC107638.2  1060
3      00R_Pgap2  1676
4  0610005C13Rik  7363
5  0610006L08Rik 34995
6  0610007P14Rik  9074

  
colnames(genes_len)<- c("GeneName","Len")

 head(exprSet)
              GSM860181_priSG-A GSM860182_SG-A GSM860183_SG-B GSM860184_lepSC
00R_AC107638.2                0              1              0              1
0610005C13Rik                20            22            11              27
0610006L08Rik                  0              0              0              2
0610007P14Rik              2075          1785          1269            1430
0610009B22Rik                256            376            300            386
0610009E02Rik                22            22            16              28
  
exprSet<-exprSet[ rownames(exprSet) %in% genes_len$GeneName ,]

total_count<- colSums(exprSet)

neededGeneLength=genes_len[  match(rownames(exprSet), genes_len$GeneName) ,2] 
  
rpkm <- t(do.call( rbind,lapply(1:length(total_count),function(i){
        10^9*exprSet[,i]/neededGeneLength/total_count[i]
}) ))
head(rpkm)
rownames(rpkm)=rownames(exprSet)
colnames(rpkm)=colnames(exprSet)

write.table(rpkm,file="rpkm.txt",sep="\t",quote=F)

  
  
library(TxDb.Mmusculus.UCSC.mm10.knownGene)
txdb=TxDb.Mmusculus.UCSC.mm10.knownGene
gt=transcriptsBy(txdb,by="gene")
lapply(gt[1:40],function(x) max(width(x)))
library(org.Mm.eg.db)

mykeys=
columns(txdb);keytypes(txdb)
neededcols <- c("GENEID", "TXSTRAND", "TXCHROM")
select(txdb, keys=mykeys, columns=neededcols, keytype="TXNAME")
```

##### 参考结果 {-}

#### [16.对有临床信息的表达矩阵批量做生存分析](http://www.biotrainee.com/thread-929-1-1.html) {-}

#####  题目 {-}

使用R实现生存分析：
用```my.surv <- surv(OS_MONTHS,OS_STATUS=='DECEASED')```构建生存曲线；
用```kmfit2 <- survfit(my.surv~TUMOR_STAGE_2009)```来做某一个因子的KM生存曲线；
用```survdiff(my.surv~type, data=dat)```来看看这个因子的不同水平是否有显著差异，其中默认用是的logrank test 方法；
用```coxph(Surv(time, status) ~ ph.ecog + tt(age), data=lung) ```来检测自己感兴趣的因子是否受其它因子(age,gender等等)的影响。

##### 代码示例 {-}

```
rm(list=ls())
## 50 patients and 200 genes 
dat=matrix(rnorm(10000,mean=8,sd=4),nrow = 200)
rownames(dat)=paste0('gene_',1:nrow(dat))
colnames(dat)=paste0('sample_',1:ncol(dat))
os_years=abs(floor(rnorm(ncol(dat),mean = 50,sd=20)))
os_status=sample(rep(c('LIVING','DECEASED'),25))

library(survival)
my.surv <- Surv( os_years,os_status=='DECEASED')
## The status indicator, normally 0=alive, 1=dead. Other choices are TRUE/FALSE (TRUE = death) or 1/2 (2=death). 
## And most of the time we just care about the time od DECEASED . 

fit.KM=survfit(my.surv~1)
fit.KM
plot(fit.KM)

log_rank_p <- apply(dat, 1, function(values1){
  group=ifelse(values1>median(values1),'high','low')
  kmfit2 <- survfit(my.surv~group)
  #plot(kmfit2)
  data.survdiff=survdiff(my.surv~group)
  p.val = 1 - pchisq(data.survdiff$chisq, length(data.survdiff$n) - 1)
})
names(log_rank_p[log_rank_p<0.05])

gender= ifelse(rnorm(ncol(dat))>1,'male','female')
age=abs(floor(rnorm(ncol(dat),mean = 50,sd=20)))
## gender and age are similar with group(by gene expression)

cox_results <- apply(dat , 1, function(values1){
  group=ifelse(values1>median(values1),'high','low')
  survival_dat <- data.frame(group=group,gender=gender,age=age,stringsAsFactors = F)
  m=coxph(my.surv ~ age + gender + group, data =  survival_dat)
  
  beta <- coef(m)
  se <- sqrt(diag(vcov(m)))
  HR <- exp(beta)
  HRse <- HR * se
  
  #summary(m)
  tmp <- round(cbind(coef = beta, se = se, z = beta/se, p = 1 - pchisq((beta/se)^2, 1),
                    HR = HR, HRse = HRse,
                    HRz = (HR - 1) / HRse, HRp = 1 - pchisq(((HR - 1)/HRse)^2, 1),
                    HRCILL = exp(beta - qnorm(.975, 0, 1) * se),
                    HRCIUL = exp(beta + qnorm(.975, 0, 1) * se)), 3)
  return(tmp['grouplow',])
  
})
cox_results[,cox_results[4,]<0.05]
```

PS: 里面的os_years应该修改为os_month，因为总的生存期不应该长达几十年，而且因为ag和os_years都是随机生成的，可能会出现很不符合自然科学的现象。但是这个是模拟数据，请大家不用较真。

####  [17.对多个差异分析结果直接取交集并集](http://www.biotrainee.com/thread-1288-1-2.html) {-}

#####  题目 {-}

编写脚本对每两个差异分析结果计算基因交集个数与基因并集个数的比值，得到一个比值矩阵。

##### 测试数据 {-}

用perl单行命令模拟数据：

```
perl -e 'BEGIN{use List::Util qw/shuffle/;@gene=A..Z}{foreach(1..10){@this_genes=@gene[(shuffle(0..25))[0..int(rand(20))+4]];print "comparison_$_ <-  ";print join(";",@this_genes);print "\n"}}'
```

总共10次差异分析，每次差异分析得到的差异基因在同一行的后面用大小字母表示。

```
comparison_1 -> I;G;E;V;C;K;B;W
comparison_2 -> G;E;N;H;Y;M;L;S;K;A;J;O;D;P;R;U;Q;F;Z;C
comparison_3 -> Y;V;U;N;H;K;I;P;S;F;D;X;G;C;Z;J;Q;T;W;O;E;M
comparison_4 -> N;T;K;B;H;Z;W;C;Q;I;V;F;D;S;R;Y;J;X;P;O;G;L;A
comparison_5 -> G;J;A;H;W;T;Z;E;Y;S;R
comparison_6 -> Z;M;D;R;P;G;L;W;Y;U;X;E;A;S;T;I;H
comparison_7 -> H;Z;T;O;W;Q;M;X;J;N;U;K;F;P;I;C;S;Y;A;B
comparison_8 -> A;R;L;T;W;Q;S;F;P;X;E;V;Y;G;K;J;Z;C
comparison_9 -> J;X;K;D;O;H;L;F;C;P;R;N
comparison_10 -> G;S;K;H;C;O;W;F;Q;X
```

##### R代码示例 {-}

```
a=readLines('test.txt')
n=unlist(lapply(a , function(x){
  trimws(strsplit(x,'<-')[[1]][1])
}))
v=lapply(a , function(x){
  trimws(strsplit(trimws(strsplit(x,'<-')[[1]][2]),';')[[1]])
})
names(v)=n
tmp=unlist(lapply(v, function(x){
  lapply(v, function(y){
    p1=length(intersect(x,y))
    p2=length(union(x,y))
    return(p1/p2)
  })
}))
out=matrix(tmp,nrow = length(n))
rownames(out)=n
colnames(out)=n
out[1:5,1:5]
```

##### 参考结果 {-}

结果的前五行如下：

```
comparison_1 comparison_2 comparison_3 comparison_4 comparison_5
comparison_1    1.0000000    0.1666667    0.3043478    0.2916667    0.1875000
comparison_2    0.1666667    1.0000000    0.6800000    0.6538462    0.4090909
comparison_3    0.3043478    0.6800000    1.0000000    0.7307692    0.3750000
comparison_4    0.2916667    0.6538462    0.7307692    1.0000000    0.4166667
comparison_5    0.1875000    0.4090909    0.3750000    0.4166667    1.0000000
```

#### [18.根据GTF格式的基因注释文件得到人所有基因的染色体坐标](http://www.biotrainee.com/thread-472-1-1.html ) {-}

#####  题目 {-}

从gencode数据库里面可以下载所有的gtf文件，编写脚本得到基因的染色体、起始终止坐标如下：

```
[jianmingzeng@gencode]$ head protein_coding.hg19.position 
chr1        69091        70008        OR4F5
chr1        367640        368634        OR4F29
chr1        621096        622034        OR4F16
chr1        859308        879961        SAMD11
chr1        879584        894689        NOC2L
chr1        895967        901095        KLHL17
chr1        901877        911245        PLEKHN1
chr1        910584        917473        PERM1
chr1        934342        935552        HES4
chr1        936518        949921        ISG15
[jianmingzeng@gencode]$ head protein_coding.hg38.position 
chr1        69091        70008        OR4F5
chr1        182393        184158        FO538757.2
chr1        184923        200322        FO538757.1
chr1        450740        451678        OR4F29
chr1        685716        686654        OR4F16
chr1        923928        944581        SAMD11
chr1        944204        959309        NOC2L
chr1        960587        965715        KLHL17
chr1        966497        975865        PLEKHN1
chr1        975204        982093        PERM1
```


# 名词解释 {#glossary}

包括测序相关，组学相关的，给第1,6章节作补充。


<!--chapter:end:07-appendix.Rmd-->

